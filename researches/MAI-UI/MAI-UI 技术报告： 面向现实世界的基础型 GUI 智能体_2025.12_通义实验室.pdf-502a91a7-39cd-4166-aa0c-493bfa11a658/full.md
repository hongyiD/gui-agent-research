# MAI-UI Technical Report: Real-World Centric Foundation GUI Agents

Hanzhang Zhou*, Xu Zhang*, Panrong Tong, Jianan Zhang, Liangyu Chen, Quyu Kong Chenglin Cai, Chen Liu, Yue Wang (☑), Jingren Zhou, Steven HOI

Tongyi Lab, Alibaba Group

![](images/d573757405ec367fef46d6680a8980007906dacb393d42e03f3de726a0a8184a.jpg)

https://github.com/Tongyi-MAI/MAI-UI

# Abstract

The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent-user interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI addresses these issues with a unified methodology: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device-cloud collaboration system routes execution by task state, and an online RL framework with advanced optimizations to scale parallel environments and context length.

MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation. On grounding benchmarks, it reaches  $73.5\%$  on ScreenSpot-Pro,  $91.3\%$  on MMBench GUI L2,  $70.9\%$  on OSWorld-G, and  $49.2\%$  on UI-Vision, surpassing Gemini-3-Pro and Seed1.8 on ScreenSpot-Pro. On mobile GUI navigation, it sets a new SOTA of  $76.7\%$  on AndroidWorld, surpassing UI-Tars-2, Gemini-2.5-Pro and Seed1.8. On MobileWorld, MAI-UI obtains  $41.7\%$  success rate, significantly outperforming end-to-end GUI models and competitive with Gemini-3-Pro based agentic frameworks. Our online RL experiments show significant gains from scaling parallel environments from 32 to 512 (+5.2 points) and increasing environment step budget from 15 to 50 (+4.3 points). Finally, the native device-cloud collaboration system improves on-device performance by  $33\%$ , reduces cloud model calls by over  $40\%$ , and preserves user privacy.

![](images/e89c38bb155b225305677f56ca1c523c895b6fc129aea90070e627b44208bb16.jpg)  
Figure 1: MAI-UI achieves SOTA performance across GUI grounding and navigation benchmarks.

# 1 Introduction

Graphical user interface (GUI) agent (Wang et al., 2024b) is an agent system that can perceive, reason, and act within GUIs in response to natural language instructions. By translating high-level user intents into concrete UI operations, GUI agents potentially represent a revolution in digital interaction, transforming human-computer interaction from the manual navigation of complex interfaces to goal-oriented natural language control. Motivated by this vision, the community's increasing efforts have accelerated progress on GUI agents, advancing UI perception, visual grounding, and GUI navigation (Wang et al., 2025a; Ye et al., 2025; Gu et al., 2025c).

Despite rapid technological advances, today's GUI agents remain insufficient for reliable, robust, and secure deployment in practice. We highlight several open challenges to close this gap. (1) Agent-user interaction: Existing systems are typically optimized for end-to-end execution, however, user instructions in real-world settings are often ambiguous or incomplete. To ensure alignment with user intent, the agent must proactively ask clarifying questions, collect missing details, and seek consent for sensitive operations. Effective agent-user interaction is therefore a critical yet often neglected capability. (2) Beyond UI-only action: Relying solely on UI manipulation poses two issues: (i) the long, multi-step sequences of UI operations increase brittleness to per-step errors and amplify error propagation; and (ii) it limits the agent to tasks that are UI-reachable. Integrating external tools via the Model Context Protocol (MCP) provides structured shortcuts that compress long, fragile UI operation sequences into a few API calls and unlock tasks previously infeasible on mobile. For example, via MCP tools, a mobile agent can manipulate GitHub repositories, bringing traditionally desktop-only workflows to mobile. (3) Native device-cloud collaboration capability: Current GUI agents are typically categorized into lightweight, on-device variants or large models that can only be used as cloud services. However, cloud-only solutions introduce privacy risks, higher costs, and dependence on network connectivity, whereas on-device-only approaches are constrained by model capacity and capability. Consequently, foundation GUI agents lack native device-cloud collaboration capability for privacy-aware and cost-efficient routing and seamless handoff. (4) Robustness to dynamic environments: Agents trained on static, pre-collected trajectories often overfit to specific interface patterns and struggle in out-of-domain scenarios. In practice, real-world GUIs are highly dynamic: layouts vary across app versions and devices, and pop-ups or permission dialogs can appear unexpectedly. Without exposure to dynamic environments in training, agents generalize poorly and remain brittle to real-world unpredictability.

To enhance GUI agent capabilities and address these challenges for realistic deployment, we introduce MAI-UI, a foundational GUI agent for general GUI grounding and mobile navigation.

- Agent-user interaction and MCP augmentation. To equip GUI agents with user interaction and MCP tool use capability, we introduce a self-evolving data pipeline that incorporates training data for general navigation as well as these two capabilities. The data pipeline iteratively updates both the model and the training corpus using three sources of data: rejection-sampled trajectories, manually-annotated trajectories, and automatic agent rollouts. The action space is also extended to allow the agent to choose among UI manipulation, user engagement, and MCP tool use.  
- Device-cloud collaboration system. For realistic deployment, MAI-UI introduces a pioneering native device-cloud collaborative system, which can dynamically select on-device or cloud execution based on task execution state and data sensitivity. The system consists of a local GUI agent that both act as a GUI agent and as trajectory monitor, a high-capacity cloud GUI agent, and a local unified trajectory memory that maintains consistent information exchange between local and cloud agents.  
- Reinforcement learning in dynamic environments. MAI-UI incorporates online reinforcement learning as a core training component, enabling improvement through interaction with dynamic environments. Our system-level optimizations scale to  $500+$  GUI environments for parallel rollouts. We further support asynchronous rollout and hybrid parallelism for training, enabling training on long-horizon GUI tasks with up to 50 interactive steps. This training stage yields improved GUI

navigation accuracy and stronger robustness to real-world unpredictability.

MAI-UI includes a full-spectrum of sizes to meet real-world deployment constraints, ranging from efficient 2B on device variants to mid size 8B and 32B models, and large scale 235B-A22B models. Across sizes, our models achieve state-of-the-art performance against strong baselines at comparable scales. Notably, our 2B on-device model achieves a relative improvement of  $75.4\%$  over Ferret-UI Lite (Yang et al., 2025b), our mid size 8B and 32B models surpasses GUI-Owl(Ye et al., 2025), Step-GUI (Yan et al., 2025), and UI-Venus(Gu et al., 2025c), and our 235B-A22B variants outperform UI-Tars-2 (Wang et al., 2025a), Seed1.8 (Seed, 2025a), and Gemini-2.5-Pro (DeepMind, 2025a) on AndroidWorld.

MAI-UI sets a new state of the art across diverse evaluation settings, including GUI grounding, offline and online mobile GUI navigation, and a realistic-oriented benchmark that incorporates MCP tool use and agent-user interaction, consistently outperforming prior works.

- Grounding. Our model establishes new state of the art performance across five well known grounding benchmarks. Notably, MAI-UI achieves  $67.9\%$  on ScreenSpot-Pro (73.5% with zoom-in),  $91.3\%$  on MMBench GUI L2,  $70.9\%$  on OSWorld-G (75.0% on OSWorld-G-Refine),  $47.1\%$  on UI-Vision (49.2% with zoom-in), and  $96.5\%$  on ScreenSpot-V2, substantially surpassing the strongest counterparts.  
- Offline GUI navigation. On Android Control, MAI-UI attains strong exact match accuracy. On GUI Odyssey, it achieves a success rate of  $83.4\%$ , outperforming prior methods.  
- Online GUI navigation. In challenging dynamic real-time environments, MAI-UI-235B-A22B achieves a new state of the art success rate of  $76.7\%$  on AndroidWorld, surpassing UI-Tars-2 (Wang et al., 2025a), Gemini-2.5-Pro (DeepMind, 2025a), and Seed1.8 (Seed, 2025a). Additionally, MAI-UI-32B, MAI-UI-8B, and MAI-UI-2B attain success rate of  $73.3\%$ ,  $70.7\%$ , and  $49.1\%$  on AndroidWorld, respectively, all outperforming competitive models at comparable scales.  
- Realistic-oriented evaluation. To bridge the gap for more challenging and realistic online assessment, we adopt our MobileWorld benchmark (Kong et al., 2025), which comprises evaluations beyond pure GUI operations. MAI-UI obtains  $41.7\%$  success rate, surpassing end-to-end GUI model baselines by  $+20.8$  and competitive with agentic frameworks with GPT-5 or Gemini-3-Pro as planners. Furthermore, on tasks that require agent-user interaction and MCP tool use, MAI-UI gains a success rate of  $37.5\%$  and  $51.1\%$ , respectively, representing an absolute increase of  $+32.1$  and  $+18.7$ .

# 2 MAI-UI

This section details the methodology of MAI-UI. Our approach integrates (i) a training paradigm for GUI grounding, (ii) a self-evolving trajectory data pipeline, (iii) training for agent-user interaction and MCP tool augmentation, (iv) online reinforcement learning, and (v) a native device-cloud collaboration system. Figure 2 illustrates a demo trajectory of MAI-UI.

# 2.1 System Overview

We present a system overview of our model, covering task formulation, the action space, and the model architecture.

# 2.1.1 Task Formulation

MAI-UI cover tasks of two categories: general GUI grounding and mobile GUI navigation.

Grounding Task. GUI grounding aims to localize the UI element corresponding to a natural language instruction on a graphical interface (Wang et al., 2024c). Formally, given a GUI screenshot and a natural language instruction  $\mathcal{I}$ , the GUI agent model predicts a coordinate point  $\mathcal{P} = (x,y)$  that indicates the location of the target UI element.

Navigation Task. Mobile navigation task can be formulated as a Partially Observable Markov Decision Process (POMDP)  $(S, \mathcal{O}, \mathcal{A}, \mathcal{T})$  (Qin et al., 2025b), where:  $S$  denotes the state space capturing the

![](images/6c469bea0f8d5a1168dc0521022a9fbf36fc74905606f16019b1db33ffde3be8.jpg)

![](images/5c9e898054c2e9bb5884f9839f6ba7f8b311731d9ea3400009f2ee29e7e0c5f0.jpg)

![](images/3587ba46e494857c945838582703f8177d5f107e534c2b532c806e35570e646b.jpg)  
Figure 2: Example trajectory of MAI-UI. MAI-UI completes GUI agent tasks via both UI operations and extended actions, including agent-user interaction and MCP tool use, and integrates a native device-cloud collaboration system.

underlying environment;  $\mathcal{O}$  represents the observation space, which consists of a natural language instruction  $\mathcal{I}$  along with one or more screenshots;  $\mathcal{A}$  defines the action space comprising standard mobile UI operations (e.g., click, swipe or typing);  $\mathcal{T}: S \times \mathcal{A} \to S$  specifies the state transition of the environment. At time step  $t$ , the agent predicts the next action as as  $a_t = \pi(\mathcal{I}, o_t, h_t)$ , where  $\mathcal{I}$  is the natural language instruction,  $o_t \in \mathcal{O}$  is the current observation, and  $h_t = (a_1, o_1, \ldots, a_{t-1}, o_{t-1})$  denotes the history context of previous actions and observations.

# 2.1.2 Foundation GUI Agent for Mobile Use

We introduce MAI-UI, a foundation GUI agent with strong GUI grounding and mobile navigation capabilities. It supports effective agent-user interaction for clarification and integrates MCP-based tool augmentation for API use, and features a native device-cloud collaboration system for practical deployment. MAI-UI offers a broad range of model sizes, each specifically trained to deliver robust grounding and navigation capabilities.

Action Space. MAI-UI provides a comprehensive action space for mobile GUI control, including click, swipe, type, wait, etc. By leveraging these actions, MAI-UI can interact with the mobile device through its graphical user interface to complete a wide range of tasks. We also include an answer action to directly respond to user queries in question-answering scenarios.

To better support real-world scenarios, we integrate two specialized actions that extend the agent beyond pure GUI operation through active user interaction and MCP tool use. The ask_user action allows the agent to request clarification when the instruction is vague or underspecified. The mcp_call action enables the agent to leverage external MCP tools rather than relying solely on GUI operations. The full action space is shown in Table 1.

Model Architecture. We employ Qwen3-VL (Bai et al., 2025a) as the backbone model. The MAI-UI family spans a full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants, enabling the deployment tailored to hardware constraints and performance requirements. Each model is jointly trained on GUI grounding, perception, and mobile-use navigation data using supervised fine-tuning and

Table 1: Action Space in MAI-UI.  

<table><tr><td>Action</td><td>Definition</td></tr><tr><td>click</td><td>Clicks at coordinates (x, y).</td></tr><tr><td>long_press</td><td>Long presses at coordinates (x, y).</td></tr><tr><td>type</td><td>Types the specified text content.</td></tr><tr><td>swipe</td><td>Swipe in the given direction (up/down/left/right) at optional coordinates.</td></tr><tr><td>drag</td><td>Drags from start coordinates (x1, y1) to end coordinates (x2, y2).</td></tr><tr><td>system_button</td><td>Presses a system button (back, home, menu, or enter).</td></tr><tr><td>wait</td><td>Pauses for a brief moment.</td></tr><tr><td>terminate</td><td>Marks the task as complete with status (success or fail).</td></tr><tr><td>answer</td><td>Provides an answer with specified text.</td></tr><tr><td>ask_user</td><td>Requests user intervention with specified text.</td></tr><tr><td>mpc_call</td><td>Provides MCP tool name and corresponding arguments.</td></tr></table>

reinforcement learning. In addition, MAI-UI integrates a native device-cloud collaboration system that routes computation by task state and data sensitivity.

# 2.2 GUI Grounding & Perception

GUI grounding and perception are foundational capabilities of GUI agents, enabling the agent to understand screen layouts and to localize the correct UI elements from natural-language instructions. This section presents our data pipeline and training methodology for building these capabilities.

# 2.2.1 Data Pipeline

As shown in Figure 3, in addition to open-source datasets, our pipeline collects screenshots from real GUI environments to build a robust GUI Agent. This process results in multi-task perception data and multi-perspective grounding data.

Data Collection. To gather diverse GUI data from real-world scenarios, we not only use open-source datasets such as JEDI (Xie et al., 2025a) and OS-Altas (Wu et al., 2024), we also virtualize operating systems in containerized environments. We employed an MLLM-guided exploration strategy to navigate these environments. In every step, we ask MLLMs to identify valid actions from the current state. This procedure continuously changes the interface state and produces new screenshots. Finally, we use the a11y tree or OmniParser V2 (Lu et al., 2024b) to localize UI elements precisely.

Perception Data Generation. To facilitate the diversity in the training data, for each screenshot, we randomly select one to three UI elements as inputs and prompt MLLMs to generate a variety of tasks based on these elements, including question answering, captioning, and state prediction. These diverse tasks allow the model to develop a comprehensive understanding of the interface, specifically enhancing its capabilities in semantic understanding, relation understanding, and layout understanding.

Grounding Data Generation. As demonstrated in our prior work in UI-Ins (Chen et al., 2025), instruction diversity and correctness are critical for GUI grounding but are often neglected:

- Instruction quality: Approximately  $23.3\%$  of instructions in open-source grounding datasets exhibit quality issues, which can actively harm model performance during training.  
- Instruction diversity: Performance improves significantly when training on diverse instruction perspectives, and improves further when the model learns to select appropriate instruction perspective in different scenarios.

Following our Instruction-as-Reasoning paradigm introduced in UI-Ins (Chen et al., 2025), we ask MLLMs to create instructions from four different human-like perspectives, including appearance, function, location and intent. This design is motivated by human UI grounding behavior: people strategically

![](images/47f66f1d5e10e4c32ac66e9545242873349908056cba863c6cc725af2e9ab078.jpg)  
Figure 3: Overview of grounding and perception data pipeline

switch among instruction perspectives, selecting the most informative one for the task to support effective reasoning. We use these instructions both as inputs and as explicit reasoning pathways, thereby injecting structured reasoning into the model. This training method is introduced in the section below.

# 2.2.2 Training Paradigm

Algorithm. To build a foundation model with strong grounding capability, we follow our Instruction-as-Reasoning paradigm in UI-Ins (Chen et al., 2025). Using the grounding data described above, we first perform supervised fine-tuning (SFT) to instill instruction-as-reasoning capability: utilizing diverse instruction perspectives as explicit analytical reasoning before predicting the coordinates. To encourage dynamic, context-aware selection of the appropriate reasoning perspective across different scenarios, we then conduct a reinforcement learning (RL) stage using the GRPO algorithm.

Reward. For GUI grounding, we use a combination of a format reward and a point-in-box reward. In our experiments, dense reward formulations yield similar performance, so we adopt this simple and effective scheme.

- Format Reward. We use format reward  $R_{f}$  verify that the model's response is in a valid format. Specifically, the thinking content and the final answer must be contained in their correct tags, and the coordinates must be extractable for the point-in-box evaluation.  
- Point-in-Box Reward. We utilize a direct point-in-box reward to measure correctness during training. A prediction is considered correct if the predicted coordinate point  $p = (x_p, y_p)$  falls within the ground-truth bounding box  $b = (x_l, y_l, x_r, y_r)$ , where the  $(x_l, y_l)$  denotes the top-left corner and  $(x_r, y_r)$  represents the bottom-right corner.

$$
R _ {a c c} = \left\{ \begin{array}{l l} 1 & \text {i f} x _ {l} \leq x _ {p} \leq x _ {r} \text {a n d} y _ {l} \leq y _ {p} \leq y _ {r}, \\ 0 & \text {o t h e r w i s e .} \end{array} \right. \tag {1}
$$

Zoom-In Strategy. During inference, we introduce an optional zoom-in strategy for complex and high-resolution GUI scenarios. In the first pass, the model predicts a coarse coordinate. We then crop a window centered on this point, with width and height equal to half of the original image dimensions, and resize the crop back to the original resolution. In the second pass, the model refines the prediction by outputting the precise coordinate on the zoomed region.

![](images/d62714a2e1f87d7482ce9cc2efd40a2ec3e3047c599242dccd54ffe241c4e1ad.jpg)  
Figure 4: Overview of the self-evolving data pipeline for trajectory synthesis. The pipeline comprises task generation, trajectory construction via human annotation and autonomous agent rollouts, and iterative rejection sampling that jointly evolve the model and the training corpus.

# 2.3 Mobile GUI Navigation

Beyond GUI perception and grounding in general setting, MAI-UI excels in mobile GUI navigation tasks. The training pipeline of GUI navigation consists of two main stages: supervised fine-tuning (SFT) and online reinforcement learning (RL). In the first stage, we build a self-evolving data pipeline to collect and synthesize diverse multi-step trajectories, and train the model with these trajectories to obtain strong navigation capability. In the second stage, we enhance the model's generalization to real-world scenarios through online reinforcement learning in dynamic environments. The following sections provide detailed descriptions of both components.

# 2.3.1 Supervised Fine-Tuning

MAI-UI employs a self-evolving SFT data pipeline (overview in Figure 4), comprising three key components: Navigation Task Generation, Trajectory Synthesis, and Iterative Rejection Sampling. In the Navigation Task Generation stage, we leverage multiple sources (APP manuals, expert-designed tasks, and open-source data) to construct high-quality seed tasks. In the Trajectory Synthesis stage, we first expand the seed tasks, then combine model-based synthesis and human annotation to generate diverse trajectories. The generated trajectories undergo two quality control steps: fine-grained correctness judgment via automated evaluation and annotation quality checks by human reviewers. This ensures both diversity and quality of the synthesized trajectories. In the final stage, Iterative Rejection Sampling, we initialize training with a set of Stage 2 trajectories as cold-start data to obtain an initial SFT model. We then alternate between fine-tuning the model and deploying the updated policy to rollout new trajectories. Newly generated trajectories are filtered via rejection sampling, retaining only high-quality examples aligned with the model's evolving capabilities. In parallel, we continually inject new trajectories from Stage 2 to broaden coverage and raise the performance ceiling. This closed loop of training and data synthesis makes both the model and the training corpus self-evolving.

Navigation Task Generation. In this stage, navigation task instructions are derived from three distinct sources: (1) application manuals, from which common usage scenarios are parsed and distilled into intent-level task descriptions; (2) expert-designed tasks, where human annotators formulate realistic and diverse mobile navigation goals aligned with commonly used scenarios; and (3) open-source datasets, which are filtered by task complexity and reachability. This multi-source strategy expands both task diversity and scale, while rigorous source selection and filtering ensures data quality.

Trajectory Synthesis. In the second stage, we first expand the set of tasks to increase diversity, and then generate trajectory data through parallel pipelines that leverage both model-based rollouts and human annotation.

- Seed task expansion. Starting from seed tasks, we prompt a multimodal large language model (MLLM) to generate a variety of novel tasks. We categorize this diversity into two levels: L1 adjusts critical parameters of the original task goal, and typical parameters include date/time ranges, numeric thresholds, sorting/filter criteria, etc; L2 replaces the core objects involved in the task while remaining constrained to the same scenario and set of applications.  
- Model synthesis and human annotation. Given the expanded set of tasks, we generate execution trajectories via two parallel pipelines: (1) human annotation: annotators manually perform tasks on an Android emulator and record both screenshots and ground-truth action sequences at each step. (2) model-based synthesis: since human annotation is time-consuming and costly, we also adopt multiple GUI agents to automatically produce valid action sequences for navigation tasks. Notably, for a given task goal, multiple valid execution paths often exist. By combining these complementary sources, we significantly broaden trajectory coverage and enhance dataset robustness.  
- Fine-grained correctness judgment and annotation quality validation. After generating diverse trajectories from both model synthesis and human annotation, we perform quality assessment through two independent validation pipelines:

1. Manual quality check: All human-annotated trajectories are reviewed by a second annotator who verifies alignment between the action sequence, screenshots, and the original task goal. Inconsistent or ambiguous demonstrations are either corrected or discarded.  
2. Fine-grained correctness judgment. Trajectories produced by GUI agents rollouts are examined by an MLLM-as-a-judge module (Gu et al., 2025a). The judge analyzes the task instruction, action history, and screenshots to assess correctness at both trajectory and step levels. Because some expanded tasks are infeasible and rollout models can struggle in complex tasks, many generated trajectories fail to fully complete the intended goal. However, failed trajectories often contain a substantial prefix of correct actions, with errors typically occurring only at intermediate or later steps. Recognizing that not all steps in a failed trajectory are erroneous, we adopt a fine-grained judging approach to identify and retain useful sub-trajectories.

The evaluation comprises two components: (1) Overall Trajectory Judgment: The MLLM-as-a-judge assesses end-to-end success, prioritizing visual evidence from screenshots over textual claims generated by the GUI agent; (2) Erroneous Trajectory Reuse: For failed trajectories, the judge identifies the longest prefix of correct actions before the first deviation. This enables reuse of failed trajectories, reducing data waste and enabling the model to learn from partial successes.

Iterative Rejection Sampling. We adopt an iterative self-improvement loop that progressively refines both the model and the trajectory data distribution. Let  $\mathcal{M}^{(t)}$  denote our model after the  $t$ -th round of fine-tuning, and let  $\mathcal{I}_{\text{expansion}}$  be the set of diverse task instructions from Diverse Task Expansion. In round  $t + 1$ , we use  $\mathcal{M}^{(t)}$  as the rollout policy to generate new trajectories on  $\mathcal{I}_{\text{expansion}}$ :

$$
\mathcal {D} _ {\mathrm {R S}} ^ {(t + 1)} = \left\{\text {R o l l o u t} \left(\mathcal {M} ^ {(t)}, i\right) \mid i \in \mathcal {I} _ {\text {e x p a n s i o n}} \right\}, \tag {2}
$$

where each rollout is filtered through the fine-grained correctness judgment module to retain only high-quality or partially correct segments. The training set for the next iteration is then constructed by mixing the newly generated rejection sampling data with novel trajectories synthesized from the Trajectory Synthesis stage:

$$
\mathcal {D} ^ {(t + 1)} = \cdot \mathcal {D} _ {\mathrm {R S}} ^ {(t + 1)} \cup \cdot \mathcal {D} _ {\text {s y n t h e s i s}}, \tag {3}
$$

where  $\mathcal{D}_{\mathrm{synthesis}}$  denotes the trajectories generated from the human annotation and the agent rollouts. The model  $\mathcal{M}^{(t + 1)}$  is then fine-tuned on  $\mathcal{D}^{(t + 1)}$ , completing the iteration. Rejection-sampled data helps close the gap between pass@1 and pass@N, while novel trajectories introduced in each iteration continually

![](images/af4fd09410d5c62002ba8200e615dba71cc40025675e84f74ac10aefdaf22a52.jpg)  
Figure 5: Overview of the agentic reinforcement learning framework. The framework alternates between rollout phases where the latest policy interacts with online mobile environments to generate trajectories and training phases that progressively improve the policy using trajectory-level rewards.

raise the pass@N performance ceiling. This process encourages the data distribution to gradually align with the model's evolving capabilities.

# 2.3.2 Enabling Agent-User Interaction and MCP Tool Use

To teach the model to interact with users and use MCP tools, we augment our self-evolving data pipeline with trajectories that explicitly cover agent-user interaction and MCP augmentation. These trajectories are mixed into SFT, enabling the model to learn when to elicit missing information from the user and when to use MCP tools to complete tasks efficiently.

Agent-user interaction. We construct tasks with deliberately omitted critical information. When the annotation/ rollout reaches to a step that requires missing information, it issues an ask_user action. The query is then routed to a synthetic user agent implemented with a standard LLM that is conditioned on hidden context containing the missing information. The user agent returns concise, context-appropriate replies, including clarifications, corrections, or refusals when applicable. We log the query-response pairs in the history and continue the annotation/ rollout so the trajectory can incorporate the returned information to complete the task. We generate both single-turn and multi-turn interactions.

MCP augmentation. We design tasks that require or benefit from external MCP tools (e.g., Amap, Github, Stockstart). During trajectory annotation or rollout, the annotator/agent can issue mcp_call with a tool name and arguments. The MCP server executes the call and returns structured outputs. We record tool schemas, arguments, results, and the subsequent UI actions, and we keep trajectories that demonstrate correct MCP tool selection.

# 2.3.3 Online Reinforcement Learning

To enhance the model's reliability in long-horizon tasks and dynamic real-world environments, we employ an agentic reinforcement learning (RL) framework integrated with an online GUI environment, as illustrated in Figure 5. This framework operates through two alternating phases that drive iterative improvement: (1) during the rollout phase, the model executes multi-turn interactions with the GUI environment to complete tasks and collect full execution trajectories; (2) in the training phase, the model performs end-to-end policy updates using trajectory-level rewards. Through this iterative process, each improved policy generates higher-quality rollouts for subsequent training, progressively strengthening the model's robustness and generalization capability.

![](images/fdf35f6d550dd1b1fc4681e9568b648d9a8f69727e74cb97f1520106ecfe6d6b.jpg)  
Figure 6: Detailed rollout process within the MobileAgentLoop. The agent loop asynchronously calls inference servers to generate actions and executes them in stateful environments across multiple turns, with hybrid verifiers evaluating complete trajectories to produce final rewards.

Scalable GUI Environment. A critical bottleneck in scaling agentic RL lies in efficiently scaling stateful environments. Unlike stateless environments for mathematical reasoning or code generation, GUI environments are inherently stateful and resource-intensive, requiring each rollout to operate within an isolated instance. This constraint motivates our use of virtualized environments rather than physical devices.

Inspired by an experimental feature in AndroidWorld (Rawles et al., 2024a), we built a containerized solution that encapsulates the entire GUI environment within a Docker image, comprising a rooted Android Virtual Device (AVD), self-hosted backend services, and a dedicated REST API server for orchestration. This solution is carefully designed to ensure the following three features:

- Consistency. The unified containerization eliminates external dependencies and guarantees behavioral consistency across heterogeneous host systems.  
- Generalizability. To build a general environment for mobile use, we integrated over 35 applications, encompassing system utilities and open-source software such as Mattermost (enterprise communication), Mastodon (social media), and Mall4Uni (e-commerce). Self-hosting these applications provides full backend access, enabling precise manipulation of initial task states and deterministic verification of execution outcomes.  
- RL-native Design. We employ an AVD snapshot mechanism for reproducible task initialization and expose standard RL primitives (reset, step, getobservation, evaluate, and close) through the containerized API server, enabling parallel deployment of emulator instances.

To further scale environments across distributed infrastructure, we introduce a centralized Environment Manager that coordinates container instances across multiple physical machines. This architecture serves three critical functions: (1) efficient resource utilization: through automatic container reuse, environments are reset and reassigned upon rollout completion instead of being destroyed, (2) cross-machine orchestration: the Manager exposes a unified REST API that provides transparent access to distributed resources across heterogeneous hosts, and (3) fault tolerance: we introduce automatic detection and recovery mechanisms to handle container failures, with failover protocols that seamlessly replace compromised instances from a standby pool. By coordinating just 10 standard Alibaba Cloud ECS servers (ecs.ebmg5s.24xlarge), the Manager supports up to 512 concurrent environment instances for parallel rollout execution, while maintaining high availability essential for continuous online RL training.

Long-Horizon RL. Training RL agents for long-horizon tasks faces two interconnected challenges: traditional synchronous rollout pipelines become inefficient bottlenecks due to extensive multi-turn environment interactions, and the resulting ultra-long trajectory (up to millions of tokens per trajectory) exceed single-GPU memory capacity, necessitating advanced parallelism strategies to enable end-to-end

policy training. To address these challenges, we employ a strict on-policy, asynchronous RL training framework built on top of verl (Sheng et al., 2024), illustrated in Figure 6, with two key optimizations:

- Asynchronous Rollout for Multi-Turn Efficiency. We implement a custom agent loop that asynchronously dispatches requests to a group of inference servers hosting the latest policy model, thereby mitigating GPU idling during environment interactions. The agent loop further incorporates asynchronous environment interaction with session management, maintaining backup sessions for seamless failover and replacement. On the server side, we employ load balancing and prefetching to accelerate generation efficiency in multi-turn settings.  
- Hybrid Parallelism for Ultra-Long Sequences. To support end-to-end training of trajectories with millions of tokens, we leverage Megatron's hybrid multi-dimensional parallelism (TP+PP+CP) to shard each long rollout trajectory across GPUs along the tensor, pipeline, and context dimensions, enabling scalable training while keeping per-GPU memory bounded. Additionally, we resize images to half their original resolution, which significantly improves training efficiency without compromising model performance.

Task and Verifier Design. Effective RL training requires a well-structured task distribution that balances exploration and exploitation. We manually curate a diverse set of over 35 applications spanning simple single-app operations to complex multi-app workflows. Tasks are dynamically stratified into four difficulty levels based on the current policy's pass@K success rate (SR): frontier tasks (0–25% SR) push model capability boundaries, exploration tasks (25–50% SR) drive skill development, near-mastery tasks (50–75% SR) approach proficiency, and exploitation tasks (75–100% SR) reinforce learned behaviors.

Building on this stratification, we implement an automatic curriculum that progressively adjusts task sampling throughout training. Early stages emphasize simpler tasks to establish foundational skills, while the distribution gradually shifts toward challenging tasks as success rates improve. This adaptive strategy prevents training collapse from excessive difficulty while ensuring continuous learning signals, effectively addressing the exploration-exploitation tradeoff.

To enable scalable evaluation, we develop a hybrid verification approach tailored to task characteristics. Deterministic tasks with clear success criteria use rule-based verifiers with root-level AVD access for precise state verification. For complex tasks where rule-based verification is labor-intensive, we employ an MLLM-as-a-Judge framework to evaluate execution trajectories against task objectives. This hybrid approach achieves  $83\%$  agreement with human annotations, enabling reliable large-scale verification without manual bottlenecks.

Training Algorithm. we adopt a tailored GRPO to sample a group of outputs  $\{o_i\}_{i = 1}^G$  for each task  $q$ , and optimizes the policy via the following objective:

$$
\mathcal {J} _ {\mathrm {G R P O}} (\theta) = \mathbb {E} _ {(q) \sim \mathcal {D}, \{o _ {i} \} _ {i = 1} ^ {G} \sim \pi_ {\theta_ {\mathrm {o l d}}} (\cdot | q)} \left[ \frac {1}{\sum_ {c = 1} ^ {G} | o _ {c} |} \sum_ {i = 1} ^ {G} \sum_ {t = 1} ^ {| o _ {i} |} \min \left(r _ {i, t} (\theta) \hat {A} _ {i, t}, \operatorname {c l i p} \left(r _ {i, t} (\theta), 1 - \varepsilon_ {\mathrm {l o w}}, 1 + \varepsilon_ {\mathrm {h i g h}}\right) \hat {A} _ {i, t}\right) \right]
$$

where  $r_{i,t}(\theta) = \frac{\pi_{\theta}(o_{i,t}|q,o_{i, < t})}{\pi_{\theta_{\mathrm{old}}}(o_{i,t}|q,o_{i, < t})}$  is the importance sampling ratio, and  $\hat{A}_{i,t} = \frac{R_i - \mathrm{mean}(\{R_i\}_{i=1}^G)}{\mathrm{std}(\{R_i\}_{i=1}^G)}$  is the normalized advantage. We find a group size of 16 strikes a good balance between effectiveness and efficiency.

We additionally incorporate the following features to encourage exploration and improved stability:

- Reward Design. The reward signal comprises two components: a task completion reward and an action-level repetition penalty. Task completion is measured as a binary indicator of successful execution, determined by either a rule-based verifier or the MLLM-as-a-Judge framework described above. To discourage unproductive looping, we penalize recurring action sequences (from single repeated actions to cyclic patterns of 3-5 actions). Actions with identical types but different parameters are not penalized, enabling flexible execution while preventing non-progressive behavioral loops.  
- Clip Higher. Following DAPO (Yu et al., 2025), we employ the token-level loss with no KL divergence

![](images/f661e4cbb718a775e67e2e74f09feba11743aecb488df348ed28407719d55784.jpg)  
Figure 7: Overview of device-cloud collaboration architecture. The system adaptively routes computation between device and cloud models based on task context and data sensitivity.

and an asymmetric clipping strategy with a larger upper bound to encourage exploration. Specifically, we set  $\varepsilon_{\mathrm{low}}$  to 0.2 and  $\varepsilon_{\mathrm{high}}$  to 0.3.

- Experience Replay. We maintain a replay buffer of successful trajectories collected during training. When a rollout group contains no successful completions, we augment it with randomly sampled trajectories from the buffer. The buffer is continuously updated with newly successful experiences, retaining only the most recent eight trajectories per task to maintain near on-policy learning. This mechanism ensures continuous learning signals even during challenging exploration phases, stabilizing training and accelerating convergence.

# 2.4 Device-Cloud Collaboration

Building on the above training process, we can obtain high-capacity agents for cloud serving and lightweight yet capable on-device agents. However, neither mode alone fully meets the requirements of real-world deployment. On-device solutions are constrained by model size and thus exhibit limited GUI agent capability. Cloud deployments suffer from high latency, privacy risks, and network dependence. To address these limitations, we introduce a native device-cloud collaborative architecture that adaptively routes computation between device and cloud based on task context and data sensitivity.

# 2.4.1 System Architecture

The overall structure of our device-cloud collaboration system is illustrated in Figure 7. The system consists of a Local GUI Agent, which exhibit the ability of both GUI agent and trajectory monitor, a Cloud GUI Agent, and a Local Unified Trajectory Memory that maintains consistent information exchange between local and cloud agents. The specific function of these modules are demonstrated below:

- Local Agent. The Local Agent runs on device and functions as both a GUI agent and a monitor. As a GUI agent, it perceives the current screen, and generates actions for each step of the task. As a monitor, it evaluates whether the trajectory so far remains aligned with the user instruction. The monitor checks indicators such as action execution failure, repeated actions without progress, incorrect inputs, or general task deviations. If the trajectory deviates from user instruction and the current context does not contain privacy-sensitive data, the monitor triggers a switch to the Cloud Agent. Additionally, the monitor generates an error summary each time a deviation is detected, which we find highly effective for trajectory recovery.  
- Cloud Agent. The Cloud Agent is called only when the monitor detects trajectory deviation. In addition to the standard GUI agent inputs, it receives an error summary from the monitor that explains why the switch was triggered. Given the trajectory history and error summary, the Cloud Agent

executes subsequent steps, leveraging its higher capacity to complete the task.

- Local Unified Trajectory Memory. On device, we maintain a unified history that records the task instruction, historical screenshots, and the model's past outputs, including thoughts and actions. During action execution, the memory module projects the unified history into the action spaces expected by the device and cloud models, enabling either model can resume from any state without ambiguity.  
- Execution loop. The user provides a task instruction to the Local Agent. At each step, the Local Agent observes the current screenshot, decides an action, and executes it. The environmental observations and model outputs are then written to the Local Unified Trajectory Memory. Every few steps, the local agent assesses alignment between user instruction and the trajectory so far. If alignment is met, the loop continues on device. If deviation is detected and no sensitive data is involved, the system calls the Cloud Agent for task completion.

# 2.4.2 Local Agent Training

We train a single on-device model that unifies two roles: an agent for GUI navigation, and a trajectory monitor for alignment assessment. Compared with prior work (Jiang & Huang, 2025), our Local Agent introduces two key innovations required for practical device-side deployment:

- Integrated monitoring capability. In practice, the monitor must handle varied and complex cases, and prompt engineering alone is unlikely to deliver reliable monitoring. The on-device model is explicitly trained to judge whether the trajectory so far remains aligned with the user instruction across diverse apps, layouts, and tasks.  
- Error feedback generation. When deviation is detected, our model further generates a concise error summary to guide trajectory recovery. This signal is crucial for the Cloud Agent to complete the task correctly, since handoffs occur only after the trajectory has already deviated from the user instruction

Training procedure. We train the Local Agent jointly on two data sources: (i) standard GUI agent data covering perception, grounding, and navigation, and (ii) monitor data that include alignment reasoning, alignment decision, and error summaries. This multi-task training recipe teaches the on-device model to execute and monitor simultaneously, without requiring separate models or fragile prompt engineering.

# 2.5 MobileWorld Benchmark

Existing benchmarks for mobile GUI agents often fail to capture the complexity of real-world mobile usage. Many evaluations rely on simple applications or limited app categories, and they typically assume idealized interaction models where user instructions are perfectly clear and agents operate solely through GUI manipulation (Rawles et al., 2024b; Xu et al., 2025a). This gap between benchmark performance and real-world utility has become increasingly apparent. To evaluate MAI-UI's practical capabilities, we adopt our MOBILEWORLD (Kong et al., 2025) benchmark, a comprehensive benchmark designed to bridge this evaluation gap. MOBILEWORLD features over 200 realistic tasks spanning  $15+$  open-source applications across critical domains including e-commerce (Mall4Uni, mirroring Temu/Amazon), enterprise communication (Mattermost, mirroring Microsoft Teams/Slack), social media (Mastodon, mirroring X/Twitter), and daily productivity tools.

In addition, MOBILEWORLD extends beyond standard GUI manipulation to evaluate two essential real-world capabilities, which directly align with MAI-UI's design of practical deployment.

- Agent-User Interaction, where agents must detect ambiguous user requests and proactively seek clarification rather than making incorrect assumptions.  
- MCP Tool Integration, where agents must intelligently decide between manual GUI navigation and API-based operations via MCP (Anthropic, 2024) to optimize efficiency.

Table 2: Performance comparison on the ScreenSpot-Pro benchmark. We use  $^{\prime \prime}$  to denote the results evaluated by us. The best results are highlighted in bold, and the second-best results are underlined.  

<table><tr><td rowspan="2">Model</td><td colspan="2">CAD</td><td colspan="2">Dev.</td><td colspan="2">Creative</td><td colspan="2">Scientific</td><td colspan="2">Office</td><td colspan="2">OS</td><td rowspan="2">Avg.</td></tr><tr><td>Text</td><td>Icon</td><td>Text</td><td>Icon</td><td>Text</td><td>Icon</td><td>Text</td><td>Icon</td><td>Text</td><td>Icon</td><td>Text</td><td>Icon</td></tr><tr><td colspan="14">Proprietary Models</td></tr><tr><td>GPT-4o (OpenAI, 2024)</td><td>2.0</td><td>0.0</td><td>1.3</td><td>0.0</td><td>1.0</td><td>0.0</td><td>2.1</td><td>0.0</td><td>1.1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.8</td></tr><tr><td>Claude C. (Anthropic, 2024)</td><td>14.5</td><td>3.7</td><td>22.0</td><td>3.9</td><td>25.9</td><td>3.4</td><td>33.9</td><td>15.8</td><td>30.1</td><td>16.3</td><td>11.0</td><td>4.5</td><td>17.1</td></tr><tr><td>Gemini-3-Pro (DeepMind, 2025b)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>72.7</td></tr><tr><td>Seed1.8 (Seed, 2025a)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>73.1</td></tr><tr><td colspan="14">Open-Source Models</td></tr><tr><td>Qwen3-VL-2B* (Bai et al., 2025a)</td><td>31.0</td><td>15.6</td><td>55.2</td><td>11.7</td><td>59.1</td><td>16.1</td><td>64.6</td><td>22.7</td><td>72.3</td><td>34.0</td><td>59.8</td><td>23.6</td><td>41.9</td></tr><tr><td>InfiGUI-3B (Liu et al., 2025b)</td><td>50.8</td><td>25.0</td><td>64.9</td><td>20.0</td><td>51.5</td><td>16.8</td><td>68.8</td><td>32.7</td><td>70.6</td><td>32.1</td><td>49.5</td><td>19.7</td><td>45.2</td></tr><tr><td>Ferret-UI Lite (Yang et al., 2025b)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>53.3</td></tr><tr><td>UI-TARS-7B (Qin et al., 2025a)</td><td>20.8</td><td>9.4</td><td>58.4</td><td>12.4</td><td>50.0</td><td>9.1</td><td>63.9</td><td>31.8</td><td>63.3</td><td>20.8</td><td>30.8</td><td>16.9</td><td>35.7</td></tr><tr><td>Phi-Ground (Zhang et al., 2025a)</td><td>26.9</td><td>17.2</td><td>70.8</td><td>16.7</td><td>56.6</td><td>13.3</td><td>58.0</td><td>29.1</td><td>76.4</td><td>44.0</td><td>55.1</td><td>25.8</td><td>43.2</td></tr><tr><td>GUI-Actor-7B (Wu et al., 2025)</td><td>47.7</td><td>9.4</td><td>59.1</td><td>15.9</td><td>59.6</td><td>16.1</td><td>70.1</td><td>25.5</td><td>69.5</td><td>41.5</td><td>55.1</td><td>19.1</td><td>44.6</td></tr><tr><td>SE-GUI-7B (Yuan et al., 2025)</td><td>51.3</td><td>14.1</td><td>68.2</td><td>19.3</td><td>57.6</td><td>9.1</td><td>75.0</td><td>28.2</td><td>78.5</td><td>43.4</td><td>49.5</td><td>25.8</td><td>47.2</td></tr><tr><td>GUI-G2-7B (Tang et al., 2025)</td><td>55.8</td><td>12.5</td><td>68.8</td><td>17.2</td><td>57.1</td><td>15.4</td><td>77.1</td><td>24.5</td><td>74.0</td><td>32.7</td><td>57.9</td><td>21.3</td><td>47.5</td></tr><tr><td>Qwen3-VL-8B* (Bai et al., 2025a)</td><td>46.7</td><td>10.9</td><td>79.2</td><td>23.4</td><td>68.2</td><td>14.0</td><td>73.6</td><td>30.0</td><td>76.3</td><td>30.2</td><td>65.4</td><td>21.3</td><td>49.9</td></tr><tr><td>OpenCUA-7B (Wang et al., 2025b)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>50.0</td></tr><tr><td>GTA1-7B (Yang et al., 2025a)</td><td>53.3</td><td>17.2</td><td>66.9</td><td>20.7</td><td>62.6</td><td>18.9</td><td>76.4</td><td>31.8</td><td>82.5</td><td>50.9</td><td>48.6</td><td>25.9</td><td>50.1</td></tr><tr><td>UI-Venus-7B (Gu et al., 2025b)</td><td>60.4</td><td>21.9</td><td>74.7</td><td>24.1</td><td>63.1</td><td>14.7</td><td>76.4</td><td>31.8</td><td>75.7</td><td>41.5</td><td>49.5</td><td>22.5</td><td>50.8</td></tr><tr><td>InfiGUI-G1-7B (Liu et al., 2025b)</td><td>57.4</td><td>23.4</td><td>74.7</td><td>24.1</td><td>64.6</td><td>18.2</td><td>80.6</td><td>31.8</td><td>75.7</td><td>39.6</td><td>57.0</td><td>29.2</td><td>51.9</td></tr><tr><td>GUI-Owl-7B (Ye et al., 2025)</td><td>64.5</td><td>21.9</td><td>76.6</td><td>31.0</td><td>59.6</td><td>27.3</td><td>79.1</td><td>37.3</td><td>77.4</td><td>39.6</td><td>59.8</td><td>33.7</td><td>54.9</td></tr><tr><td>Qwen3-VL-32B* (Bai et al., 2025a)</td><td>60.4</td><td>28.1</td><td>69.5</td><td>22.1</td><td>75.8</td><td>25.2</td><td>84.7</td><td>25.5</td><td>85.9</td><td>43.4</td><td>62.6</td><td>15.7</td><td>54.9</td></tr><tr><td>OpenCUA-32B (Wang et al., 2025b)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>55.3</td></tr><tr><td>GUI-Owl-32B (Ye et al., 2025)</td><td>62.4</td><td>28.1</td><td>84.4</td><td>39.3</td><td>65.2</td><td>18.2</td><td>82.6</td><td>39.1</td><td>81.4</td><td>39.6</td><td>70.1</td><td>36.0</td><td>58.0</td></tr><tr><td>GTA1-32B (Yang et al., 2025a)</td><td>43.7</td><td>23.4</td><td>82.5</td><td>28.3</td><td>69.2</td><td>14.7</td><td>79.9</td><td>31.8</td><td>80.8</td><td>43.4</td><td>70.1</td><td>32.6</td><td>63.6</td></tr><tr><td>UGround-v1-72B (Gou et al., 2025)</td><td>16.8</td><td>4.7</td><td>55.8</td><td>4.8</td><td>54.0</td><td>10.5</td><td>70.8</td><td>22.7</td><td>61.0</td><td>18.9</td><td>40.2</td><td>7.9</td><td>34.5</td></tr><tr><td>UI-Tars-72B (Qin et al., 2025a)</td><td>18.8</td><td>12.5</td><td>63.0</td><td>17.2</td><td>57.0</td><td>15.4</td><td>64.6</td><td>20.9</td><td>63.3</td><td>26.4</td><td>42.1</td><td>15.7</td><td>38.1</td></tr><tr><td>UI-Venus-72B (Gu et al., 2025c)</td><td>66.5</td><td>29.7</td><td>84.4</td><td>33.1</td><td>73.2</td><td>30.8</td><td>84.7</td><td>42.7</td><td>83.1</td><td>60.4</td><td>75.7</td><td>36.0</td><td>61.9</td></tr><tr><td colspan="14">Ours</td></tr><tr><td>MAI-UI-2B</td><td>61.4</td><td>23.4</td><td>76.6</td><td>32.4</td><td>69.2</td><td>21.7</td><td>81.2</td><td>34.5</td><td>85.9</td><td>39.6</td><td>68.2</td><td>41.6</td><td>57.4</td></tr><tr><td>+ Zoom-In</td><td>69.5</td><td>34.4</td><td>75.3</td><td>42.8</td><td>74.7</td><td>30.1</td><td>84.0</td><td>42.7</td><td>85.3</td><td>56.6</td><td>69.2</td><td>47.2</td><td>62.8</td></tr><tr><td>MAI-UI-8B</td><td>72.6</td><td>35.9</td><td>83.8</td><td>52.4</td><td>76.3</td><td>33.6</td><td>79.9</td><td>37.3</td><td>88.7</td><td>60.4</td><td>76.6</td><td>49.4</td><td>65.8</td></tr><tr><td>+ Zoom-In</td><td>80.7</td><td>43.8</td><td>78.6</td><td>58.6</td><td>78.8</td><td>46.9</td><td>86.1</td><td>49.1</td><td>88.1</td><td>81.1</td><td>76.6</td><td>51.7</td><td>70.9</td></tr><tr><td>MAI-UI-32B</td><td>70.1</td><td>45.3</td><td>86.4</td><td>40.7</td><td>82.8</td><td>37.8</td><td>91.7</td><td>46.4</td><td>90.4</td><td>71.7</td><td>78.5</td><td>34.8</td><td>67.9</td></tr><tr><td>+ Zoom-In</td><td>79.2</td><td>53.1</td><td>84.4</td><td>57.9</td><td>87.9</td><td>46.2</td><td>91.7</td><td>54.5</td><td>88.1</td><td>79.2</td><td>80.4</td><td>47.2</td><td>73.5</td></tr></table>

# 3 Experiments

We present comprehensive experiments in this section, including extensive benchmark evaluations and detailed ablations and analyses of the main components of MAI-UI. Specifically, we introduce the experimental setup (Sec. 3.1); report the main benchmark results across GUI grounding, GUI navigation, and real-world-oriented evaluation (Sec. 3.2); present case studies of MCP augmentation and agent-user interaction (Sec. 3.3); demonstrate experiments and case studies on the native device-cloud collaboration system (Sec. 3.4); discuss online RL ablations (Sec. 3.5); and, finally, provide grounding analysis (Sec. 3.6).

# 3.1 Experimental Setup

Implementation details MAI-UI uses Qwen3-VL (Bai et al., 2025a) as the backbone across multiple model sizes (2B, 8B, 32B, and a 235B-A22B) to meet realistic deployment needs. Training proceeds in four stages: (i) SFT on perception and grounding data, (ii) SFT on mobile-use navigation data with a small portion of grounding data, (iii) RL for grounding to develop robust UI grounding capability, and (iv) online RL for mobile-use navigation in dynamic environments to enhance robustness and generalization. To further enhance the large cloud model variants, we augment the training of the 32B and 235B-A22B models with more real-world trajectories.

Table 3: Performance comparison on UI-Vision grounding dataset. The best results are highlighted in bold, and the second-best results are underlined.  

<table><tr><td>Models</td><td>Basic</td><td>Functional</td><td>Spatial</td><td>Avg</td></tr><tr><td colspan="5">Proprietary Models</td></tr><tr><td>GPT-4o (OpenAI, 2024)</td><td>1.6</td><td>1.5</td><td>1.0</td><td>1.4</td></tr><tr><td>Claude-3.7-Sonnet (Anthropic, 2024)</td><td>9.5</td><td>7.7</td><td>7.6</td><td>8.3</td></tr><tr><td colspan="5">Open-source Models</td></tr><tr><td>Qwen3-VL-2B* (Bai et al., 2025a)</td><td>0.0</td><td>19.2</td><td>0.1</td><td>6.2</td></tr><tr><td>InfiGUI-G1-3B (Liu et al., 2025b)</td><td>31.2</td><td>28.0</td><td>8.2</td><td>22.0</td></tr><tr><td>Qwen2.5-VL-7B (Bai et al., 2025b)</td><td>1.2</td><td>0.8</td><td>0.5</td><td>0.9</td></tr><tr><td>SeeClick (Cheng et al., 2024)</td><td>9.4</td><td>4.7</td><td>2.1</td><td>5.4</td></tr><tr><td>UGround-V1-7B (Gou et al., 2025)</td><td>15.4</td><td>17.1</td><td>6.3</td><td>12.9</td></tr><tr><td>OS-Atlas-7B (Wu et al., 2024)</td><td>12.2</td><td>11.2</td><td>3.7</td><td>9.0</td></tr><tr><td>Qwen3-VL-8B* (Bai et al., 2025a)</td><td>25.0</td><td>27.9</td><td>1.2</td><td>17.5</td></tr><tr><td>UI-TARS-7B (Qin et al., 2025b)</td><td>20.1</td><td>24.3</td><td>8.4</td><td>17.6</td></tr><tr><td>UI-TARS-1.5-7B (Seed, 2025b)</td><td>28.8</td><td>27.5</td><td>10.7</td><td>22.3</td></tr><tr><td>InfiGUI-G1-7B (Liu et al., 2025b)</td><td>36.2</td><td>31.9</td><td>11.5</td><td>26.1</td></tr><tr><td>UI-Venus-7B (Gu et al., 2025b)</td><td>36.1</td><td>32.8</td><td>11.9</td><td>26.5</td></tr><tr><td>Phi-Ground (Zhang et al., 2025a)</td><td>36.8</td><td>37.1</td><td>7.6</td><td>27.2</td></tr><tr><td>Qwen3-VL-32B* (Bai et al., 2025a)</td><td>32.8</td><td>34.2</td><td>14.7</td><td>26.9</td></tr><tr><td>UI-TARS-72B (Qin et al., 2025a)</td><td>31.4</td><td>30.5</td><td>14.7</td><td>25.5</td></tr><tr><td>UI-Venus-72B (Gu et al., 2025b)</td><td>45.6</td><td>42.3</td><td>23.7</td><td>36.8</td></tr><tr><td colspan="5">Ours</td></tr><tr><td>MAI-UI-2B</td><td>41.0</td><td>41.2</td><td>10.4</td><td>30.3</td></tr><tr><td>+ Zoom-In</td><td>43.2</td><td>43.0</td><td>11.3</td><td>31.9</td></tr><tr><td>MAI-UI-8B</td><td>51.7</td><td>49.6</td><td>22.5</td><td>40.7</td></tr><tr><td>+ Zoom-In</td><td>51.6</td><td>50.5</td><td>26.6</td><td>42.4</td></tr><tr><td>MAI-UI-32B</td><td>59.1</td><td>57.1</td><td>26.9</td><td>47.1</td></tr><tr><td>+ Zoom-In</td><td>58.7</td><td>56.8</td><td>33.6</td><td>49.2</td></tr></table>

Benchmarks We evaluate MAI-UI across extensive benchmarks spanning three categories: grounding, mobile-use, and real-world-oriented evaluation.

- Grounding benchmarks. We evaluate grounding with five complementary benchmarks: ScreenSpot-Pro (Li et al., 2025) for high-resolution, fine-grained professional layouts, UI-Vision (Nayak et al., 2025) for diverse applications and reasoning types (e.g., spatial and functional), MMBench-GUI L2 (Xuehui Wang et al., 2025) for hierarchical-instruction following and compositional reasoning, OSWorld-G and OSWorld-G Refine (Xie et al., 2025b) for comprehensive skills such as layout understanding, widget matching, and fine-grained manipulation, and ScreenSpot-V2 (Wu et al., 2024) to broaden coverage across different operating systems.  
- Mobile-use benchmarks. We report offline and online results. Offline evaluation includes Android Control (Li et al., 2024), which evaluates planning and action execution capabilities in the mobile setting, and GUI Odyssey (Lu et al., 2024a), which covers cross-app navigation tasks. For online evaluation, AndroidWorld (Rawles et al., 2024a) provides 116 tasks across 20 Android apps in a live Android emulator, and requires continuous interaction with dynamic mobile environment.  
- Real-world-oriented evaluation. We introduce MobileWorld (Kong et al., 2025), a more challenging, dynamic benchmark that closely mirrors production usage. It includes tasks requiring agent-user interaction and MCP tool use, enabling rigorous evaluation of these two capabilities that are critical in real-world settings.

# 3.2 Main Results

# 3.2.1 Grounding Capability

Overall Grounding Performance. We evaluate MAI-UI on five comprehensive GUI grounding benchmarks. Across all scales, our 2B, 8B, and 32B variants consistently outperform models of comparable

Table 4: Performance comparison on the MMBench-GUI L2 benchmark. The best results are highlighted in bold, and the second-best results are underlined.  

<table><tr><td rowspan="2">Model</td><td colspan="2">Windows</td><td colspan="2">MacOS</td><td colspan="2">Linux</td><td colspan="2">iOS</td><td colspan="2">Android</td><td colspan="2">Web</td><td rowspan="2">Avg.</td></tr><tr><td>Bas.</td><td>Adv.</td><td>Bas.</td><td>Adv.</td><td>Bas.</td><td>Adv.</td><td>Bas.</td><td>Adv.</td><td>Bas.</td><td>Adv.</td><td>Bas.</td><td>Adv.</td></tr><tr><td colspan="14">Proprietary Models</td></tr><tr><td>GPT-4o (OpenAI, 2024)</td><td>1.5</td><td>1.1</td><td>8.7</td><td>4.3</td><td>1.1</td><td>1.0</td><td>5.1</td><td>3.3</td><td>2.5</td><td>1.4</td><td>3.2</td><td>2.9</td><td>2.9</td></tr><tr><td>Claude-3.7 (Anthropic, 2024)</td><td>1.5</td><td>0.7</td><td>12.5</td><td>7.5</td><td>1.1</td><td>0.0</td><td>13.7</td><td>10.6</td><td>1.4</td><td>1.4</td><td>3.2</td><td>2.3</td><td>4.7</td></tr><tr><td>Qwen-Max-VL (Yang et al., 2024a)</td><td>43.9</td><td>36.8</td><td>58.8</td><td>56.1</td><td>53.9</td><td>30.1</td><td>77.4</td><td>59.1</td><td>79.5</td><td>70.1</td><td>74.8</td><td>58.8</td><td>58.0</td></tr><tr><td colspan="14">Open-Source Models</td></tr><tr><td>Qwen3-VL-2B* (Bai et al., 2025a)</td><td>81.9</td><td>0.0</td><td>80.3</td><td>46.2</td><td>67.5</td><td>0.0</td><td>90.8</td><td>0.0</td><td>91.0</td><td>0.0</td><td>88.1</td><td>0.0</td><td>46.5</td></tr><tr><td>OS-Agvis-7B (Wu et al., 2024)</td><td>36.9</td><td>18.8</td><td>44.4</td><td>21.7</td><td>31.4</td><td>13.3</td><td>74.8</td><td>48.8</td><td>69.6</td><td>46.8</td><td>61.3</td><td>35.4</td><td>41.4</td></tr><tr><td>Aguvis-7B (Xu et al., 2025b)</td><td>37.3</td><td>21.7</td><td>48.1</td><td>33.3</td><td>33.5</td><td>25.0</td><td>67.5</td><td>65.2</td><td>61.0</td><td>51.0</td><td>61.6</td><td>45.5</td><td>45.7</td></tr><tr><td>UI-TARS-1.5-7B (Seed, 2025b)</td><td>68.3</td><td>39.0</td><td>69.0</td><td>44.5</td><td>64.4</td><td>37.8</td><td>88.5</td><td>69.4</td><td>90.5</td><td>69.3</td><td>81.0</td><td>56.5</td><td>64.3</td></tr><tr><td>UGround-V1-7B (Gou et al., 2025)</td><td>66.8</td><td>39.0</td><td>71.3</td><td>48.6</td><td>56.5</td><td>31.1</td><td>92.7</td><td>70.9</td><td>93.5</td><td>71.0</td><td>88.7</td><td>64.6</td><td>65.7</td></tr><tr><td>GUI-Actor-7B* (Wu et al., 2025)</td><td>80.8</td><td>55.1</td><td>81.4</td><td>60.4</td><td>64.9</td><td>41.8</td><td>94.3</td><td>82.7</td><td>93.5</td><td>79.7</td><td>89.7</td><td>72.1</td><td>76.5</td></tr><tr><td>SE-GUI-7B* (Yuan et al., 2025)</td><td>77.5</td><td>57.7</td><td>77.1</td><td>60.7</td><td>68.6</td><td>44.9</td><td>95.5</td><td>80.0</td><td>95.5</td><td>83.7</td><td>89.7</td><td>68.8</td><td>76.6</td></tr><tr><td>Qwen3-VL-8B* (Bai et al., 2025a)</td><td>88.6</td><td>61.8</td><td>85.5</td><td>69.1</td><td>74.9</td><td>53.1</td><td>95.2</td><td>82.4</td><td>95.5</td><td>84.5</td><td>96.8</td><td>72.1</td><td>81.3</td></tr><tr><td>GTA1-7B* (Yang et al., 2025a)</td><td>76.8</td><td>57.4</td><td>80.3</td><td>63.9</td><td>68.6</td><td>53.6</td><td>93.9</td><td>83.3</td><td>96.3</td><td>84.5</td><td>90.3</td><td>74.7</td><td>78.5</td></tr><tr><td>GUI-G2-7B* (Tang et al., 2025)</td><td>79.7</td><td>55.1</td><td>79.7</td><td>64.7</td><td>69.6</td><td>50.0</td><td>95.2</td><td>82.7</td><td>96.6</td><td>85.4</td><td>91.9</td><td>75.6</td><td>78.8</td></tr><tr><td>GUI-Owl-7B (Ye et al., 2025)</td><td>86.4</td><td>61.8</td><td>81.7</td><td>64.5</td><td>74.4</td><td>61.7</td><td>94.9</td><td>83.0</td><td>95.8</td><td>83.7</td><td>93.2</td><td>72.7</td><td>80.5</td></tr><tr><td>InfiGUI-G1-7B (Liu et al., 2025b)</td><td>82.7</td><td>61.8</td><td>83.8</td><td>63.9</td><td>72.3</td><td>52.0</td><td>94.9</td><td>89.4</td><td>95.2</td><td>85.6</td><td>93.5</td><td>76.3</td><td>80.8</td></tr><tr><td>GUI-Owl-32B (Ye et al., 2025)</td><td>85.6</td><td>65.1</td><td>84.9</td><td>67.1</td><td>77.0</td><td>63.3</td><td>95.2</td><td>85.5</td><td>96.1</td><td>87.0</td><td>95.5</td><td>80.8</td><td>83.0</td></tr><tr><td>GTA1-32B* (Yang et al., 2025a)</td><td>82.3</td><td>66.9</td><td>89.0</td><td>74.0</td><td>73.3</td><td>52.0</td><td>96.2</td><td>88.2</td><td>95.8</td><td>88.5</td><td>95.2</td><td>79.9</td><td>83.4</td></tr><tr><td>Qwen3-VL-32B* (Bai et al., 2025a)</td><td>93.4</td><td>71.3</td><td>92.8</td><td>74.3</td><td>78.0</td><td>56.1</td><td>95.5</td><td>88.8</td><td>97.2</td><td>88.5</td><td>92.6</td><td>78.6</td><td>85.3</td></tr><tr><td>UI-TARS-DPO-72B (Qin et al., 2025a)</td><td>78.6</td><td>51.8</td><td>80.3</td><td>62.7</td><td>68.6</td><td>51.5</td><td>90.8</td><td>81.2</td><td>93.0</td><td>80.0</td><td>88.1</td><td>68.5</td><td>74.3</td></tr><tr><td>InternVL3-78B (Zhu et al., 2025)</td><td>70.1</td><td>42.6</td><td>75.7</td><td>52.3</td><td>59.2</td><td>41.3</td><td>93.6</td><td>80.6</td><td>92.7</td><td>78.6</td><td>90.7</td><td>65.9</td><td>72.2</td></tr><tr><td colspan="14">Ours</td></tr><tr><td>MAI-UI-2B</td><td>84.9</td><td>64.0</td><td>89.3</td><td>72.5</td><td>75.4</td><td>60.2</td><td>95.2</td><td>85.2</td><td>96.3</td><td>84.2</td><td>92.9</td><td>76.0</td><td>82.6</td></tr><tr><td>MAI-UI-8B</td><td>92.3</td><td>74.3</td><td>90.7</td><td>86.4</td><td>81.2</td><td>67.3</td><td>97.1</td><td>90.0</td><td>97.5</td><td>92.7</td><td>95.8</td><td>86.0</td><td>88.8</td></tr><tr><td>MAI-UI-32B</td><td>93.0</td><td>78.7</td><td>92.8</td><td>87.6</td><td>86.9</td><td>77.6</td><td>97.1</td><td>92.4</td><td>98.0</td><td>93.2</td><td>96.1</td><td>92.5</td><td>91.3</td></tr></table>

size and establish new state-of-the-art results. On ScreenSpot-Pro (Li et al., 2025), MAI-UI-32B attains  $67.9\%$  accuracy, an  $4.1\%$  absolute accuracy gain over the strongest baseline GTA1-32B. With the adaptive zoom-in strategy, performance further increases to  $73.5\%$ , surpassing Gemini 3 Pro (DeepMind, 2025b) and Seed1.8 (Seed, 2025a). On OSWorld-G (Xie et al., 2025b), our models show consistent improvements over the best comparable baselines. Specifically, MAI-UI-32B with zoom-in achieves  $70.9\%$  on OSWorld-G and  $75.0\%$  on OSWorld-G Refine. On UI-Vision (Nayak et al., 2025), MAI-UI-32B achieves  $47.1\%$  accuracy, and it increases to  $49.2\%$  with the zoom-in strategy, making an absolute gain of  $+12.4$  points over the previous best UI-Venus-Ground-72B. On MMBench-GUI L2 (Xuehui Wang et al., 2025), MAI-UI reaches  $91.3\%$ , surpassing the prior best GTA1 by  $+7.9$  points. On ScreenSpot-V2 (Wu et al., 2024), MAI-UI sets a new SOTA at  $96.5\%$ . It is also worth noting that MAI-UI-2B, despite its small size, demonstrates strong grounding performance. For example, it achieves  $62.8\%$  on ScreenSpot-Pro with the zoom-in operation. This score outperforms GUI-Owl-32B and UI-Venus-72B. We present grounding case studies across different operating systems in Figure 15. Detailed per-benchmark results are discussed below.

Grounding for high-resolution scenario. ScreenSpot-Pro evaluates grounding performance on high-resolution professional software with dense and fine-grained UI layouts. As illustrated in Table 2, MAI-UI achieves the best grounding performance across all categories, including CAD, development, creative, scientific, office, and operation systems. Averaged over categories, the MAI-UI's 2B, 8B, and 32B variants achieve  $57.4\%$ ,  $65.8\%$ , and  $67.9\%$ , surpassing the best baselines of comparable size by  $+4.1$ ,  $+10.9$ , and  $+4.3$  points, respectively. With the zoom-in strategy, MAI-UI-32B further attains an average accuracy of  $73.5\%$ , significantly surpassing strong open-source baselines such as GTA1-32B (Yang et al., 2025a), UI-Venus (Gu et al., 2025c), and GUI-Owl (Ye et al., 2025), and even outperforming Gemini 3 Pro (DeepMind, 2025b) and Seed1.8 (Seed, 2025a). These results highlight the effectiveness of our model on complex, high-resolution screens.

Table 5: Performance comparison of state-of-the-art models on the OSWorld-G. The best results are highlighted in bold, and the second-best results are underlined.  

<table><tr><td>Agent Model</td><td>Text Matching</td><td>Element Recognition</td><td>Layout Understanding</td><td>Fine-grained Manipulation</td><td>Refusal</td><td>Avg</td></tr><tr><td colspan="7">Proprietary Models</td></tr><tr><td>Operator (OpenAI, 2025)</td><td>51.3</td><td>42.4</td><td>46.6</td><td>31.5</td><td>0.0</td><td>40.6</td></tr><tr><td>Seed1.5-VL (Seed, 2025c)</td><td>73.9</td><td>66.7</td><td>69.6</td><td>47.0</td><td>18.5</td><td>62.9</td></tr><tr><td colspan="7">Open-Source Models</td></tr><tr><td>Jedi-3B (Xie et al., 2025a)</td><td>67.4</td><td>53.0</td><td>53.8</td><td>44.3</td><td>7.4</td><td>50.9</td></tr><tr><td>Qwen3-VL-2B* (Bai et al., 2025a)</td><td>61.7</td><td>45.8</td><td>54.2</td><td>39.6</td><td>-</td><td>45.9</td></tr><tr><td>OS-Atlas-7B (Wu et al., 2024)</td><td>44.1</td><td>29.4</td><td>35.2</td><td>16.8</td><td>7.4</td><td>27.7</td></tr><tr><td>UGround-7B (Gou et al., 2025)</td><td>51.3</td><td>40.3</td><td>43.5</td><td>24.8</td><td>0.0</td><td>36.4</td></tr><tr><td>Aguvis-7B (Xu et al., 2025b)</td><td>55.9</td><td>41.2</td><td>43.9</td><td>28.2</td><td>0.0</td><td>38.7</td></tr><tr><td>UI-TARS-7B (Qin et al., 2025b)</td><td>60.2</td><td>51.8</td><td>54.9</td><td>35.6</td><td>0.0</td><td>47.5</td></tr><tr><td>UI-TARS-1.5-7B (Seed, 2025b)</td><td>36.8</td><td>62.7</td><td>62.2</td><td>50.8</td><td>0.0</td><td>52.8</td></tr><tr><td>Jedi-7B (Xie et al., 2025a)</td><td>65.9</td><td>55.5</td><td>57.7</td><td>46.9</td><td>7.4</td><td>54.1</td></tr><tr><td>Qwen3-VL-8B* (Bai et al., 2025a)</td><td>69.0</td><td>55.5</td><td>59.7</td><td>47.7</td><td>-</td><td>54.8</td></tr><tr><td>GTA1-7B (Yang et al., 2025a)</td><td>42.1</td><td>65.7</td><td>62.7</td><td>56.1</td><td>0.0</td><td>55.1</td></tr><tr><td>GUI-Owl-7B (Ye et al., 2025)</td><td>64.8</td><td>63.6</td><td>61.3</td><td>41.0</td><td>-</td><td>55.9</td></tr><tr><td>UI-Venus-7B (Gu et al., 2025b)</td><td>74.6</td><td>60.5</td><td>61.5</td><td>45.5</td><td>-</td><td>58.8</td></tr><tr><td>OpenCUA-32B (Wang et al., 2025b)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>59.6</td></tr><tr><td>GUI-Owl-32B (Ye et al., 2025)</td><td>67.0</td><td>64.5</td><td>67.2</td><td>45.6</td><td>-</td><td>58.0</td></tr><tr><td>Qwen3-VL-32B* (Bai et al., 2025a)</td><td>72.8</td><td>63.3</td><td>66.4</td><td>51.7</td><td>-</td><td>60.6</td></tr><tr><td>GTA1-32B (Yang et al., 2025a)</td><td>63.2</td><td>78.4</td><td>73.3</td><td>65.2</td><td>0.0</td><td>65.2</td></tr><tr><td>UI-TARS-72B (Qin et al., 2025a)</td><td>69.4</td><td>60.6</td><td>62.9</td><td>45.6</td><td>0.0</td><td>57.1</td></tr><tr><td>UI-Venus-72B (Gu et al., 2025b)</td><td>82.1</td><td>71.2</td><td>70.7</td><td>64.4</td><td>-</td><td>70.4</td></tr><tr><td colspan="7">Ours</td></tr><tr><td>MAI-UI-2B</td><td>62.8</td><td>56.7</td><td>59.3</td><td>40.3</td><td>-</td><td>52.0</td></tr><tr><td>+ Zoom-In</td><td>66.7</td><td>59.4</td><td>63.2</td><td>44.3</td><td>-</td><td>55.9</td></tr><tr><td>MAI-UI-8B</td><td>72.0</td><td>63.3</td><td>66.0</td><td>51.0</td><td>-</td><td>60.1</td></tr><tr><td>+ Zoom-In</td><td>72.8</td><td>67.6</td><td>71.1</td><td>56.4</td><td>-</td><td>64.2</td></tr><tr><td>MAI-UI-32B</td><td>73.6</td><td>72.4</td><td>73.9</td><td>57.7</td><td>-</td><td>67.6</td></tr><tr><td>+ Zoom-In</td><td>78.5</td><td>75.2</td><td>78.3</td><td>62.4</td><td>-</td><td>70.9</td></tr></table>

Grounding for diverse and complex instruction. To assess grounding performance in terms of instruction diversity and complexity, we use UI-Vision and MMBench-GUI L2 for evaluation. UI-Vision provides multi-perspective queries (e.g., spatial, functional), and MMBench-GUI L2 includes instructions of Basic (low-level, attribute/appearance) and Advanced (high-level, goal-oriented) categories. Together, these benchmarks reflect realistic usage where user instructions are heterogeneous, high-level, and often implicit. On UI-Vision, our model sets a new state of the art: MAI-UI-32B with zoom-in achieves a  $49.2\%$  average accuracy, exceeding the strongest baseline (UI-Venus-Ground-72B  $36.8\%$ ) by +12.4 points. The 8B and 2B variants of MAI-UI also outperform baselines of similar size by +15.2 and +9.9 points, demonstrating superior grounding performance across diverse instruction perspectives. On MMBench-GUI L2, MAI-UI-32B, 8B and 2B variants attain a  $91.3\%$ ,  $88.8\%$ ,  $82.6\%$ , setting new state-of-the-art results at each scale. Additionally, the improvement on the high-level Advanced setting is much larger than on Basic setting, demonstrating strong grounding under high-level, implicit instructions.

Grounding for complex desktop scenario. OSWorld-G and OSWorld-G-Refine assess grounding in complex desktop scenarios that require software commonsense, layout understanding, and fine-grained manipulation. As illustrated in Table 5 and Table 11, MAI-UI demonstrate consistent gains across categories and model sizes. On OSWorld-G, MAI-UI-32B achieves an average of  $67.6\%$ , and increases to  $70.9\%$  with zoom-in. This exceeds the strongest baselines, including UI-Venus-72B and GTA1-32B. MAI-UI-2B, and MAI-UI-8B also outperform baselines of similar scale by 5.0 and 5.4 points, respectively. Category-wise, the MAI-UI shows balanced performance, with the 32B variant achieving  $78.5\%$  in Text Matching,  $75.2\%$  in Element Recognition,  $78.3\%$  in Layout Understanding, and  $62.4\%$  in Fine-grained Manipulation. On OSWorld-G Refine, which reduces instruction ambiguity and emphasizes precise manipulation, MAI-UI-32B reaches  $73.9\%$  and further improves to  $75.0\%$  with zoom-in, exceeding strong

Table 6: Performance comparison on AndroidWorld Benchmark. The best results are highlighted in bold, and the second-best results are underlined.  

<table><tr><td>MODEL</td><td>PARAS.</td><td>SUCCEED RATE</td></tr><tr><td colspan="3">Baselines</td></tr><tr><td>Qwen3-VL-2B (Bai et al., 2025a)</td><td>2B</td><td>36.4</td></tr><tr><td>ScaleCUA-3B (Liu et al., 2025e)</td><td>3B</td><td>23.7</td></tr><tr><td>Ferret-UI Lite-3B (Yang et al., 2025b)</td><td>3B</td><td>28.0</td></tr><tr><td>UI-Tars-7B (Qin et al., 2025b)</td><td>7B</td><td>33.0</td></tr><tr><td>UI-Tars-1.5-7B (Seed, 2025c)</td><td>7B</td><td>30.0</td></tr><tr><td>UI-Venus-7B (Gu et al., 2025c)</td><td>7B</td><td>49.1</td></tr><tr><td>GUI-Owl-7B (Ye et al., 2025)</td><td>7B</td><td>66.4</td></tr><tr><td>Step-GUI-8B (Yan et al., 2025)</td><td>8B</td><td>67.7</td></tr><tr><td>Qwen3-VL-8B (Bai et al., 2025a)</td><td>8B</td><td>47.6</td></tr><tr><td>Qwen3-VL-32B (Bai et al., 2025a)</td><td>32B</td><td>57.3</td></tr><tr><td>UI-Tars-SFT-72B (Qin et al., 2025b)</td><td>72B</td><td>46.6</td></tr><tr><td>UI-Venus-72B (Gu et al., 2025c)</td><td>72B</td><td>65.9</td></tr><tr><td>Seed1.5-VL (Guo et al., 2025b)</td><td>-</td><td>62.1</td></tr><tr><td>Qwen3-VL-235B-A22B (Bai et al., 2025a)</td><td>235B</td><td>63.7</td></tr><tr><td>UI-Tars-1.5 (Seed, 2025c)</td><td>-</td><td>64.2</td></tr><tr><td>Gemini-2.5-Pro (DeepMind, 2025a)</td><td>-</td><td>69.7</td></tr><tr><td>Seed1.8 (Seed, 2025a)</td><td>-</td><td>70.7</td></tr><tr><td>UI-Tars-2 (Wang et al., 2025a)</td><td>230B</td><td>73.3</td></tr><tr><td colspan="3">Ours</td></tr><tr><td>MAI-UI-2B</td><td>2B</td><td>49.1</td></tr><tr><td>MAI-UI-8B</td><td>8B</td><td>70.7</td></tr><tr><td>MAI-UI-32B</td><td>32B</td><td>73.3</td></tr><tr><td>MAI-UI-235B-A22B</td><td>235B</td><td>76.7</td></tr></table>

basielinesuchasOpenCUA-32BandGTA1-32B.

Grounding across different operating systems. ScreenSpot-V2 spans mobile, desktop, and web interfaces with both text and icon grounding tasks. As shown in Table 10 in Appendix, MAI-UI-32B achieves a new state of the art with  $96.5\%$  average accuracy, demonstrating strong results across all domains (e.g.,  $99.5\%$  on Desktop-Text and  $94.6\%$  on Web-Icon). Notably, MAI-UI-8B attains  $95.5\%$  average accuracy, outperforming much larger models such as UI-Venus-72B and GTA1-32B. The MAI-UI-2B on-device variant reaches  $92.5\%$ , surpassing many 7B baselines.

# 3.2.2 Mobile-Use Navigation Capability

In addition to GUI grounding evaluation, we conduct a series of experiments to validate the effectiveness of mobile navigation capabilities. This evaluation comprises two components: static offline benchmarking and challenging dynamic online benchmarking.

Offline Benchmark We evaluate MAIUI on Android Control (Li et al., 2024) and GUI Odyssey (Lu et al., 2024a), two static offline benchmarks that assess GUI action execution, and grounding. Android Control comprises two

Table 7: Performance comparison on Android Control (high-level instruction) and GUI Odyssey. Baseline results are mainly sourced from scores reported in (Zhang et al., 2025b).  

<table><tr><td>Model</td><td>AC-High</td><td>GUI-Odyssey</td></tr><tr><td>AgentCPM-GUI-8B (Zhang et al., 2025b)</td><td>69.2</td><td>75.0</td></tr><tr><td>UI-TARS-7B (Qin et al., 2025b)</td><td>74.4</td><td>67.9</td></tr><tr><td>OS-Atlas-7B (Wu et al., 2024)</td><td>56.5</td><td>76.8</td></tr><tr><td>Aguvis-7B (Xu et al., 2024)</td><td>54.2</td><td>13.5</td></tr><tr><td>OdysseyAgent-7B (Lu et al., 2024a)</td><td>32.7</td><td>73.7</td></tr><tr><td>UI-Venus-72B (Gu et al., 2025c)</td><td>77.2</td><td>72.4</td></tr><tr><td>MAI-UI-2B</td><td>67.3</td><td>72.6</td></tr><tr><td>MAI-UI-8B</td><td>69.1</td><td>80.1</td></tr><tr><td>MAI-UI-32B</td><td>75.5</td><td>83.4</td></tr></table>

instruction types: high-level instructions provide only a natural-language goal, whereas low-level in-

Table 8: Performance comparison on MobileWorld Benchmark (User-Int. is short for User-Interaction). The best results of end-to-end models are highlighted in bold, and the second-best results are underlined.  

<table><tr><td>MODEL</td><td>GUI-ONLY (116)</td><td>USER-INT. (45)</td><td>MCP (40)</td><td>OVERALL</td></tr><tr><td colspan="5">Agentic Framework</td></tr><tr><td>Claude-4.5-Sonnet (Anthropic, 2024) + UI-Ins-7B</td><td>47.8</td><td>37.8</td><td>50.0</td><td>43.8</td></tr><tr><td>Gemini-3-Pro (DeepMind, 2025b) + UI-Ins-7B</td><td>55.6</td><td>24.4</td><td>48.6</td><td>46.3</td></tr><tr><td>GPT-5 (OpenAI, 2025) + UI-Ins-7B</td><td>54.0</td><td>62.2</td><td>51.6</td><td>51.7</td></tr><tr><td colspan="5">End-to-End Model</td></tr><tr><td>GUI-Owl-7B (Ye et al., 2025)</td><td>7.7</td><td>-</td><td>-</td><td>4.5</td></tr><tr><td>GUI-Owl-32B (Ye et al., 2025)</td><td>8.5</td><td>-</td><td>-</td><td>5.5</td></tr><tr><td>UI-Venus-7B (Gu et al., 2025b)</td><td>8.5</td><td>2.3</td><td>-</td><td>5.5</td></tr><tr><td>UI-Venus-72B (Gu et al., 2025b)</td><td>16.4</td><td>4.7</td><td>-</td><td>10.4</td></tr><tr><td>Qwen3-VL-8B (Bai et al., 2025a)</td><td>9.4</td><td>0.0</td><td>0.0</td><td>5.5</td></tr><tr><td>Qwen3-VL-32B (Bai et al., 2025a)</td><td>11.9</td><td>6.7</td><td>2.7</td><td>9.0</td></tr><tr><td>Qwen3-VL-235B-A22B (Bai et al., 2025a)</td><td>12.8</td><td>4.4</td><td>5.4</td><td>9.5</td></tr><tr><td>Doubao-1.5-UI-TARS (Seed, 2025b)</td><td>26.3</td><td>32.4</td><td>-</td><td>20.9</td></tr><tr><td colspan="5">Ours</td></tr><tr><td>MAI-UI-8B</td><td>27.5</td><td>22.2</td><td>20.0</td><td>24.9</td></tr><tr><td>MAI-UI-32B</td><td>36.2</td><td>46.7</td><td>30.0</td><td>37.3</td></tr><tr><td>MAI-UI-235B-A22B</td><td>39.7</td><td>51.1</td><td>37.5</td><td>41.7</td></tr></table>

structures include step-wise action annotations. GUI Odyssey targets cross-application navigation in mobile environments. As shown in Table 7, MAI-UI produces competitive results on Android Control. On GUI Odyssey, MAI-UI-32B achieves a new state of the art on the exact match score, substantially outperforming strong baselines and highlighting superior cross-application navigation capability of our model.

Online Benchmark Online evaluation provides a more realistic assessment of agent capability than static offline tests, as it requires multi-turn adaptive perception, reasoning, and action in dynamic environments. We evaluate MAI-UI on online benchmark AndroidWorld (Rawles et al., 2024a), a live emulator-based benchmark comprising 116 tasks across 20 mobile applications. For a fair comparison, we report results only for end-to-end GUI agents, isolating the intrinsic GUI capability of foundation models without interference from pipeline frameworks or external tools.

As shown in Table 6, MAI-UI achieves new state-of-the-art results on AndroidWorld across model scales. MAI-UI-235B-A22B attains a  $76.7\%$  success rate, surpassing UI-TARS-2  $(73.3\%)$  and Gemini-2.5-Pro  $(69.7\%)$  by  $+3.4$  and  $+7.0$  points, respectively. The 32B variant reaches  $73.3\%$ , establishing SOTA at its scale and outperforming larger baseline models, including UI-Venus-72B, Gemini-2.5-Pro, and UI-TARS-1.5. MAI-UI-8B achieves  $70.7\%$  success rate, exceeding strong similar-scale models such as GUI-Owl-7B and Step-GUI-8B. Finally, the 2B model attains  $49.1\%$  success rate, improving over the strongest on-device baseline Ferret-UI-Lite-3B  $(28.0\%)$ , by  $+21.1$  points  $(+75.4\%$  relative), providing a strong foundation for our native device-cloud collaboration system. These results demonstrate the strong GUI capability of MAI-UI across the full spectrum of scales.

Real-world-oriented online evaluation. Beyond AndroidWorld, we assess our models on MobileWorld (Kong et al., 2025), a more challenging and more realistic online benchmark with 201 tasks across 20 applications. MobileWorld emphasizes long-horizon, cross-application tasks and includes evaluation beyond pure UI operation, such as MCP tool use and agent-user interaction. As summarized in Table 8, MAI-UI achieve substantial improvements against end-to-end models: MAI-UI-235B-A22B reaches  $41.7\%$  overall success rate, MAI-UI-32B attains  $37.3\%$ , and MAI-UI-8B scores  $24.9\%$ . These outperform the strongest end-to-end baseline (Doubao-1.5-UI-TARS,  $20.9\%$ ) by  $+20.8$ ,  $+16.4$ , and  $+4.0$  points, respectively. We also compare against agentic frameworks that use a strong planner such as GPT-5 and an external

![](images/40b276597596f33f7a027bafdb90a3f2dc8275ac580f4886ca969d5cbca9b159.jpg)

# # User Construction

中介给我发了两套房子的信息，我想比较一下哪一套离阿里西溪c园区开车更近，好决定租哪一间。公司地址是「杭州市余杭区文一西路969号」；把最近那套房子的地址发给我朋友Mia

![](images/1a437d1e45e7de81f25a2167d44daad255e594e99c84d009a3c1cad88533dce7.jpg)  
Click [412, 1977]

![](images/f36ff745a00303e07cc55575fa3aae82fb7e78f8d179069278ef9e9ec5188c12.jpg)  
(a)

![](images/b3c963653fadd26f3db02853163174009978acf674f8bb634fedc5e767ae8313.jpg)

# # User Instruction

Help me check the recent 3 commits summary from the google-research/android_world repository (including author and commit message), format each line as 'author: commit message' and email to mike@gmail.com, email subject as "Recent Commits"

![](images/4f45efb5aaf59b1f52a7e8639a0318c2c6bd617e6022b78c82960ba8b4fff22f.jpg)  
MCP call

![](images/9a881966b4fafdf220918475f7384bb8789c072f9a02cbd36f7cf81fc2f6228a.jpg)  
(b)  
Figure 8: Case studies of MCP tool using of MAI-UI. (a): Using MCP tools provide shortcuts that compress multiple UI actions into a few API calls; (b): Using MCP tools brings traditionally desktop-only workflows (e.g., GitHub commit search) to mobile. The user instruction for (a) is: "Compare the two apartment listings sent by the agent and determine which has the shorter driving time to Alibaba Xixi Campus (Zone C; 969 Wenyi West Road, Yuhang District, Hangzhou). Send the address of the nearer apartment to my friend Mia".

grounding model (UI-Ins (Chen et al., 2025)) as executor. These pipelines benefit from strong planning and reasoning ability and an external execution model, making the comparison not strictly fair. However, our pure end-to-end models remain competitive, with MAI-UI-235B-A22B reaching  $41.7\%$  overall success rate, close to Gemini-3-Pro+UI-Ins  $(46.3\%)$  and Claude-4.5-Sonnet+UI-Ins  $(43.8\%)$ .

To quantitatively assess our model's MCP tool-use and agent-user interaction capabilities, we evaluate on the MobileWorld benchmark's two relevant subsets (User-Int., 45 tasks; MCP, 40 tasks). Our MAI-UI-235B-A22B achieves  $51.1\%$  on User-Int. and  $37.5\%$  on MCP, outperforming existing end-to-end baselines by +18.7 and +32.1 points, respectively (best prior scores:  $32.4\%$  and  $5.4\%$ ). Compared with agentic frameworks, our end-to-end model is competitive on agent-user interaction, surpassing Gemini-3-Pro+UI-Ins-7B  $(24.4\%)$  and Claude-4.5-Sonnet+UI-Ins-7B  $(37.8\%)$ . Overall, these results demonstrate strong

![](images/c10da0943437ab33745cd71e66b4f5d9c3ba3ce127139facd2a40c1547936287.jpg)  
Figure 9: A case study of agent user interaction. The user instruction is: "In the Downloads folder, locate resume file(s) downloaded within one month and send them to my HR colleague with the subject "candidates_cv".

MCP tool-use and agent-user interaction capabilities relative to end-to-end models, and competitive performance compared with agentic frameworks.

# 3.3 MCP Augmentation and Agent-User Interaction

In addition to the quantitative results, we present case studies for the MCP tool use and user-agent interaction capability of MAI-UI.

MCP-Augmented Tasks Figure 8 shows two representative scenarios that benefit from MCP tool use, illustrating two core benefits: compressing multi step GUI operations into a few tool calls and enabling traditionally desktop only workflows on mobile.

Figure 8a illustrates a realistic cross-application task that aggregates information and compares route distance. The user receives two apartment addresses by SMS and asks the agent to compare driving time from Alibaba Xixi Campus, then send the nearer address to Mia. Traditionally, this would require repeated switching between SMS and a maps app, copying and pasting addresses, and running two separate route searches. With an Amap MCP call, the agent can simply sets the campus as the origin and each candidate address from the SMS as the destination, and retrieves structured travel time and distance. This process compresses multiple GUI operations into a small number of tool calls, significantly reducing GUI interactions and improving end-to-end efficiency.

Figure 8b shows a workflow that is usually handled on desktop environments. The user asks the agent to retrieve the author and commit message from an GitHub repository, and send the summary by email. On

![](images/126306bf62c60342154213ec1d7006b0ef2e04a2db13c035e7c86a5f83bfbd3c.jpg)  
Figure 10: The native device-cloud collaboration (DCC) capability significantly improves the on-device model's online performance, surpassing the random switch (RS) baseline, w/o error summary (ES) baseline, and several larger pure-cloud models by a substantial margin. Our native DCC system also improves efficiency, executing  $42.7\%$  of steps locally and completing  $40.5\%$  of tasks entirely on-device, thereby reducing cloud calls.

mobile, browsing commit history is inconvenient due to limited screen space. Through MCP API calls, the agent directly queries the repository and receives commit metadata in structured form, and extracts the fields required by the task. This case demonstrates that using MCP tools can not only compress UI operations but also expand the capability of mobile GUI agent, providing access to services that are commonly available in desktop applications.

Agent-User-Interaction To evaluate the agent's ability to interact with the user when necessary, we present a case study of a file-sharing task that requires proactive clarification (Figure 9). The user instructs the agent to locate recent resume files in the Downloads folder and send them to an HR colleague, but several critical parameters are under-specified, including the recipient's email address and the email body. Detecting these gaps, MAI-UI pauses execution and issues an ask_user action to request the missing details. After receiving the user's response, the agent resumes the GUI trajectory: it auto-fills and sends the email. This example demonstrates MAI-UI's proactive agent-user interaction capability, which is essential in realistic GUI tasks where instructions are often ambiguous or incomplete. Overall, the combination of MCP enhancement and Agent-User Interaction enables the model to better handle real-world tasks.

# 3.4 Device-Cloud Collaboration Analysis

On-device capability gains. On the AndroidWorld online benchmark, we evaluate how device-cloud collaboration (DCC) enhances the on-device model's performance. Our system uses MAI-UI-2B as the local agent and MAI-UI-32B as the cloud model. We compare our DCC system with four settings: (i) Local GUI agent, which uses the on-device model solely as the GUI agent; (ii) Cloud GUI agent, which relies on large scale cloud GUI agents; and (iii) Random switch (RS) baseline, which calls the cloud model at the same frequency as our DCC method but without monitor-guided switch. As shown in the left part of Figure 10, our DCC system achieves relative improvement of  $33.4\%$  over the on-device model, significantly strengthening its ability to complete mobile tasks. It further surpasses the RS baseline by  $7.7\%$ , demonstrating that monitor-driven cloud-model switching effectively triggers cloud assistance and boosts collaboration quality. Finally, compared with several pure cloud GUI agent baselines, our device-cloud system achieves higher scores, demonstrating strong performance under

![](images/ae16211562c672703f47f26a100dd7b3657c8177cb31fa5a776c7dfa9407401f.jpg)  
Figure 11: A pilot study demonstrating privacy protection in the device-cloud collaboration system. At step 3, the local agent deviates, as it repeatedly tapping the Login button without entering a password. The trajectory monitor flags the misalignment and proposes switch to the cloud agent. However, the privacy detection module detects sensitive credentials and blocks the switch, keeping execution on device. The local monitor ultimately corrects the trajectory and completes the task.

realistic deployment setting.

Efficiency gains versus cloud-only solution. We analyze cloud usage and on-device completion under our device-cloud collaboration system in the right part of Figure 10. Compared to cloud-only serving, our system reduces cloud model calls by  $42.7\%$ , substantially lowering serving cost and latency for using cloud model. On AndroidWorld, over  $40\%$  of tasks are completed entirely on device, further confirming that unnecessary cloud model calls are reduced. These results demonstrate that our device-cloud collaboration obtains substantial efficiency gains compared to cloud-only solutions.

Impact of error summary information. To assess the effectiveness of monitor-generated error summaries, we run an ablation that removes the error summary (ES) at switching and compare against our original system. As shown in the left part of Figure 10, compared to the baseline that without error summary, providing the error summary yields a  $+6.9$  increase in task success rate, underscoring its importance for the cloud agent's trajectory recovery process.

Privacy preservation. We present a pilot case study to showcase the system's privacy protection capability. We introduce an additional local privacy monitor, which blocks cloud switch whenever privacy-sensitive content is present, even if a trajectory deviation is detected. As shown in Figure 11, for

![](images/61b5f860aa86d38338a6de291081b92ee0a19bdba660d7f84b207391d63dceba.jpg)  
(a)

![](images/3422133b740eeeb4f57de51ff75297f3be065a7d8e176ac47bb93ce228a1068f.jpg)  
(b)  
Figure 12: MAI-UI-8B-RL training details: (a) train set reward trend (b) performance scaling with the number of environments.

a task involving user-sensitive content (password entry), the system continues on-device execution even when the monitor detects trajectory deviation (repeated clicks on the login icon in steps 2-4), thereby adhering privacy constraints. The Local Agent ultimately corrects the trajectory and completes the task. Throughout this case, no privacy-sensitive content is transmitted to the cloud, demonstrating that this device-cloud collaboration system effectively protects user privacy.

# 3.5 Online RL Analysis

Performance Gains. As shown in Table 9, online RL consistently enhances performance across all model scales. Specifically, the 2B model achieves an absolute improvement of 4.0 percentage points  $(45.1\% \rightarrow 49.1\%)$ , the 8B model gains 6.0 percentage points  $(64.7\% \rightarrow 70.7\%)$ , and the 32B model improves by 3.5 percentage points  $(69.8\% \rightarrow 73.3\%)$ . These results correspond to relative improvements of  $8.9\%$ ,  $9.3\%$ , and  $5.0\%$ , respectively, demonstrating that online RL effectively enhances agent performance regardless of model size. Figure 12a further illustrates this improvement, showing that the reward metric steadily increases throughout the training process.

Experimental Analysis. We perform ablation studies to evaluate the proposed key components in online RL, with the results shown in Table 9 and Figure 12.

Table 9: Online RL performance gains and ablations on standard GRPO and interaction budget per trajectory (max_env_steps)  

<table><tr><td>Method</td><td>AndroidWorld</td></tr><tr><td colspan="2">SFT vs. RL Comparison</td></tr><tr><td>MAI-UI-2B-SFT</td><td>45.1</td></tr><tr><td>MAI-UI-2B-RL</td><td>49.1 (+4.0)</td></tr><tr><td>MAI-UI-8B-SFT</td><td>64.7</td></tr><tr><td>MAI-UI-8B-RL</td><td>70.7 (+6.0)</td></tr><tr><td>MAI-UI-32B-SFT</td><td>69.8</td></tr><tr><td>MAI-UI-32B-RL</td><td>73.3 (+3.5)</td></tr><tr><td colspan="2">RL Ablation on 8B</td></tr><tr><td>MAI-UI-8B-SFT</td><td>64.7</td></tr><tr><td>GRPO (max_env_steps=50)</td><td>66.5 (+1.8)</td></tr><tr><td>Our method (max_env_steps=15)</td><td>66.4 (+1.7)</td></tr><tr><td>Our method (max_env_steps=30)</td><td>68.5 (+3.8)</td></tr><tr><td>Our method (max_env_steps=50)</td><td>70.7 (+6.0)</td></tr></table>

- Comparison with standard GRPO. Standard GRPO applied after SFT yields a modest gain of  $+1.8$  percentage points on AndroidWorld. In contrast, our enhanced GRPO with data curriculum, repetition penalty, and experience replay achieves a  $+6.0$  percentage point improvement, delivering an additional  $+4.2$  percentage points over the baseline.  
- Effect of interaction budget. The maximum environment interaction budget per trajectory substantially influences performance. Extending the budget from 15 to 30 and subsequently to 50 steps yields progressive improvements (+1.7, +3.8, and +6.0 percentage points, respectively). A larger budget enables more extensive rollouts and provides richer exploration opportunities during training.  
- Impact of image resolution. Image resolution critically affects online RL efficiency, as higher resolutions introduce more visual tokens and slow down both training and inference. Leveraging the

![](images/25cb433290d12bddf6bd8d5efe935e279978228a86c3017c33bdeea32b20a32a.jpg)  
(a)  
Figure 13: MAI-UI demonstrates robustness to unexpected permission dialogs (a) and pop-ups (b) in AndroidWorld, and reliably resumes the task.

![](images/0977f11c8114dce115140947b2fa829df555ebbe3e1efa740e3d707282b5abb4.jpg)  
(b)

relative coordinate mechanism of Qwen3-VL, we found that 720p resolution achieves performance comparable to 1080p while providing a  $\sim 50.1\%$  speedup per step. Conversely, 540p resolution, despite faster processing, substantially degrades model performance due to insufficient detail for fine-grained UI element perception.

- Scaling parallel environments. Figure 12b shows how parallel environment count affects model performance. Increasing parallel environments from 32 to 512 significantly accelerates learning and improves final performance  $(65.5\% \rightarrow 70.7\%)$ . Training with fewer environments exhibits early saturation, indicating that limited environments constrain policy improvement. These findings highlight that scaling parallel environments to enhance exploration diversity is critical for overcoming performance bottlenecks in GUI agent RL training.

Case Studies: Enhanced Robustness. Figures 13 and 14 illustrate qualitative improvements in robustness after online RL training. Figure 13 demonstrates MAI-UI's ability to handle unexpected permission dialogs and pop-ups during task execution. When creating a new contact for "Emilia Gonzalez," the agent encounters a notification permission request that was not present during offline training. The RL-trained model successfully dismisses the dialog and continues task execution without deviation, whereas the base model often fails to recover from such interruptions. Figure 14 showcases the agent's capability to recover from failed actions in a complex expense management task. When instructed to delete duplicate expenses, the agent initially navigates to the wrong application. Nevertheless MAI-UI correct the trajectory and complete the task in the following steps. These case studies highlight that online RL training substantially improves the agent's robustness to real-world unpredictability. This is difficult to acquire through offline training alone, where the diversity of edge cases and failure modes is inherently limited.

# 3.6 Analysis on Grounding

As demonstrated by our prior work in UI-Ins (Chen et al., 2025), our Instruction-as-Reasoning approach offers benefits from several perspectives:

- Reasoning that helps grounding. Prior studies find that general chain-of-thought reasoning can often degrade grounding performance (Lu et al., 2025; Yang et al., 2025a; Zhou et al., 2025). Inspired by how humans approach grounding, we train models to use diverse instruction perspectives as explicit

![](images/70c7a8d3de41f10fc22e7ce65b284e3a315d94df982f0fdedeaf49fea33188e7.jpg)  
Figure 14: MAI-UI shows robustness in recovering from recovering from failures: the agent initially navigates to the wrong application, but MAI-UI corrects the trajectory and completes the task.

reasoning pathways, making reasoning actionable and beneficial for GUI grounding.

- Mitigating policy collapse in SFT + RL framework. Policy collapse often occurs in grounding after SFT with coordinate-only supervision (Zhang et al., 2025a). Instruction-as-Reasoning stabilizes RL by pretraining the model to generate diverse reasoning pathways, which enhances exploratory behavior and stabilizes policy optimization in the RL phase.  
- Emergent multi-perspective capabilities. After employing our instruction-as-reasoning method, the model can strategically select appropriate reasoning perspectives given different contexts and compose multiple perspectives into a cohesive one. Interestingly, it can also generate novel analytical angles beyond the four trained perspectives.

# 4 Related Works

# 4.1 GUI Grounding

GUI grounding is the foundational capability of GUI agents that maps natural language instructions to the locations of target elements in screenshots. Prior GUI grounding methods mainly focus on training in a Supervised Fine-Tuning (SFT) paradigm, such as JEDI (Xie et al., 2025a), OS-Atlas (Wu et al., 2024), Aguvis (Xu et al., 2025b), Uground (Gou et al., 2025) and Aria-UI (Yang et al., 2024b). Reinforcement learning methods, particularly GRPO (Guo et al., 2025a) have demonstrated remarkable success on various visual-language tasks, including Semantic Segmentation (Liu et al., 2025c), Visual Question-Answering (Liu et al., 2025d; Huang et al., 2025) and Temporal Video Grounding (Wang et al., 2025c). Consequently, recent efforts have increasingly focused on adapting RL for GUI grounding. GUI Grounding methods like GUI-R1 (Luo et al., 2025), GUI-Actor (Wu et al., 2025) and GTA1 (Yang et al., 2025a) play as an pioneer role in pure RL paradigm and surpass SFT-based methods by a large margin. However, a key limitation of a pure RL paradigm is that it overlooks the substantial benefit offered by an initial SFT stage. While InfigUI-R1 (Liu et al., 2025a) achieved success with an SFT+RL framework by reframing GUI grounding as a trajectory-level task that encourages model reflection, the SFT+RL paradigm remains difficult to implement in practice. It is also demonstrated by Phi-Ground (Zhang

![](images/e36bc3d92092972c6bfbd1bbdc5d979d63a3bc561f957e2151184c6c09725233.jpg)  
Figure 15: Grounding case studies across different operating systems.

et al., 2025a) that SFT+RL framework is prone to policy collapse. Our grounding method overcomes this issue by using the SFT stage to teach the model diverse reasoning through different instruction perspectives, and then utilizes the RL stage to further incentivize the model to select the appropriate reasoning pathway, thereby establishing an effective example for the SFT+RL training paradigm.

# 4.2 GUI Navigation

Moving beyond single-step grounding, research on GUI agents has advanced to GUI navigation tasks, where agents must execute multi-step action sequences to complete a user instruction. Early works achieve this goal through agent frameworks that contain multiple components, such as a planner and a grounding module (Wang et al., 2024a; Yang et al., 2024b; Zhou et al., 2024). For practical deployment and cost efficiency, recent efforts increasingly target unified vision-language-action models that jointly learn grounding and navigation (Qin et al., 2025a; Zhang et al., 2025b; Gu et al., 2025b). Most of these models are large-scale to enable stronger reasoning and planning, while a growing line of work builds competitive small on-device models for lower latency, improved privacy, and resilience to connectivity constraints (Yang et al., 2025b). To meet deployment needs, we release both large-scale and on-device models, and further integrate a native device-cloud collaboration capability.

With the rapid development in GUI navigation, progress has also been made in real-world deployment of GUI agents (Google, 2025). In practice, however, current agents function as copilots rather than

standalone executors, making effective agent-user interaction a critical yet often overlooked capability. Moreover, most agents remain limited to pure UI manipulation, and only recently have efforts begun to equip them with extended SDK functions (Wang et al., 2025a). Integrating external tools such as the MCP can compress long, brittle UI action sequences into a handful of API calls and unlock desktop workflows that were previously infeasible on mobile.

# 5 Conclusion

In this work, we present MAI-UI, a family of foundation GUI agents. To address challenges for real-world deployment of GUI agents, we introduce three main components: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device-cloud collaboration system that routes execution by task state and data sensitivity, and an online RL framework with advanced system optimizations. Experimental results show that MAI-UI sets a new state of the art on grounding benchmarks and both online and offline evaluation of mobile use navigation. Benchmark results on MobileWorld and qualitative case studies verify effective agent-user interaction and MCP-enabled tool use capability of MAI-UI. The device-cloud collaboration substantially improves on-device performance while reducing cloud model calls, yielding performance, privacy and cost benefits. Taken together, these advances make MAI-UI a step further toward practical foundation GUI agent for mobile use.

# References

Anthropic. What is the model context protocol (mcp)? https://modelcontextprotocol.io/docs/getting-started/intro, 2024.  
Anthropic. Claude 3.7 sonnet. https://www.anthropic.com/news/claude-3-7-sonnet, 2024. Accessed: 2025-08-02.  
Anthropic. Our 3.5 models and computer use. https://wwwanthropic.com/news/3-5-models-and-computer-use, sep 2024. Accessed: 2025-09-22.  
Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025a.  
Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b.  
Liangyu Chen, Hanzhang Zhou, Chenglin Cai, Jianan Zhang, Panrong Tong, Quyu Kong, Xu Zhang, Chen Liu, Yuqi Liu, Wenxuan Wang, et al. Ui-ins: Enhancing gui grounding with multi-perspective instruction-as-reasoning. arXiv preprint arXiv:2510.20286, 2025.  
Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents, 2024. URL https://arxiv.org/abs/2401.10935.  
Google DeepMind. Gemini computer use model. Google Blog, 2025a. URL https://blog.google/technology/google-deepmind/gemini-computer-use-model/.  
Google DeepMind. Gemini 3 pro, 2025b. URL https://deepmind.google/models/gemini/pro/.  
Google. Gemini live, 2025. URL https://gemini.google/overview/gemini-live/.  
Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for GUI agents. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/orum?id=kxnoqaisCT.  
Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Saizhuo Wang, Kun Zhang, Yuanzhuo Wang, Wen Gao, Lionel Ni, and Jian Guo. A survey on llm-as-a-judge, 2025a. URL https://arxiv.org/abs/2411.15594.  
Zhangxuan Gu, Zhengwen Zeng, Zhenyu Xu, Xingran Zhou, Shuheng Shen, Yunfei Liu, Beitong Zhou, Changhua Meng, Tianyu Xia, Weizhi Chen, Yue Wen, Jingya Dou, Fei Tang, Jinzhen Lin, Yulin Liu, Zhenlin Guo, Yichen Gong, Heng Jia, Changlong Gao, Yuan Guo, Yong Deng, Zhenyu Guo, Liang Chen, and Weiqiang Wang. Ui-venus technical report: Building high-performance ui agents with rft, 2025b. URL https://arxiv.org/abs/2508.10833.  
Zhangxuan Gu, Zhengwen Zeng, Zhenyu Xu, Xingran Zhou, Shuheng Shen, Yunfei Liu, Beitong Zhou, Changhua Meng, Tianyu Xia, Weizhi Chen, et al. Ui-venus technical report: Building high-performance ui agents with rft. arXiv preprint arXiv:2508.10833, 2025c.

Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a.  
Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025b.  
Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models, 2025. URL https://arxiv.org/abs/2503.06749.  
Yangqin Jiang and Chao Huang. Lightagent: Mobile agentic foundation models. arXiv preprint arXiv:2510.22009, 2025.  
Quyu Kong, Xu Zhang, Zhenyu Yang, Nolan Gao, Chen Liu, Panrong Tong, Chenglin Cai, Hanzhang Zhou, Jianan Zhang, Liangyu Chen, Zhidan Liu, Steven Hoi, and Yue Wang. Mobileworld: Benchmarking autonomous mobile agents in agent-user interactive, and mcp-augmented environments. arXiv preprint arXiv:2512.19432, 2025.  
Kaixin Li, Meng Ziyang, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenshot-pro: GUI grounding for professional high-resolution computer use. In Workshop on Reasoning and Planning for Large Language Models, 2025. URL https://openreview.net/forum?id=XaKNDIAHas.  
Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on ui control agents, 2024. URL https://arxiv.org/abs/2406.03679.  
Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners, 2025a. URL https://arxiv.org/abs/2504.14239.  
Yuhang Liu, Zeyu Liu, Shuanghe Zhu, Pengxiang Li, Congkai Xie, Jiasheng Wang, Xueyu Hu, Xiaotian Han, Jianbo Yuan, Xinyao Wang, et al. Infigui-g1: Advancing gui grounding with adaptive exploration policy optimization. arXiv preprint arXiv:2508.05731, 2025b.  
Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv preprint arXiv:2503.06520, 2025c.  
Yuqi Liu, Tianyuan Qu, Zhisheng Zhong, Bohao Peng, Shu Liu, Bei Yu, and Jiaya Jia. Visionreasoner: Unified visual perception and reasoning via reinforcement learning. arXiv preprint arXiv:2505.12081, 2025d.  
Zhaoyang Liu, Jingjing Xie, Zichen Ding, Zehao Li, Bowen Yang, Zhenyu Wu, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, Shenglong Ye, Qingyun Li, Xuan Dong, Yue Yu, Chenyu Lu, YunXiang Mo, Yao Yan, Zeyue Tian, Xiao Zhang, Yuan Huang, Yiqian Liu, Weijie Su, Gen Luo, Xiangyu Yue, Biqing Qi, Kai Chen, Bowen Zhou, Yu Qiao, Qifeng Chen, and Wenhai Wang. Scalecua: Scaling open-source computer use agents with cross-platform data. arXiv preprint arXiv:2509.15221, 2025e. URL https://github.com/OpenGVLab/ScaleCUA. Preprint.  
Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, Yu Qiao, and Ping Luo. Gui odyssey: A comprehensive dataset for cross-app gui navigation on mobile devices. arXiv preprint arXiv:2406.08451, 2024a.  
Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omniparser for pure vision based gui agent, 2024b. URL https://arxiv.org/abs/2408.00203.

Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025.  
Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1: A generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025.  
Shravan Nayak, Xiangru Jian, Kevin Qinghong Lin, Juan A. Rodriguez, Montek Kalsi, Rabiul Awal, Nicolas Chapados, M. Tamer Özsu, Aishwarya Agrawal, David Vazquez, Christopher Pal, Perouz Taslakian, Spandana Gella, and Sai Rajeswar. Ui-vision: A desktop-centric gui benchmark for visual perception and interaction, 2025. URL https://arxiv.org/abs/2503.15661.  
OpenAI. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276.  
OpenAI. Developing a generalist computer-using agent. OpenAI, 2025. URL https://openai.com/index/computer-using-agent/. Accessed: October 22, 2025.  
OpenAI. Introducing gpt-5. Technical report, OpenAI, 2025. URL https://openai.com/zh-Hans-CN/index/introducing-gpt-5/.  
Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025a.  
Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025b.  
Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, Daniel Toyama, Robert Berry, Divya Tyamagundlu, Timothy Lillicrap, and Oriana Riva. Androidworld: A dynamic benchmarking environment for autonomous agents, 2024a. URL https://arxiv.org/abs/2405.14573.  
Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. Androidworld: A dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024b.  
Bytedance Seed. Seed1.8 model card: Towards generalized real-world agency. arXiv preprint, December 2025a. Technical Report.  
ByteDance Seed. Ui-tars-1.5. https://seed-tars.com/1.5, 2025b.  
ByteDance Seed. Ui-tars-1.5. https://seed-tars.com/1.5, 2025c.  
Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024.  
Fei Tang, Zhangxuan Gu, Zhengxi Lu, Xuyang Liu, Shuheng Shen, Changhua Meng, Wen Wang, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, and Yueting Zhuang. Gui-g²: Gaussian reward modeling for gui grounding, 2025. URL https://arxiv.org/abs/2507.15846.  
Haoming Wang, Haoyang Zou, Huatong Song, Jiazhan Feng, Junjie Fang, Junting Lu, Longxiang Liu, Qinyu Luo, Shihao Liang, Shijue Huang, et al. Ui-tars-2 technical report: Advancing gui agent with multi-turn reinforcement learning. arXiv preprint arXiv:2509.02544, 2025a.

Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. arXiv preprint arXiv:2401.16158, 2024a.  
Shuai Wang, Weiwen Liu, Jingxuan Chen, Yuqi Zhou, Weinan Gan, Xingshan Zeng, Yuhan Che, Shuai Yu, Xinlong Hao, Kun Shao, et al. Gui agents with foundation models: A comprehensive survey. arXiv preprint arXiv:2411.04890, 2024b.  
Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Boyuan Zheng, Peihang Li, Fangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Jiarui Hu, Yuyan Wang, Jixuan Chen, Yuxiao Ye, Danyang Zhang, Dikang Du, Hao Hu, Huarong Chen, Zaida Zhou, Haotian Yao, Ziwei Chen, Qizheng Gu, Yipu Wang, Heng Wang, Diyi Yang, Victor Zhong, Flood Sung, Y. Charles, Zhilin Yang, and Tao Yu. Opencua: Open foundations for computer-use agents, 2025b. URL https://arxiv.org/abs/2508.09123.  
Ye Wang, Ziheng Wang, Boshen Xu, Yang Du, Kejun Lin, Zihan Xiao, Zihao Yue, Jianzhong Ju, Liang Zhang, Dingyi Yang, Xiangnan Fang, Zewen He, Zhenbo Luo, Wenxuan Wang, Junqi Lin, Jian Luan, and Qin Jin. Time-r1: Post-training large vision language model for temporal video grounding, 2025c. URL https://arxiv.org/abs/2503.13377.  
Yiqin Wang, Haoji Zhang, Jingqi Tian, and Yansong Tang. Ponder & press: Advancing visual gui agent towards general computer control, 2024c. URL https://arxiv.org/abs/2412.01268.  
Qianhui Wu, Kanzhi Cheng, Rui Yang, Chaoyun Zhang, Jianwei Yang, Huiqiang Jiang, Jian Mu, Baolin Peng, Bo Qiao, Reuben Tan, et al. Gui-actor: Coordinate-free visual grounding for gui agents. arXiv preprint arXiv:2506.03143, 2025.  
Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, and Yu Qiao. Os-atlas: A foundation action model for generalist gui agents, 2024. URL https://arxiv.org/abs/2410.23218.  
Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, Yiheng Xu, Junli Wang, Doyen Sahoo, Tao Yu, and Caiming Xiong. Scaling computer-use grounding via user interface decomposition and synthesis, 2025a. URL https://arxiv.org/abs/2505.13227.  
Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, Yiheng Xu, Junli Wang, Doyen Sahoo, Tao Yu, and Caiming Xiong. Scaling computer-use grounding via user interface decomposition and synthesis, 2025b. URL https://arxiv.org/abs/2505.13227.  
Yifan Xu, Xiao Liu, Xueqiao Sun, Siyi Cheng, Hao Yu, Hanyu Lai, Shudan Zhang, Dan Zhang, Jie Tang, and Yuxiao Dong. Androidlab: Training and systematic benchmarking of android autonomous agents. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2144-2166, 2025a.  
Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024.  
Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction, 2025b. URL https://arxiv.org/abs/2412.04454.

JingJing Xie Xuehui Wang, Zhenyu Wu et al. Mmbench-gui: Hierarchical multi-platform evaluation framework for gui agents. arXiv preprint arXiv:2507.19478, 2025.  
Haolong Yan, Jia Wang, Xin Huang, Yeqing Shen, Ziyang Meng, Zhimin Fan, Kaijun Tan, Jin Gao, Lieyu Shi, Mi Yang, et al. Step-gui technical report. arXiv preprint arXiv:2512.15431, 2025.  
An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024a. URL https://arxiv.org/abs/2407.10671.  
Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha, Zeyuan Chen, Ran Xu, Liyuan Pan, Caiming Xiong, and Junnan Li. Gta1: Gui test-time scaling agent, 2025a. URL https://arxiv.org/abs/2507.05791.  
Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. Aria-ui: Visual grounding for gui instructions, 2024b. URL https://arxiv.org/abs/2412.16256.  
Zhen Yang, Zi-Yi Dou, Di Feng, Forrest Huang, Anh Nguyen, Keen You, Omar Attia, Yuhao Yang, Michael Feng, Haotian Zhang, et al. Ferret-ui lite: Lessons from building small on-device gui agents. arXiv preprint arXiv:2509.26539, 2025b.  
Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, et al. Mobile-agent-v3: Fundamental agents for gui automation. arXiv preprint arXiv:2508.15144, 2025.  
Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025.  
Xinbin Yuan, Jian Zhang, Kaixin Li, Zhuoxuan Cai, Lujian Yao, Jie Chen, Enguang Wang, Qibin Hou, Jinwei Chen, Peng-Tao Jiang, et al. Enhancing visual grounding for gui agents via self-evolutionary reinforcement learning. arXiv preprint arXiv:2505.12370, 2025.  
Miaosen Zhang, Ziqiang Xu, Jialiang Zhu, Qi Dai, Kai Qiu, Yifan Yang, Chong Luo, Tianyi Chen, Justin Wagle, Tim Franklin, et al. Phi-ground tech report: Advancing perception in gui grounding. arXiv preprint arXiv:2507.23779, 2025a.  
Zhong Zhang, Yaxi Lu, Yikun Fu, Yupeng Huo, Shenzhi Yang, Yesai Wu, Han Si, Xin Cong, Haotian Chen, Yankai Lin, et al. Agentcpm-gui: Building mobile-use agents with reinforcement fine-tuning. arXiv preprint arXiv:2506.01391, 2025b.  
Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, and Yuchen Eleanor Jiang. Symbolic learning enables self-evolving agents. 2024. URL https://arxiv.org/abs/2406.18532.  
Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinglin Jia, and Jun Xu. Gui-g1: Understanding r1-zero-like training for visual grounding in gui agents, 2025. URL https://arxiv.org/abs/2505.15810.  
Jinguo Zhu, Weiyun Wang, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. URL https://arxiv.org/abs/2504.10479.

# A Additional Grounding Results

We present more grounding results of ScreenSpot-v2 and OSWorld-G Refine in Table 10, and Table 11.

Table 10: Performance comparison on ScreenSpot-V2. We use  $^\prime \ast \prime$  to denote the results evaluated by us. The best results are highlighted in bold, and the second-best results are underlined.  

<table><tr><td rowspan="2">Model</td><td colspan="2">Mobile</td><td colspan="2">Desktop</td><td colspan="2">Web</td><td rowspan="2">Avg.</td></tr><tr><td>Text</td><td>Icon.</td><td>Text</td><td>Icon.</td><td>Text</td><td>Icon.</td></tr><tr><td colspan="8">Open-Source Models</td></tr><tr><td>Qwen3-VL-2B* (Bai et al., 2025a)</td><td>95.5</td><td>82.0</td><td>95.4</td><td>73.6</td><td>89.7</td><td>76.4</td><td>86.7</td></tr><tr><td>Phi-ground (Zhang et al., 2025a)</td><td>90.2</td><td>76.4</td><td>93.6</td><td>75.9</td><td>96.5</td><td>62.0</td><td>83.8</td></tr><tr><td>OS-Atlas-7B (Wu et al., 2024)</td><td>95.2</td><td>75.8</td><td>90.7</td><td>63.6</td><td>90.6</td><td>77.3</td><td>85.1</td></tr><tr><td>UGround-v1-7B (Gou et al., 2025)</td><td>83.6</td><td>90.5</td><td>85.8</td><td>86.3</td><td>95.5</td><td>83.2</td><td>87.7</td></tr><tr><td>UI-Tars-1.5-7B (Seed, 2025b)</td><td>92.2</td><td>81.5</td><td>91.0</td><td>84.2</td><td>95.5</td><td>84.5</td><td>89.0</td></tr><tr><td>SE-GUI-7B* (Yuan et al., 2025)</td><td>99.3</td><td>89.1</td><td>96.4</td><td>78.6</td><td>92.7</td><td>81.3</td><td>90.8</td></tr><tr><td>UI-TARS-7B (Qin et al., 2025a)</td><td>96.9</td><td>89.1</td><td>95.4</td><td>85.0</td><td>93.6</td><td>85.2</td><td>91.6</td></tr><tr><td>Qwen3-VL-8B* (Bai et al., 2025a)</td><td>97.9</td><td>84.8</td><td>95.9</td><td>87.9</td><td>95.7</td><td>83.7</td><td>91.7</td></tr><tr><td>GUI-Actor-7B* (Wu et al., 2025)</td><td>97.6</td><td>88.2</td><td>96.9</td><td>85.7</td><td>93.2</td><td>86.7</td><td>92.1</td></tr><tr><td>OpenCUA-7B (Wang et al., 2025b)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>92.3</td></tr><tr><td>GTA1-7B (Yang et al., 2025a)</td><td>99.0</td><td>88.6</td><td>94.9</td><td>89.3</td><td>92.3</td><td>86.7</td><td>92.4</td></tr><tr><td>GUI-Owl-7B (Ye et al., 2025)</td><td>99.0</td><td>92.4</td><td>96.9</td><td>85.0</td><td>93.6</td><td>85.2</td><td>92.8</td></tr><tr><td>GUI-G2-7B* (Tang et al., 2025)</td><td>98.3</td><td>91.9</td><td>95.4</td><td>89.3</td><td>94.0</td><td>87.7</td><td>93.3</td></tr><tr><td>InfiGUI-G1-7B* (Liu et al., 2025b)</td><td>99.0</td><td>91.9</td><td>94.3</td><td>82.1</td><td>97.9</td><td>89.2</td><td>93.5</td></tr><tr><td>UI-Venus-7B (Gu et al., 2025b)</td><td>99.0</td><td>90.0</td><td>96.9</td><td>90.7</td><td>96.2</td><td>88.7</td><td>94.1</td></tr><tr><td>Qwen3-VL-32B* (Bai et al., 2025a)</td><td>96.2</td><td>90.0</td><td>97.4</td><td>85.0</td><td>95.7</td><td>89.7</td><td>93.0</td></tr><tr><td>GUI-Owl-32B (Ye et al., 2025)</td><td>98.6</td><td>90.0</td><td>97.9</td><td>87.8</td><td>94.4</td><td>86.7</td><td>93.2</td></tr><tr><td>OpenCUA-32B (Wang et al., 2025b)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>93.4</td></tr><tr><td>GTA1-32B (Yang et al., 2025a)</td><td>99.7</td><td>90.5</td><td>99.0</td><td>94.3</td><td>95.7</td><td>90.1</td><td>95.2</td></tr><tr><td>UI-Venus-72B (Gu et al., 2025b)</td><td>99.7</td><td>93.8</td><td>95.9</td><td>90.0</td><td>96.2</td><td>92.6</td><td>95.3</td></tr><tr><td colspan="8">Ours</td></tr><tr><td>MAI-UI-2B</td><td>99.3</td><td>87.2</td><td>97.4</td><td>88.6</td><td>94.0</td><td>84.7</td><td>92.5</td></tr><tr><td>MAI-UI-8B</td><td>99.3</td><td>89.1</td><td>99.0</td><td>92.1</td><td>97.9</td><td>91.1</td><td>95.2</td></tr><tr><td>MAI-UI-32B</td><td>99.0</td><td>92.9</td><td>99.5</td><td>93.6</td><td>97.4</td><td>94.6</td><td>96.5</td></tr></table>

Table 11: Performance comparison of state-of-the-art models on the OSWorld-G-Refine. The best results are highlighted in **bold**, and the second-best results are **underlined**.  

<table><tr><td>Agent Model</td><td>Text Matching</td><td>Element Recognition</td><td>Layout Understanding</td><td>Fine-grained Manipulation</td><td>Refusal</td><td>Avg</td></tr><tr><td colspan="7">Proprietary Models</td></tr><tr><td>Operator OpenAI (2025)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>57.8</td></tr><tr><td colspan="7">Open-Source Models</td></tr><tr><td>Qwen3-VL-2B* (Bai et al., 2025a)</td><td>69.3</td><td>60.9</td><td>69.2</td><td>45.0</td><td>-</td><td>57.4</td></tr><tr><td>Jedi-3B Xie et al. (2025a)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>61.0</td></tr><tr><td>Jedi-7B Xie et al. (2025a)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>63.8</td></tr><tr><td>UI-TARS-1.5-7B Seed (2025b)</td><td>52.6</td><td>75.4</td><td>72.4</td><td>66.7</td><td>0.0</td><td>64.2</td></tr><tr><td>Qwen3-VL-8B* (Bai et al., 2025a)</td><td>73.9</td><td>68.2</td><td>73.1</td><td>54.4</td><td>-</td><td>64.4</td></tr><tr><td>GTA1-7B (Yang et al., 2025a)</td><td>63.2</td><td>82.1</td><td>74.2</td><td>70.5</td><td>0.0</td><td>67.7</td></tr><tr><td>Qwen2.5-VL-32B Bai et al. (2025b)</td><td>57.9</td><td>70.2</td><td>73.8</td><td>49.2</td><td>0.0</td><td>59.6</td></tr><tr><td>OpenCUA-32B Wang et al. (2025b)</td><td>63.2</td><td>79.9</td><td>84.9</td><td>62.1</td><td>7.4</td><td>70.2</td></tr><tr><td>Qwen3-VL-32B* (Bai et al., 2025a)</td><td>77.4</td><td>73.6</td><td>76.3</td><td>57.7</td><td>-</td><td>69.0</td></tr><tr><td>GTA1-32B (Yang et al., 2025a)</td><td>63.2</td><td>83.6</td><td>84.4</td><td>70.5</td><td>0.0</td><td>72.2</td></tr><tr><td colspan="7">Ours</td></tr><tr><td>MAI-UI-2B</td><td>70.9</td><td>69.1</td><td>72.7</td><td>47.7</td><td>-</td><td>63.5</td></tr><tr><td>+ Zoom-In</td><td>71.3</td><td>71.8</td><td>78.3</td><td>51.0</td><td>-</td><td>66.3</td></tr><tr><td>MAI-UI-8B</td><td>77.4</td><td>73.0</td><td>78.3</td><td>55.7</td><td>-</td><td>68.6</td></tr><tr><td>+ Zoom-In</td><td>79.3</td><td>78.8</td><td>84.2</td><td>59.7</td><td>-</td><td>72.9</td></tr><tr><td>MAI-UI-32B</td><td>79.7</td><td>79.4</td><td>81.0</td><td>61.7</td><td>-</td><td>73.9</td></tr><tr><td>+ Zoom-In</td><td>79.3</td><td>79.7</td><td>84.2</td><td>64.4</td><td>-</td><td>75.0</td></tr></table>