# Unified Configuration for MAI-UI Training
# This config consolidates all settings for the training pipeline

# Project metadata
project:
  name: "MAI-UI-Training"
  version: "1.0.0"
  description: "End-to-end training pipeline for MAI-UI models"

# Data preprocessing settings
data:
  # Raw data
  raw_data_dir: "../../dataset/20260119_201327"
  
  # Processed data output (与 raw_data_dir 同级，和 trajectory.jsonl 放一起)
  processed_data_dir: "../../dataset/20260119_201327"
  processed_data_file: "../../dataset/20260119_201327/sft_train.jsonl"
  
  # Processing options
  output_format: "prompt_response"  # openai_messages, prompt_response, full_trajectory
  image_format: "base64"  # base64, path, skip
  compress_images: true
  image_quality: 85
  image_max_size: null  # e.g., [1920, 1080] for resizing
  
  # Filtering
  max_samples_per_trajectory: -1  # -1 for all
  include_history: true
  history_window: 5
  skip_failed_steps: true

# Model settings
model:
  # Base model
  name: "MAI-UI-2B"
  path: "/workspace/MAI-UI-2B" # 32.115: "/data/huggingface_cache/hub/models--Tongyi-MAI--MAI-UI-2B/snapshots/503050934809558c8dfd2ddedaf9621fa74ac2de/"
  
  # SFT model output
  sft_output_dir: "./models/sft_model"
  
  # RL model output (if enabled)
  rl_output_dir: "./models/rl_model"

# Training settings
training:
  # SFT training
  # MAI-UI SFT focuses on Grounding + Navigation data with Instruction-as-Reasoning
  sft:
    max_length: 6200  # Longer context for multi-step reasoning and history
    num_train_epochs: 3  # More epochs for small dataset (31 samples)
    per_device_train_batch_size: 1  # Keep small for memory efficiency with VL model
    gradient_accumulation_steps: 4  # Effective batch size = 4 (better for small dataset)
    learning_rate: 1.0e-4  # Higher LR for LoRA (standard: 1e-4 to 2e-4)
    warmup_steps: 1  # Shorter warmup for small dataset
    # warmup_ratio: 0.1  # 10% warmup
    logging_steps: 1  # Log every step for small dataset monitoring
    save_steps: 5  # Save more frequently
    save_total_limit: 3
    bf16: false  # V100 doesn't support bf16, will auto-switch to fp16
  
  # Memory optimization
  # V100 32GB needs both gradient_checkpointing AND 4-bit quantization for VL models
  gradient_checkpointing: true  # Required to avoid OOM
  use_4bit: true  # Enable 4-bit quantization - reduces VRAM significantly and speeds up training
  
  # Image limit per sample (critical for training speed!)
  # Each image adds ~500MB VRAM and significant processing time
  # 26 images per sample is WAY too many - limit to 1-3 for practical training
  max_images_per_sample: 3  # Start with 1, increase if VRAM allows
  
  # LoRA configuration (Parameter-Efficient Fine-Tuning)
  # NOTE: MAI-UI uses SFT + GRPO (RL) paradigm, different from AgentCPM-GUI's vision LoRA approach
  # Vision encoder is typically frozen during SFT; Grounding is enhanced via RL Stage 1 (GRPO)
  lora:
    enabled: true  # Enable LoRA for memory-efficient SFT
    r: 16  # V100 32GB + 4bit: 适当提高表达能力
    alpha: 32  # 2x r for stable learning
    dropout: 0.05  # LoRA dropout
    target_modules:  # 只训练 attention 层（Qwen3-VL 官方推荐，节省显存）
      - q_proj
      - k_proj
      - v_proj
      - o_proj
  
  # NOTE: MAI-UI does NOT use vision LoRA for SFT
  # Grounding capability is enhanced via:
  #   1. Multi-perspective Instruction-as-Reasoning data
  #   2. RL Stage 1 with GRPO + Point-in-Box reward
  # tune_vision: false (not applicable for MAI-UI SFT)
    
  # RL training (optional)
  rl:
    enabled: false
    num_epochs: 10
    ppo_steps: 100
    batch_size: 32
    learning_rate: 1.0e-5
    gamma: 0.99
    lam: 0.95
    cliprange: 0.2
    vf_coef: 0.5
    num_parallel_envs: 4
    max_step: 50
    replay_buffer_size: 8

# Evaluation settings
evaluation:
  agent_type: "mai_ui_agent"
  max_step: 50
  enable_mcp: false
  
  # Test tasks (empty = all tasks)
  tasks: []
  
  # Batch evaluation
  batch_eval:
    enabled: true
    results_dir: "./batch_eval_results"
    generate_reports: true
    report_formats: ["markdown", "json", "html"]

# Environment settings (for RL training and evaluation)
environment:
  suite_family: "mobile_world"
  device: "emulator-5554"
  step_wait_time: 1.0
  env_name_prefix: "mobile_world_env"
  env_image: "mobile_world"

# Pipeline settings
pipeline:
  checkpoint_dir: "./pipeline_checkpoints"
  log_dir: "./pipeline_logs"
  
  # Stages to run
  stages:
    data_preprocessing: true
    data_validation: true
    sft_training: true
    sft_evaluation: true
    rl_training: false
    final_evaluation: true
  
  # Error handling
  stop_on_error: true
  retry_failed_stages: false
  max_retries: 0

# Logging and monitoring
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  
  # TensorBoard
  tensorboard:
    enabled: false
    log_dir: "./tensorboard_logs"
  
  # Weights & Biases
  wandb:
    enabled: false
    project: "mai-ui-training"
    entity: null

# Resource management
resources:
  # GPU settings
  gpu_ids: [0]
  mixed_precision: true
  
  # Memory
  dataloader_num_workers: 4
  prefetch_factor: 2
