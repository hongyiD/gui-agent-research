#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Data Building Pipeline for MAI-UI.

This script builds SFT datasets from trajectory logs (traj.json) generated by
the upstream MobileWorld runner, aligning with upstream action/observation formats.
"""

from __future__ import annotations

import argparse
import base64
import json
import os
import random
from io import BytesIO
from pathlib import Path
from typing import Any

import yaml # type: ignore
from PIL import Image
from tqdm import tqdm # type: ignore

from mobile_world.core.log_viewer.utils import (
    get_all_trajectory_steps,
    get_task_folders,
    get_task_status,
    get_task_tools,
)
from mobile_world.runtime.utils.trajectory_logger import SCORE_FILE_NAME


def parse_result_file(result_file: str) -> tuple[float | None, str | None]:
    """Parse result.txt to get score and reason."""
    if not os.path.exists(result_file):
        return None, None
    with open(result_file) as f:
        lines = f.readlines()
        score = None
        if len(lines) > 0 and "score:" in lines[0]:
            score = float(lines[0].split("score:")[1].strip())
        reason = lines[1].strip() if len(lines) > 1 else None
        return score, reason


def load_image_as_base64(image_path: str, resize: tuple[int, int] | None = None) -> str:
    """Load image and encode as base64."""
    img = Image.open(image_path)
    if resize:
        img = img.resize(resize, Image.Resampling.LANCZOS)
    buffer = BytesIO()
    img.save(buffer, format="PNG")
    img_bytes = buffer.getvalue()
    return base64.b64encode(img_bytes).decode("utf-8")


def build_prompt_from_trajectory(
    task_goal: str,
    history_steps: list[dict],
    current_step: dict,
    tools: list[dict] | None = None,
    include_images: bool = True,
    image_format: str = "path",
    log_dir: str | None = None,
) -> str:
    """Build prompt following MAI-UI format."""
    prompt_parts = []
    
    # System prompt (simplified version matching MAI_MOBILE_SYS_PROMPT_ASK_USER_MCP)
    system_prompt = """You are a GUI agent. You are given a task and your action history, with screenshots. You need to perform the next action to complete the task.

## Output Format
For each function call, return the thinking process in <thinking> </thinking> tags, and a json object with function name and arguments within <tool_call></tool_call> XML tags:
```
<thinking>
...
</thinking>
<tool_call>
{"name": "mobile_use", "arguments": <args-json-object>}
</tool_call>
```

## Action Space

{"action": "click", "coordinate": [x, y]}
{"action": "long_press", "coordinate": [x, y]}
{"action": "type", "text": ""}
{"action": "swipe", "direction": "up or down or left or right", "coordinate": [x, y]}
{"action": "open", "text": "app_name"}
{"action": "drag", "start_coordinate": [x1, y1], "end_coordinate": [x2, y2]}
{"action": "system_button", "button": "button_name"}
{"action": "wait"}
{"action": "terminate", "status": "success or fail"}
{"action": "answer", "text": "xxx"}
{"action": "ask_user", "text": "xxx"}
{"action": "double_click", "coordinate": [x, y]}
"""
    
    if tools:
        tools_str = "\n".join([json.dumps(tool, ensure_ascii=False) for tool in tools])
        system_prompt += f"\n## MCP Tools\nYou are also provided with MCP tools:\n{tools_str}\n"
    
    prompt_parts.append(system_prompt)
    prompt_parts.append(f"\n## Task Goal\n{task_goal}\n")
    
    # Add history
    if history_steps:
        prompt_parts.append("## Action History\n")
        for hist_step in history_steps[-3:]:  # Keep last 3 steps
            hist_action = hist_step.get("action", {})
            hist_prediction = hist_step.get("prediction", "")
            prompt_parts.append(f"Step {hist_step.get('step', '?')}: {hist_prediction}")
            if hist_action:
                prompt_parts.append(f"Action: {json.dumps(hist_action, ensure_ascii=False)}")
            prompt_parts.append("")
    
    # Current observation
    prompt_parts.append("## Current Observation\n")
    if include_images and log_dir:
        screenshot_path = os.path.join(log_dir, "screenshots", f"{current_step.get('step', 0)}.png")
        if os.path.exists(screenshot_path):
            if image_format == "base64":
                img_b64 = load_image_as_base64(screenshot_path)
                prompt_parts.append(f"<image_base64>{img_b64}</image_base64>")
            else:
                prompt_parts.append(f"<image_path>{screenshot_path}</image_path>")
    
    return "\n".join(prompt_parts)


def build_response_from_step(step: dict) -> str:
    """Build response from trajectory step."""
    prediction = step.get("prediction", "")
    return prediction


def process_trajectory(
    task_folder: str,
    config: dict[str, Any],
) -> list[dict[str, Any]]:
    """Process a single trajectory folder into SFT samples."""
    traj_steps = get_all_trajectory_steps(task_folder)
    if not traj_steps:
        return []
    
    # Check score filter
    result_file = os.path.join(task_folder, SCORE_FILE_NAME)
    score, _ = parse_result_file(result_file)
    if score is not None and score < config["filtering"]["min_score"]:
        return []
    
    # Check step count filter
    if len(traj_steps) < config["filtering"]["min_steps"]:
        return []
    if config["filtering"]["max_steps"] > 0 and len(traj_steps) > config["filtering"]["max_steps"]:
        traj_steps = traj_steps[: config["filtering"]["max_steps"]]
    
    # Get tools if available
    tools = get_task_tools(task_folder)
    
    samples = []
    task_goal = traj_steps[0].get("task_goal", "") if traj_steps else ""
    
    for i, step in enumerate(traj_steps):
        history = traj_steps[:i]
        prompt = build_prompt_from_trajectory(
            task_goal=task_goal,
            history_steps=history,
            current_step=step,
            tools=tools if tools else None,
            include_images=config["image_processing"]["include_images"],
            image_format=config["image_processing"]["image_format"],
            log_dir=task_folder,
        )
        response = build_response_from_step(step)
        
        sample = {
            "prompt": prompt,
            "response": response,
            "metadata": {
                "task_name": os.path.basename(task_folder),
                "step": step.get("step", i),
                "action": step.get("action", {}),
                "tool_call": step.get("tool_call"),
                "ask_user_response": step.get("ask_user_response"),
            },
        }
        samples.append(sample)
    
    return samples


def build_sft_dataset(config_path: str) -> None:
    """Build SFT dataset from trajectory logs."""
    with open(config_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    
    log_root = Path(config["data_sources"]["log_root"])
    if not log_root.exists():
        raise ValueError(f"Log root directory does not exist: {log_root}")
    
    task_folders = get_task_folders(str(log_root))
    if config["sampling"]["shuffle"]:
        random.seed(config["sampling"]["seed"])
        random.shuffle(task_folders)
    
    all_samples = []
    max_per_task = config["sampling"]["max_trajectories_per_task"]
    
    for task_folder_name in tqdm(task_folders, desc="Processing tasks"):
        task_folder = os.path.join(log_root, task_folder_name)
        status, score, _ = get_task_status(task_folder)
        
        if status != "Finished":
            continue
        
        samples = process_trajectory(task_folder, config)
        if samples:
            if max_per_task > 0:
                samples = samples[:max_per_task]
            all_samples.extend(samples)
    
    # Write output
    output_path = Path(config["output"]["output_path"])
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    samples_per_file = config["output"]["samples_per_file"]
    if samples_per_file > 0 and len(all_samples) > samples_per_file:
        # Shard output
        for shard_idx in range(0, len(all_samples), samples_per_file):
            shard_samples = all_samples[shard_idx : shard_idx + samples_per_file]
            shard_path = output_path.parent / f"{output_path.stem}_shard_{shard_idx // samples_per_file}.jsonl"
            with open(shard_path, "w", encoding="utf-8") as f:
                for sample in shard_samples:
                    f.write(json.dumps(sample, ensure_ascii=False) + "\n")
            print(f"Written {len(shard_samples)} samples to {shard_path}")
    else:
        with open(output_path, "w", encoding="utf-8") as f:
            for sample in all_samples:
                f.write(json.dumps(sample, ensure_ascii=False) + "\n")
        print(f"Written {len(all_samples)} samples to {output_path}")


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, required=True, help="Path to data_config.yaml")
    args = parser.parse_args()
    build_sft_dataset(args.config)


if __name__ == "__main__":
    main()
