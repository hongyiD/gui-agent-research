trainer$ python sft_trainer.py --config configs/my_config_119.yaml
Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.13.0
============================================================
Training Configuration Summary
============================================================
Model path: /workspace/MAI-UI-2B
Data path: /workspace/mai-ui-trainer/dataset/20260119_201327/sft_train.jsonl
Output directory: /workspace/mai-ui-trainer/trainer/models/sft_model/20260126_133019
Max length: 6200
Use LoRA: True
  LoRA r: 16
  LoRA alpha: 32
  LoRA dropout: 0.05
  LoRA target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']
Use 4-bit quantization: True
Gradient checkpointing: True
============================================================

Using 4-bit quantization with compute dtype: torch.float16
GPU compute capability (7, 0), using float16 (V100 or older)
Loaded as Qwen3VL model with multi-modal support
VL Model: True
Gradient checkpointing enabled
trainable params: 6,422,528 || all params: 2,133,954,560 || trainable%: 0.3010
LoRA applied successfully
Loading dataset from: /workspace/mai-ui-trainer/dataset/20260119_201327/sft_train.jsonl
Dataset size: 26
Sample keys: ['messages', 'metadata']
  messages: list with 3 items
    First item type: <class 'dict'>
  metadata: <class 'dict'>

Data contains images: True
Training with fp16=True (GPU does not support bfloat16, using float16)
Using MultiModalDataCollator for vision-language training (max 3 images/sample)
Image base directory: /workspace/mai-ui-trainer/dataset/20260119_201327

============================================================
DATASET PREPROCESSING
============================================================
  Max images per sample: 3
  Total samples: 26
  Data directory: /workspace/mai-ui-trainer/dataset/20260119_201327
Decoding images:   0%|                                                                                                 | 0/26 [00:00<?Decoding images:   4%|███▍                                                                                     | 1/26 [00:00<00:03,  6Decoding images:  12%|██████████▎                                                                              | 3/26 [00:00<00:02, 11Decoding images:  19%|█████████████████                                                                        | 5/26 [00:00<00:01, 12Decoding images:  27%|███████████████████████▉                                                                 | 7/26 [00:00<00:01, 11Decoding images:  35%|██████████████████████████████▊                                                          | 9/26 [00:00<00:01, 11Decoding images:  42%|█████████████████████████████████████▏                                                  | 11/26 [00:00<00:01, 12Decoding images:  50%|████████████████████████████████████████████                                            | 13/26 [00:01<00:01, 13Decoding images:  58%|██████████████████████████████████████████████████▊                                     | 15/26 [00:01<00:00, 13Decoding images:  65%|█████████████████████████████████████████████████████████▌                              | 17/26 [00:01<00:00, 13Decoding images:  73%|████████████████████████████████████████████████████████████████▎                       | 19/26 [00:01<00:00, 12Decoding images:  81%|███████████████████████████████████████████████████████████████████████                 | 21/26 [00:01<00:00, 12Decoding images:  88%|█████████████████████████████████████████████████████████████████████████████▊          | 23/26 [00:01<00:00, 13Decoding images:  96%|████████████████████████████████████████████████████████████████████████████████████▌   | 25/26 [00:01<00:00, 13Decoding images: 100%|████████████████████████████████████████████████████████████████████████████████████████| 26/26 [00:02<00:00, 12.67it/s]

============================================================
PREPROCESSING SUMMARY
============================================================
  Samples processed: 26
  Original images: 26 (1.0 avg/sample)
  After limiting: 26 (1.0 avg/sample)
  Avg image size: 1080x2400 pixels
============================================================

Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.

NOTE: First step may be slow due to JIT compilation and image processing.
      Watch for detailed step-by-step logs below.

The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.

======================================================================
TRAINING STARTED
======================================================================
  Total steps: 21
  Epochs: 3
  Batch size: 1
  Gradient accumulation: 4
  Effective batch size: 4
  Learning rate: 0.0001
  FP16: True, BF16: False
  Gradient checkpointing: True
  GPU: Tesla V100-SXM2-32GB
  GPU Memory: 31.7 GB total
======================================================================

  0%|                                                                                                                  | 0/21 [00:00<?, ?it/s]
--- Epoch 1/3 ---
[DEBUG] apply_chat_template: 0.05s, processor: 0.15s
[DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.12s
[DEBUG] apply_chat_template: 0.00s, processor: 0.12s
[DEBUG] apply_chat_template: 0.00s, processor: 0.11s
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.



[Step 1/21] time: 686.69s | VRAM: 2.4/31.7GB (8%) | ETA: 3.8h
  5%|████▉                                                                                                  | 1/21 [11:27<3:49:09, 687                                                                                                                                      {'loss': 1.57, 'grad_norm': 8.696393966674805, 'learning_rate': 0.0, 'epoch': 0.15}
  5%|████▉                                                                                                  | 1/21 [11:27<3:49:09, 687.46s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.11s
[DEBUG] apply_chat_template: 0.00s, processor: 0.10s
[DEBUG] apply_chat_template: 0.00s, processor: 0.14s
[DEBUG] apply_chat_template: 0.00s, processor: 0.14s
[Step 2/21] time: 684.49s | loss: 1.5700 | lr: 0.00e+00 | VRAM: 2.4/31.7GB (8%) | ETA: 3.6h
 10%|█████████▊                                                                                             | 2/21 [22:52<3:37:15, 686                                                                                                                                      {'loss': 1.5984, 'grad_norm': 12.643355369567871, 'learning_rate': 2e-05, 'epoch': 0.31}
 10%|█████████▊                                                                                             | 2/21 [22:52<3:37:15, 686.08s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.10s
[DEBUG] apply_chat_template: 0.00s, processor: 0.13s
[DEBUG] apply_chat_template: 0.00s, processor: 0.15s
[DEBUG] apply_chat_template: 0.00s, processor: 0.14s
[Step 3/21] time: 696.19s | loss: 1.5984 | lr: 2.00e-05 | VRAM: 2.4/31.7GB (8%) | ETA: 3.4h
 14%|██████████████▋                                                                                        | 3/21 [34:29<3:27:18, 691                                                                                                                                      {'loss': 1.3883, 'grad_norm': 9.098527908325195, 'learning_rate': 4e-05, 'epoch': 0.46}
 14%|██████████████▋                                                                                        | 3/21 [34:29<3:27:18, 691.01s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.10s
[DEBUG] apply_chat_template: 0.00s, processor: 0.12s
[Step 4/21] time: 700.08s | loss: 1.3883 | lr: 4.00e-05 | VRAM: 2.4/31.7GB (8%) | ETA: 3.3h
 19%|███████████████████▌                                                                                   | 4/21 [46:10<3:16:51, 694                                                                                                                                      {'loss': 1.3891, 'grad_norm': 7.858142852783203, 'learning_rate': 6e-05, 'epoch': 0.62}
 19%|███████████████████▌                                                                                   | 4/21 [46:10<3:16:51, 694.79s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.11s
[DEBUG] apply_chat_template: 0.00s, processor: 0.11s
[DEBUG] apply_chat_template: 0.00s, processor: 0.13s
[Step 5/21] time: 691.11s | loss: 1.3891 | lr: 6.00e-05 | VRAM: 2.4/31.7GB (8%) | ETA: 3.1h
 24%|████████████████████████▌                                                                              | 5/21 [57:41<3:04:58, 693                                                                                                                                      {'loss': 1.2408, 'grad_norm': 6.496952533721924, 'learning_rate': 8e-05, 'epoch': 0.77}
 24%|████████████████████████▌                                                                              | 5/21 [57:41<3:04:58, 693.67s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.10s
[DEBUG] apply_chat_template: 0.00s, processor: 0.12s
[DEBUG] apply_chat_template: 0.00s, processor: 0.14s
[Step 6/21] time: 674.28s | loss: 1.2408 | lr: 8.00e-05 | VRAM: 2.4/31.7GB (8%) | ETA: 2.9h
 29%|██████████████████████████▌                                                                  | 6/21 [1:08:57<2:51:53, 687.54s/it]{'loss': 0.9757, 'grad_norm': 4.346643924713135, 'learning_rate': 0.0001, 'epoch': 0.92}                                              
 29%|██████████████████████████▌                                                                  | 6/21 [1:08:57<2:51:53, 687.54s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.10s
[Step 7/21] time: 334.17s | loss: 0.9757 | lr: 1.00e-04 | VRAM: 2.3/31.7GB (7%) | ETA: 2.5h
{'loss': 0.7886, 'grad_norm': 3.4891176223754883, 'learning_rate': 9.375e-05, 'epoch': 1.0}                                           
 33%|███████████████████████████████                                                              | 7/21 [1:14:31<2:13:28, 572.06s/it]
--- Epoch 2/3 ---
[DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.10s
[DEBUG] apply_chat_template: 0.00s, processor: 0.10s
[DEBUG] apply_chat_template: 0.00s, processor: 0.10s
[DEBUG] apply_chat_template: 0.00s, processor: 0.08s
[Step 8/21] time: 669.67s | loss: 0.7886 | lr: 9.38e-05 | VRAM: 2.4/31.7GB (8%) | ETA: 2.3h
{'loss': 0.7984, 'grad_norm': 2.9982078075408936, 'learning_rate': 8.75e-05, 'epoch': 1.15}                                           
 38%|███████████████████████████████████▍                                                         | 8/21 [1:25:41<2:10:43, 603.32s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.10s
[DEBUG] apply_chat_template: 0.00s, processor: 0.13s
[DEBUG] apply_chat_template: 0.00s, processor: 0.15s
[Step 9/21] time: 667.81s | loss: 0.7984 | lr: 8.75e-05 | VRAM: 2.4/31.7GB (8%) | ETA: 2.2h
{'loss': 0.5163, 'grad_norm': 2.075021743774414, 'learning_rate': 8.125000000000001e-05, 'epoch': 1.31}                               
 43%|███████████████████████████████████████▊                                                     | 9/21 [1:36:50<2:04:43, 623.67s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.10s
[DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.10s
[Step 10/21] time: 669.11s | loss: 0.5163 | lr: 8.13e-05 | VRAM: 2.4/31.7GB (8%) | ETA: 2.0h
{'loss': 0.5491, 'grad_norm': 1.672989845275879, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.46}                               
 48%|███████████████████████████████████████████▊                                                | 10/21 [1:47:59<1:56:56, 637.85s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.13s
[DEBUG] apply_chat_template: 0.00s, processor: 0.14s
[Step 11/21] time: 668.28s | loss: 0.5491 | lr: 7.50e-05 | VRAM: 2.4/31.7GB (8%) | ETA: 1.8h
{'loss': 0.591, 'grad_norm': 2.080547332763672, 'learning_rate': 6.875e-05, 'epoch': 1.62}                                            
 52%|████████████████████████████████████████████████▏                                           | 11/21 [1:59:09<1:47:56, 647.61s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.13s
[DEBUG] apply_chat_template: 0.00s, processor: 0.13s
[DEBUG] apply_chat_template: 0.00s, processor: 0.15s
[Step 12/21] time: 667.60s | loss: 0.5910 | lr: 6.88e-05 | VRAM: 2.4/31.7GB (8%) | ETA: 1.6h
{'loss': 0.579, 'grad_norm': 1.9775160551071167, 'learning_rate': 6.25e-05, 'epoch': 1.77}                                            
 57%|████████████████████████████████████████████████████▌                                       | 12/21 [2:10:17<1:38:04, 653.89s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.10s
[Step 13/21] time: 667.15s | loss: 0.5790 | lr: 6.25e-05 | VRAM: 2.4/31.7GB (8%) | ETA: 1.5h
{'loss': 0.384, 'grad_norm': 1.2445783615112305, 'learning_rate': 5.6250000000000005e-05, 'epoch': 1.92}                              
 62%|████████████████████████████████████████████████████████▉                                   | 13/21 [2:21:25<1:27:44, 658.05s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.10s
[Step 14/21] time: 334.77s | loss: 0.3840 | lr: 5.63e-05 | VRAM: 2.3/31.7GB (7%) | ETA: 1.2h
{'loss': 0.4482, 'grad_norm': 1.307408094406128, 'learning_rate': 5e-05, 'epoch': 2.0}                                                
 67%|█████████████████████████████████████████████████████████████▎                              | 14/21 [2:27:00<1:05:23, 560.44s/it]
--- Epoch 3/3 ---
[DEBUG] apply_chat_template: 0.00s, processor: 0.10s
[DEBUG] apply_chat_template: 0.00s, processor: 0.10s
[DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.10s
[DEBUG] apply_chat_template: 0.00s, processor: 0.13s
[Step 15/21] time: 699.21s | loss: 0.4482 | lr: 5.00e-05 | VRAM: 2.4/31.7GB (8%) | ETA: 1.1h
{'loss': 0.4168, 'grad_norm': 1.2550888061523438, 'learning_rate': 4.375e-05, 'epoch': 2.15}                                          
 71%|█████████████████████████████████████████████████████████████████▋                          | 15/21 [2:38:40<1:00:14, 602.46s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.10s
[DEBUG] apply_chat_template: 0.00s, processor: 0.12s
[DEBUG] apply_chat_template: 0.00s, processor: 0.14s
[DEBUG] apply_chat_template: 0.00s, processor: 0.14s
[Step 16/21] time: 700.20s | loss: 0.4168 | lr: 4.37e-05 | VRAM: 2.4/31.7GB (8%) | ETA: 53.2m
{'loss': 0.3908, 'grad_norm': 1.171146273612976, 'learning_rate': 3.7500000000000003e-05, 'epoch': 2.31}                              
 76%|███████████████████████████████████████████████████████████████████████▌                      | 16/21 [2:50:21<52:41, 632.31s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.10s
[DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[Step 17/21] time: 712.47s | loss: 0.3908 | lr: 3.75e-05 | VRAM: 2.4/31.7GB (8%) | ETA: 42.9m
{'loss': 0.3776, 'grad_norm': 1.2938169240951538, 'learning_rate': 3.125e-05, 'epoch': 2.46}                                          
 81%|████████████████████████████████████████████████████████████████████████████                  | 17/21 [3:02:14<43:46, 656.56s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.10s
[Step 18/21] time: 672.58s | loss: 0.3776 | lr: 3.13e-05 | VRAM: 2.4/31.7GB (8%) | ETA: 32.2m
{'loss': 0.3915, 'grad_norm': 0.8917765021324158, 'learning_rate': 2.5e-05, 'epoch': 2.62}                                            
 86%|████████████████████████████████████████████████████████████████████████████████▌             | 18/21 [3:13:27<33:04, 661.51s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.14s
[DEBUG] apply_chat_template: 0.00s, processor: 0.14s
[Step 19/21] time: 702.31s | loss: 0.3915 | lr: 2.50e-05 | VRAM: 2.4/31.7GB (8%) | ETA: 21.6m
{'loss': 0.5138, 'grad_norm': 1.3946396112442017, 'learning_rate': 1.8750000000000002e-05, 'epoch': 2.77}                             
 90%|█████████████████████████████████████████████████████████████████████████████████████         | 19/21 [3:25:10<22:27, 673.95s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[DEBUG] apply_chat_template: 0.00s, processor: 0.12s
[DEBUG] apply_chat_template: 0.00s, processor: 0.12s
[DEBUG] apply_chat_template: 0.00s, processor: 0.14s
[Step 20/21] time: 692.58s | loss: 0.5138 | lr: 1.88e-05 | VRAM: 2.4/31.7GB (8%) | ETA: 10.8m
{'loss': 0.3989, 'grad_norm': 0.8666816353797913, 'learning_rate': 1.25e-05, 'epoch': 2.92}                                           
 95%|█████████████████████████████████████████████████████████████████████████████████████████▌    | 20/21 [3:36:44<11:19, 679.72s/it][DEBUG] apply_chat_template: 0.00s, processor: 0.09s
[Step 21/21] time: 350.55s | loss: 0.3989 | lr: 1.25e-05 | VRAM: 2.3/31.7GB (7%) | ETA: 0s
{'loss': 0.4405, 'grad_norm': 1.4720468521118164, 'learning_rate': 6.25e-06, 'epoch': 3.0}                                            
{'train_runtime': 13356.2681, 'train_samples_per_second': 0.006, 'train_steps_per_second': 0.002, 'train_loss': 0.7498440671534765, 'epoch': 3.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [3:42:36<00:00, 581.19s/it]
======================================================================
TRAINING COMPLETED
======================================================================
  Total time: 3.7h
  Total steps: 21
  Final loss: 0.4405
  Peak GPU memory: 12.73 GB
======================================================================

100%|██████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [3:42:36<00:00, 636.01s/it]
LoRA weights saved to /workspace/mai-ui-trainer/trainer/models/sft_model/20260126_133019
SFT training completed. Output saved to /workspace/mai-ui-trainer/trainer/models/sft_model/20260126_133019