# MAI-UI SFT 模型 vLLM 部署指南

本文档说明如何使用 vLLM 部署训练完成的 LoRA 模型。

## 前置条件

- vLLM >= 0.3.0（支持 LoRA adapter）
- 训练完成的 LoRA 模型路径：`/data/codes/mai-ui-trainer/trainer/configs/models/sft_model`
- 基础模型：`Tongyi-MAI/MAI-UI-2B` 或 `Tongyi-MAI/MAI-UI-8B`

## 方式一：vLLM 直接加载 LoRA（推荐）

vLLM 从 0.3.0 版本开始支持动态加载 LoRA adapter，无需合并权重即可部署。

### 1. 启动服务

```bash
python -m vllm.entrypoints.openai.api_server \
    --model Tongyi-MAI/MAI-UI-2B \
    --enable-lora \
    --lora-modules mai-ui-sft=/data/codes/mai-ui-trainer/trainer/configs/models/sft_model \
    --served-model-name MAI-UI-SFT \
    --host 0.0.0.0 \
    --port 8000 \
    --trust-remote-code \
    --max-model-len 8192
```

**参数说明：**

| 参数 | 说明 |
|------|------|
| `--model` | 基础模型路径（HuggingFace ID 或本地路径） |
| `--enable-lora` | 启用 LoRA adapter 支持 |
| `--lora-modules` | LoRA 模块配置，格式：`<名称>=<路径>`，可配置多个 |
| `--served-model-name` | 对外暴露的模型名称 |
| `--max-model-len` | 最大上下文长度（根据显存调整） |
| `--trust-remote-code` | 信任远程代码（MAI-UI 模型需要） |

### 2. 多 LoRA 配置

可以同时加载多个 LoRA adapter：

```bash
python -m vllm.entrypoints.openai.api_server \
    --model Tongyi-MAI/MAI-UI-2B \
    --enable-lora \
    --lora-modules \
        mai-ui-sft=/data/codes/mai-ui-trainer/trainer/configs/models/sft_model \
        mai-ui-v2=/path/to/another/lora \
    --host 0.0.0.0 \
    --port 8000 \
    --trust-remote-code
```

### 3. 调用 API

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="EMPTY"  # vLLM 本地部署不需要 API key
)

# 使用 LoRA adapter 名称作为 model
response = client.chat.completions.create(
    model="mai-ui-sft",  # 对应 --lora-modules 中定义的名称
    messages=[
        {"role": "system", "content": "You are a helpful GUI agent."},
        {"role": "user", "content": "打开设置应用"}
    ],
    max_tokens=2048,
    temperature=0.0
)

print(response.choices[0].message.content)
```

### 4. 使用 curl 测试

```bash
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "mai-ui-sft",
        "messages": [
            {"role": "system", "content": "You are a helpful GUI agent."},
            {"role": "user", "content": "打开设置应用"}
        ],
        "max_tokens": 2048,
        "temperature": 0.0
    }'
```

## 方式二：合并 LoRA 后部署

如果遇到 LoRA 动态加载问题，可以先合并权重再部署。

### 1. 合并 LoRA 权重

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# 加载基础模型
base_model_path = "Tongyi-MAI/MAI-UI-2B"
model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(base_model_path)

# 加载并合并 LoRA
lora_path = "/data/codes/mai-ui-trainer/trainer/configs/models/sft_model"
model = PeftModel.from_pretrained(model, lora_path)
model = model.merge_and_unload()

# 保存合并后的模型
merged_path = "/data/codes/mai-ui-trainer/trainer/configs/models/sft_model_merged"
model.save_pretrained(merged_path)
tokenizer.save_pretrained(merged_path)

print(f"模型已合并保存到: {merged_path}")
```

### 2. 部署合并后的模型

```bash
python -m vllm.entrypoints.openai.api_server \
    --model /data/codes/mai-ui-trainer/trainer/configs/models/sft_model_merged \
    --served-model-name MAI-UI-SFT \
    --host 0.0.0.0 \
    --port 8000 \
    --trust-remote-code \
    --max-model-len 8192
```

## 方式三：Docker 部署

### 使用合并后的模型

```bash
docker run -d --gpus all \
    -v /data/codes/mai-ui-trainer/trainer/configs/models/sft_model_merged:/model \
    -p 8000:8000 \
    --ipc=host \
    --name vllm-mai-sft \
    vllm/vllm-openai:latest \
    --model /model \
    --served-model-name MAI-UI-SFT \
    --trust-remote-code \
    --max-model-len 8192
```

### 容器管理

```bash
# 查看日志
docker logs -f vllm-mai-sft

# 停止容器
docker stop vllm-mai-sft

# 删除容器
docker rm vllm-mai-sft
```

## 验证服务

```bash
# 健康检查
curl http://localhost:8000/health

# 查看可用模型
curl http://localhost:8000/v1/models
```

## 常见问题

### 1. 显存不足

降低 `--max-model-len` 或使用量化：

```bash
python -m vllm.entrypoints.openai.api_server \
    --model Tongyi-MAI/MAI-UI-2B \
    --enable-lora \
    --lora-modules mai-ui-sft=/path/to/lora \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.9
```

### 2. LoRA 加载失败

检查 LoRA 模型目录结构：

```
sft_model/
├── adapter_config.json
├── adapter_model.safetensors (或 adapter_model.bin)
└── tokenizer_config.json (可选)
```

### 3. 多 GPU 部署

使用张量并行：

```bash
python -m vllm.entrypoints.openai.api_server \
    --model Tongyi-MAI/MAI-UI-8B \
    --tensor-parallel-size 2 \
    --enable-lora \
    --lora-modules mai-ui-sft=/path/to/lora \
    --host 0.0.0.0 \
    --port 8000
```

## 与 MAI-UI WebUI 集成

启动 vLLM 服务后，在 `MAI-UI-WebUI/model_config.yaml` 中配置：

```yaml
vllm_local:
  display_name: "MAI-UI SFT (本地)"
  api_base: "http://localhost:8000/v1"
  api_key: ""
  default_model: "mai-ui-sft"
```

然后启动 WebUI：

```bash
cd MAI-UI-WebUI
python start_web_ui.py
```

在 WebUI 中选择 "MAI-UI SFT (本地)" 即可使用训练后的模型。
