# MOBILE-AGENT-v3: FUNDAMENTAL AGENTS FOR GUI AUTOMATION

![](images/9045741a63955fcd412bf5a87ec31e4f409ba62fb972b429324cc1ed6059d90d.jpg)

# ABSTRACT

This paper introduces GUI-Owl, a foundational GUI agent model that achieves new state-of-the-art performance among open-source end-to-end models across ten GUI benchmarks spanning both desktop and mobile environments, covering grounding, question answering, planning, decision-making, and general procedural knowledge in GUI automation scenarios. Notably, GUI-Owl-7B achieves a score of 66.4 on the AndroidWorld benchmark and 29.4 on the OSWorld-Verified benchmark. Building on this model, we propose a general-purpose GUI agent framework, Mobile-Agent-v3, which further enhances GUI-Owl's performance (73.3 on AndroidWorld and 37.7 on OSWorld-Verified), achieving a new state-of-the-art among GUI agent frameworks based on open-source models. GUI-Owl incorporates several key innovations: 1) Large-scale Environment Infrastructure: We introduce a cloud-based virtual environment infrastructure spanning different operating systems (including Android, Ubuntu, macOS, and Windows). This underpins our Self-Evolving GUI Trajectory Production framework, which generates high-quality interaction data through sophisticated query generation and correctness judgment. The framework leverages GUI-Owl's capabilities to continuously refine trajectories, creating a self-reinforcing improvement cycle. It supports multiple downstream data pipelines, enabling robust data collection while reducing manual annotation needs. 2) Diverse Foundational Agents Capability Construction: by incorporating foundational UI data—such as grounding, planning, and action semantic recognition—alongside diverse reasoning and reflecting patterns, GUI-Owl not only supports end-to-end decision making but can also serve as a specialized module integrated into multi-agent frameworks; 3) Scalable Environment RL: we also develop a scalable reinforcement learning framework that enables fully asynchronous training and better aligns the model's decision with real-world usage. In addition, we introduce Trajectory-aware Relative Policy Optimization (TRPO) for online environment RL, which achieves 34.9 on the OSWorld-Verified benchmark. GUI-Owl and Mobile-Agent-v3 are open-sourced at: https://github.com/X-PLUG/MobileAgent.

![](images/4a0eecc2e0ef8e7ac0155472c183f9fc976f28abb97b66a83e49100765794f5e.jpg)  
OSWorld-G

![](images/0f2317ca4601215142a3c9071826b4e768cda6a4d57c00356e50f0e76133390b.jpg)  
ScreenSpotPro

![](images/ea3b7bc5d9f8a8f122d9b1c70483b2f4d3c791bb57a49b7a9bb39e66dc267c71.jpg)  
MMBench-GUI L1 (Hard)

![](images/90badae93817c7c8289779ab5d52bee7c577f4bc5140b0f010b931b358454b7e.jpg)  
Figure 1: Performance overview on mainstream GUI-automation benchmarks.

![](images/e2584ef727889f4b707578ad8b924db7714c7308d6c6cf3879b4aba8a8dec0a8.jpg)

![](images/92cc3a82fb3599f4a43134d10c5db64bb33ab5db4846af890956e7869523bd27.jpg)

![](images/eb24c8df4c9e8602c80ac3e456bec0d12973d8f74060c4fdaaace53deeac9ca1.jpg)

![](images/60e7d3bb8f7ad9431e690d55e10f7edaa50d1ec48686b57919f0848a6e0e00da.jpg)

![](images/00441286434bfe8a29d17abf03e0669cc3e33e931ca3cec590bbd1cea7e1ee98.jpg)  
Figure 2: Overview of our Mobile-Agent-v3. We illustrate our multi-platform environment supporting, our core capability, and some GUI automation examples generated by Mobile-Agent-v3.

# 1 INTRODUCTION

Graphical User Interfaces (GUIs) Agents (Hu et al., 2024; Zhang et al., 2024a; Nguyen et al., 2024; Wang et al., 2024d; Gao et al., 2024; Wang et al., 2024a) is designed to automate daily and professional tasks based on human instructions across various device environments, thereby enhancing production efficiency. With the rapid advancement of multimodal large models and reasoning technologies, vision-based GUI agents have demonstrated strong task execution capabilities across various device environments, including PCs, mobile devices, and web platforms.

The existing methods can be broadly divided into two categories. The first category builds agent frameworks based on closed-source models (Yang et al., 2025; Xie et al., 2025; Agashe et al., 2025; Song et al., 2025; Wang et al., 2024b), however, these approaches struggle to handle unfamiliar tasks and adapt to dynamic environments. The second category focuses mainly on end-to-end model performance (Qin et al., 2025; Wang et al., 2025a), but such methods often fail to follow instructions faithfully and lack compatibility with diverse agent frameworks, significantly limiting their practical utility. The GUI agents require this foundational model to have the following capabilities: 1) The strong UI perception capabilities (such as for Mobile, PC, and Web); 2) The planning, reflection, and reasoning in various dynamic environments; 3) The flexibility to integrate with various multi-agent frameworks.

In this paper, we propose GUI-Owl, a native end-to-end multimodal agent designed as a foundational model for GUI automation. Built upon Qwen2.5-VL and extensively post-trained on large-scale, diverse GUI interaction data, GUI-Owl unifies perception, grounding, reasoning, planning, and action execution within a single policy network. The model achieves robust cross-platform interaction, handling multi-turn decision making with explicit intermediate reasoning, and supports both autonomous operation and role-specific deployment in multi-agent systems. To further enhance its adaptability, we develop specialized datasets for core foundational tasks—including UI grounding, task planning, and action semantics—and employ a scalable reinforcement learning framework to align GUI-Owl's decision policy with real-world task success. Beyond single-agent deployment, GUI-Owl can be instantiated as different specialized agents within a multi-agent framework Mobile-Agent-v3 where multiple role

agents coordinate and share partial observations and reasoning traces to tackle complex, long-horizon automation workflows.

Large-scale Environment Infrastructure. To train our GUI agent, we developed a comprehensive large-scale environment infrastructure for GUI interaction data collection. This infrastructure leverages cloud-based technologies, including cloud phones and cloud computers on Alibaba Cloud (Cloud, 2018), spanning mobile, PC, and web platforms. It creates dynamic virtual environments that enable diverse and realistic interaction scenarios across various operating systems and devices. Central to this infrastructure is our Self-Evolving GUI Trajectory Production pipeline. This innovative system collects high-quality trajectory data through a sophisticated process involving: high-quality query generation that mimics real-world user interactions, model roll-outs where GUI-Owl and Mobile-Agent-v3 interacts with virtual environments, rigorous correctness judgment to ensure data quality, and query-specific guidance generation for challenging scenarios. This self-evolving pipeline creates a continuous improvement cycle, enhancing both our dataset quality and the GUI-Owl model's capabilities over time. By combining cloud technology with multi-platform environments, our infrastructure enables efficient, scalable model development while reducing manual annotation needs.

Diverse Foundational Agents Capability Construction. Based on the generated trajectories, we introduce multiple downstream data construction pipelines to enhance the agent's fundamental UI capabilities, including: (i) a grounding pipeline that covers both UI element localization—based on functional, appearance, and layout instructions—and fine-grained word/character grounding; (ii) a task planning pipeline that distills procedural knowledge from historical successful trajectories and large-scale pretrained LLMs to handle long-horizon, multi-application tasks; and (iii) an action semantics pipeline that captures the relationship between actions and resulting state transitions through before/after UI observations. Furthermore, we synthesize reasoning and reflecting data using offline hint-guided rejection sampling, distillation from a multi-agent framework, and iterative online rejection sampling. This supervision enables the agent to perform independent reasoning and to engage in complex, long-horizon collaborative reasoning within the Mobile-Agent-v3 framework, adapting its reasoning style to the specific role it assumes.

Scalable Environment RL. We also develop a scalable training framework grounded in a unified interface for multi-task training that standardizes interactions for both single-turn reasoning and multi-turn agentic tasks, and we decouple experience generation from policy updates to provide fine-grained control over policy adherence. This design supports fully asynchronous training and better aligns the model's decision-making with real-world usage. We further introduce Trajectory-aware Relative Policy Optimization (TRPO) to address training with long, variable-length action sequences in online-environment RL. TRPO uses trajectory-level rewards to compute step-level advantages and employs a replay buffer to improve the stability of reinforcement learning.

We evaluate GUI-Owl across a wide range of benchmarks that comprehensively measure native agent capability on GUI automation including grounding, single-step decision, general question answering and evaluate on online environment. GUI-Owl-7B outperforms all state-of-the-art models of comparable size. In particular, GUI-Owl-7B achieves scores of 34.9 on OSWorld-Verified and 66.4 on AndroidWorld. Moreover, GUI-Owl-32B demonstrates outstanding performance, surpassing even proprietary models. On MMBench-GUI and AndroidControl, GUI-Owl-32B outperforms all models, including GPT-4o and Claude 3.7. In grounding capability evaluations, GUI-Owl-32B surpasses all models of the same size and achieves competitive performance compared with proprietary models. When combined with Mobile-Agent-v3, it achieves scores of 37.7 on OSWorld and 73.3 on AndroidWorld, which clearly demonstrates its capability as a fundamental agent for GUI automation.

# 2 GUI-OwL

GUI-Owl is an end-to-end multimodal model that unifies capabilities such as perception, planning, decision-making, and grounding within GUI scenarios. By leveraging extensive and diverse datasets for post-training based on Qwen2.5-VL, GUI-Owl is able to interact with graphical user interfaces on Mobile, PC, and Web platforms. We further apply reinforcement learning to GUI-Owl to align its capabilities with diverse downstream requirements. This alignment enables the model not only to autonomously perform multi-turn GUI interaction tasks, but also to generalize to specific applications such as question answering, captioning, planning, and grounding. Moreover, GUI-Owl can assume various roles within a multi-agent framework, in which individual agents fulfill their respective responsibilities, coordinate their actions, and collaboratively accomplish more complex tasks.

# 2.1 END-TO-END GUI INTERACTIONS

We model the interaction process between GUI-Owl and the device, as well as the completion of the specified task, as a multi-turn decision-making process. Given the available action space  $A = \{a^1, a^2, \ldots, a^{|A|}\}$  of the environment, the current environment observation  $S_t \in S$ , which can be a screenshot in common, and the history

![](images/5268538020ed3daff05ab27208d356c107d1475ae4d59416fddd280715816dd6.jpg)  
Figure 3: Illustration of the interaction flow of GUI-Owl. The system message defines the available action space, the user message contains the task instruction, compressed histories, and current observation, while the response message includes the agent's reasoning, action summaries, and the final action output.

of past operations  $H_{t} = \{(S_{1},a_{1}),(S_{2},a_{2}),\ldots ,(S_{t - 1},a_{t - 1})\}$ , the model selects an action from the action space  $A$  and executes it in the environment to obtain the next time-step's observation  $S_{t + 1}$ .

Formally, at each time step  $t$ ,  $a_{t} \sim \pi(\cdot \mid S_{t}, H_{t})$ , here,  $\pi$  denotes policy model (GUI-Owl), which maps the current observation and historical operations to a probability distribution over the action space  $A$ .

We present the interaction flow of GUI-Owl in Figure 3. In practice, we support flexible prompts to organize the action space into system messages. By default, we adopt the Qwen function calling format. Detail action space definition is presented in Table 9 and Table 10. For user messages, we sequentially provide the original task, historical information, and observations. To save GPU memory and improve inference speed, we typically retain only the most recent 1 to 3 images. Notably, requiring a robust reasoning process before the actual output of an action decision can enhance the model's ability to adapt to complex tasks and situations. Therefore, we require the model to first "reasoning" before making a decision, and then execute the action based on this reasoning content. However, since lengthy thoughts over multiple turn interactions may cause the conversation history to become excessively long, we additionally require the model to output a "conclusion" summarizing the key information of the current step. Finally, only the conclusion is stored in the historical context.

The actions output by the model are translated into actual device operation commands (for example, we use ADB commands for Android devices, and pyautogui code for desktop operations). Meanwhile, the latest screenshot of the device's display is further captured and used as the observation of the environment.

# 2.2 FOUNDATIONAL AGENTS CAPABILITY

GUI-Owl can function not only as a native agent capable of independently interacting with GUIs, but also provides a variety of foundational capabilities to support downstream standalone calls or integration into a multi-agent framework. To this end, we collect and construct datasets for various capabilities such as grounding, caption and planning. These datasets are mixed with general instruction data during training, and we found that the model also possesses zero-shot GUI question-answering capability as well as general instruction-following abilities for unseen tasks.

# 2.2.1 SELF-EVOLVING TRAJECTORY PRODUCTION FRAMEWORK

To scale up the trajectory data, we propose a Self-Evolving GUI Trajectory Production pipeline, which contrasts with traditional methods that strongly rely on manual annotation (Wang et al., 2025a; Qin et al., 2025). This framework leverages the capabilities of GUI-Owl itself, continuously generating new trajectories through roll-out and assessing their correctness to obtain large-scale high-quality interaction data. Subsequently, these data are

![](images/5df1fa91a7a92e52b0f57843020d8f9d6d5562e5cf9617673b8ce229afe08f99.jpg)  
Figure 4: Illustration of the our self-evolving trajectory data production pipeline.

utilized to enhance the model's capabilities, creating a reinforcing cycle of improvement. As shown in Figure 4, the process begins with constructing dynamic virtual environments across mobile, PC, and web platforms, paired with high-quality query generation. Given these queries, the GUI-Owl model and Mobile-Agent-v3 framework performs step-by-step actions in the environments to produce roll-out trajectories. A Trajectory Correctness Judgment Module then evaluates these trajectories at both the step and trajectory levels to identify and filter out errors. For difficult queries, a Query-specific Guidance Generation module provides human- or model-generated ground-truth trajectories to guide the agent. The cleaned and enriched data is then used for reinforcement fine-tuning, enabling the model to iteratively improve its ability to generate successful GUI trajectories, thereby reducing human annotation needs and achieving continuous self-improvement. More details can be found in Section 5.4.

- High-Quality Query Generation. For mobile apps, we developed a screenshot-action framework utilizing a human-annotated Directed Acyclic Graph (DAG) (Patil et al., 2025) that models realistic navigation flows and captures multi-constraint user queries. The process involves path sampling, metadata extraction, instruction synthesis using LLMs, refinement through few-shot LLM prompting, and interface validation via web crawlers. This framework minimizes LLM hallucinations while ensuring realistic and controllable query generation. For computer applications, we addressed the challenges of atomic operational skills and software operational pathways. We combined manual annotation and LLM-assisted generation to create queries for atomic operations (e.g., clicking, scrolling, dragging) and complex software interactions. We utilized accessibility trees and deep-search chains to acquire operational pathways, and employed MLLMs to generate executable commands based on screenshots and exemplar inputs. This comprehensive approach ensures diverse, realistic, and accurate query generation across different GUI environments.  
- Trajectory Correctness Judgment Module. Our Trajectory Correctness Judgment Module employs a two-tiered system to evaluate the quality of generated GUI trajectories. It consists of a Step-Level Critic and a Trajectory-Level Critic. The Step-Level Critic analyzes individual actions within a trajectory by examining pre-action and post-action states, producing an analysis, summary, and categorical annotation (GOOD, NEUTRAL, HARMFUL) for each step. The Trajectory-Level Critic assesses the overall trajectory using a dual-channel approach: a Textual Reasoning Channel leveraging large language models, and a Multi-Modal Reasoning Channel incorporating both visual and textual data. The final correctness judgment is determined through a consensus mechanism. This comprehensive approach ensures robust evaluation of GUI trajectories, combining granular step-level insights with holistic trajectory-level assessment to maintain high-quality training data for our GUI-Owl model.  
- Query-specific Guidance Generation. This module utilizes successful trajectories to create guidance for improved model performance. The process involves: (1) Action Description: A VLM generates descriptions for each action's outcome in reference trajectories. Inputs include pre- and post-action screenshots and action decisions. For coordinate-based actions, we highlight interaction points to aid VLM analysis. (2) Quality Control: For model-generated trajectories, the VLM cross-references the model's decision rationale to validate step effectiveness, filtering out suboptimal actions. (3) Guidance Synthesis: Action descriptions are concatenated and fed into a Large Language Model (LLM), which summarizes the essential steps required to complete the query, producing query-specific guidance. This

approach enables the generation of targeted guidance, potentially improving the model's ability to handle complex queries and reducing the need for extensive rollouts or manual annotations.

# 2.2.2 DIVERSE GUI DATA SYNTHESIS

![](images/8cf4753aea6c851ac30e4c29582bcdd48e7c206e5e8b4b0ec1988932e2fe686a.jpg)  
Figure 5: Overview of our grounding data construction pipeline.

Grounding. Accurate localization and semantic understanding of graphical user interface elements are essential for the development of robust and reliable visual interface agents, with these capabilities primarily embodied in grounding. As illustrated in Figure 5, to improve grounding capabilities, we construct two types of grounding task datasets from multiple data sources.

- For UI element grounding (with function-based or appearance- & layout-based instruction), we collect data from three sources: 1) Open-source datasets: Publicly released GUI datasets are utilized from UI-Vision (Nayak et al., 2025), and GUI-R1 (Luo et al., 2025). 2) Grounding data synthesis using A11y tree: Extracting bounding boxes and functional descriptive information of UI elements through the accessibility (a11y) tree in both mobile and computer environments. And the appearance and layout descriptions are additionally generated using MLLMs such as Qwen2.5VL (Bai et al., 2025). 3) Dense grounding generation on crawled PC images: To tackle the scarcity of PC grounding data, diverse screenshots are crawled from Google Images using popular app names as keywords. Given the high density and visual complexity of UI components in PC screenshots, we employ SAM (Kirillov et al., 2023) to segment images into subregions, enabling more precise grounding. MLLMs then perform dense grounding within each segmented region.

To reduce noise and further improve the quality, we clean the collected grounding annotations by comparing them with UI detection results from Omniparser V2 (Yu et al., 2025) (bounding boxes with  $IoU < \tau_g$  are removed, where  $\tau_g = 0.5$ ). Additionally, we rephrase the original instructions into more natural, task-oriented descriptions using LLMs (e.g., Qwen2.5-Max (Team, 2024)).

- For fine-grained words and characters grounding, we collect a set of document images and employ OCR tools to extract textual content and their corresponding spatial positions. Based on the annotated data, we build fine-grained text localization data to enable precise grounding of specific words and characters.

Task Planning. As foundational agents are often used to accomplish long-horizon, complex tasks, the model needs to possess background knowledge of complex task planning. We construct such data from two perspectives:

- 1) Distilling from Historical Trajectories. Given historical successful trajectory data, we first construct fine-grained descriptions of each page transition. This information is then combined with the model's

historical actions and organized into a task execution manual through an LLM. By feeding this manual into GUI-Owl, we evaluate its quality based on changes in the task completion rate.

- 2) Distilling from Large-scale Pretrained LLM. To further enhance the model's generalization on various tasks, we further distill knowledge from large-scale pretrained LLMs. First, we collect a list which covering mainstream apps, and then use either human annotators or models to synthesize plausible tasks. These tasks are designed to be as complex as possible and to span multiple features and even multiple applications; task specifications with obvious errors are filtered out. We then feed these tasks to a language model (e.g., Qwen3-235B) and further consolidate and clean the resulting plans, yielding task-specific planning data.

Action Semantics. We notice that a model's ability to perceive how actions affect page-state changes is crucial. Based on collected trajectories, we extensively collect a large corpus of pre- and post-action screenshots and construct a two-tier dataset. At the first tier, we require the model to directly predict the intervening action—including its type and parameters—based on the before-and-after images; such data can be obtained directly from offline-collected trajectories.

Subsequently, we ask the model to produce a natural-language description that covers both the executed action and its effects. To construct annotations for this data, we design a workflow that first generates an action description from the pre-action screenshot and the given action parameters (for coordinate-aware actions, the target location is drawn on the image to cue the model), using a multimodal model (e.g., Qwen-VL-Max). We then use the same multimodal model with the before-and-after images to analyze page changes and assess whether the changes are semantically consistent with the action. Through multiple rounds of voting, we retain the higher-scoring action descriptions.

# 2.2.3 ENHANCED ROBUST REASONING

Reasoning ability is essential for a fundamental agent, as it enables the model to move beyond merely imitating action sequences and instead capture the underlying logic that governs them. To this end, we first propose a set of diverse data synthesis strategies to enrich the variety of reasoning patterns. We then integrate reinforcement learning to further align these reasoning patterns with the dynamics of real-world environments.

Offline Hint-Guided Rejection Sampling. We synthesize reasoning data via rejection sampling. Specifically, given a collected trajectory

$$
T = \left\{\left(a _ {0}, S _ {0}\right), \left(a _ {1}, S _ {1}\right), \dots , \left(a _ {t}, S _ {t}\right) \right\},
$$

we prompt VLMs to generate reasoning content for each step based on its preceding history. The generated reasoning is then separated from the original context and used independently for action prediction. We evaluate the validity of each reasoning sequence by checking whether the predicted action matches the ground-truth action.

To encourage diversity in reasoning patterns, we adopt hints of different styles, for instance, requiring the model to follow a fixed chain-of-thought template or encouraging it to produce the simplest possible reasoning process. During this procedure, we observe that, for certain steps, the VLMs struggle to obtain actions consistent with the ground truth. For such cases, we first manually verify the correctness of the ground-truth action. If the action is deemed reasonable, we then feed it back to the VLMs to guide the generation of reasoning conclusions consistent with that action type.

Distillation from Multi-Agent Framework. We note that even when style prompts are provided to encourage end-to-end reasoning generation, the model can still be influenced by certain inherent biases, which in turn limit reasoning diversity. In contrast, a multi-agent framework decomposes a single-step decision into the collaboration of multiple specialized roles, each approaching the current step from a different perspective. Since each agent focuses exclusively on its own subtask, it can more effectively avoid such biases. Motivated by this observation, we collect the outputs of individual agents from the Mobile-Agent-v3, and employ a large language model to integrate their diverse reasoning contents into a unified end-to-end reasoning output. The resulting reasoning content is paired with the action sequences obtained from Mobile-Agent-v3, forming the training dataset for reasoning.

Iterative Online Rejection Sampling. We observe that improving the base model's reasoning capability also enhances its ability to accomplish a wider range of tasks. Moreover, newly generated trajectory data can be further exploited for model training. Therefore, we adopt an iterative online rejection sampling framework, in which our model rolls out trajectories on query data under two modes:

1. End-to-end generation: the model directly generates reasoning and action predictions in an end-to-end fashion, which is used to improve its holistic reasoning capability.  
2. Integration with Mobile-Agent-v3: GUI-Owl is incorporated into the Mobile-Agent-v3 framework. The inputs and outputs are collected to train the corresponding agent role models.

Formally, given query data  $\mathcal{Q}$  and a model  $M^{(k)}$  at iteration  $k$ , trajectories are generated as:

$$
\mathcal {T} ^ {(k)} = \operatorname {R o l l o u t} \left(M ^ {(k)}, \mathcal {Q}\right), \tag {1}
$$

where  $\mathcal{T}^{(k)}$  contains both end-to-end outputs  $\mathcal{T}_{\mathrm{E2E}}^{(k)}$  and role-specific outputs  $\mathcal{T}_{\mathrm{Role}}^{(k)}$ . The newly collected trajectories are then used to update the model:

$$
M ^ {(k + 1)} = \operatorname {T r a i n} \left(M ^ {(k)}, \mathcal {T} _ {\text {f i l t e r e d}} ^ {(k)}\right). \tag {2}
$$

Directly training on all collected steps often yields a suboptimal model. To address this, we apply the following filtering and balancing strategies:

1. Critic-based filtering: A Critic pipeline scores each step  $s_t \in \mathcal{T}^{(k)}$ , and those with scores below a threshold  $\tau_c$  are removed:

$$
\mathcal {T} _ {\text {f i l t e r e d}} = \left\{s _ {t} \mid \text {C r i t i c S c o r e} \left(s _ {t}\right) \geq \tau_ {c} \right\}.
$$

2. Thought-action consistency check: We verify the logical consistency between the reasoning content (thought) and the executed action. Steps that fail this check are discarded.  
3. Task re-weighting: Let  $p_{\mathrm{succ}}(task)$  denote the success rate of a given task. For tasks with high  $p_{\mathrm{succ}}$ , we downsample their training occurrence, while for tasks with low  $p_{\mathrm{succ}}$ , we upsample their instances to ensure balanced learning.  
4. Reflector balancing: We observe that the Reflector Agent predominantly produces positive outputs, leading to class imbalance. We recalibrate its data as follows:

- If the Reflector marks step  $i$  as negative and this feedback causes step  $i + 1$  to be judged positive, the feedback for step  $i$  is retained.  
- Otherwise, we retain Reflector inputs and responses only from trajectories where all steps are judged positive by the reflector.

Finally, we re-balance the dataset so that positive and negative samples have equal size.

# 3 TRAINING PARADIGM

GUI-Owl is initialized from Qwen2.5-VL and trained through a three-stage process designed to progressively enhance its capabilities in GUI understanding, reasoning, and robust execution.

- Pre-training Phase: We collect a large-scale pre-training corpus covering fundamental UI understanding, interaction trajectory data, and general reasoning data. This data is used to continually pre-train Qwen2.5-VL, strengthening its grounding in basic GUI element recognition, action prediction, and general reasoning, thereby establishing a strong foundation for subsequent interaction-oriented training.  
- Iterative Tuning Phase: We deploy the model in real-world environments such as desktops and mobile devices to perform large-scale task execution. The resulting trajectories are cleaned, scored, and further transformed into diverse reasoning datasets, which are then used for offline training. This iterative Tuning process enables GUI-Owl to accumulate effective reasoning patterns applicable across varied scenarios, improving its adaptability and decision-making in complex UI tasks.  
- Reinforcement Learning Phase: We develop an asynchronous RL framework that allows the model to efficiently learn from direct interaction with real environments. This phase focuses on reinforcing successful behaviors and increasing execution consistency, thereby improving both the success rate and stability of GUI-Owl in practical deployments.

# 3.1 SCALABLE REINFORCEMENT LEARNING

# 3.1.1 INFRASTRUCTURE

Where enriched trajectory and reasoning data expand the model's knowledge base and reasoning capabilities, the model is expected to exhibit lower uncertainty and higher stability in real-world usage. Therefore, we further introduce reinforcement learning to better align GUI-Owl with practical application.

To facilitate an efficient and flexible training framework for training with environment multi-turn interactions, we develop a general infrastructure with the following key features:

- Unified Interface for multi-task training: Our framework is built on a unified task plug-in interface that standardizes interactions for both single-turn reasoning and complex, multi-turn agentic tasks. This modular design allows diverse new environments and tasks to be seamlessly integrated without altering the core infrastructure.

![](images/3244791ca57cc5a2025ab411e081ea191aafa00b25c3452ae96eea01d6531a5f.jpg)  
Figure 6: Overview of our scalable RL infrastructure, which unifies single-turn reasoning and multi-turn agentic training in a fully decoupled rollout-updated framework. All components can run in parallel for high throughput, with diverse task-specific interactions plugged into the scalable experience maker with a unified interface. A rollout manager assigns task IDs, collects trajectories and rewards, and coordinates data flow via a shared data center.

- Decoupled, Controllable Rollout: We decouple the experience generation (rollout) phase from policy updates, giving operators precise control over the entire data supply chain. This control is multi-faceted: the manager can dictate the degree of policy-adherence, from a strictly synchronous on-policy mode to an asynchronous, slightly off-policy mode for speed. It also has full control over resource allocation, deploying rollouts on hardware optimized for inference to maximize throughput. This granular control enables us to fine-tune the data generation process to achieve an optimal balance among optimization guarantees, speed, and cost.

# 3.1.2 TASK MIXTURE

We apply GRPO (Guo et al., 2025) to train GUI-Owl on static tasks, and apply the trajectory-aware Relative Policy Optimization (TRPO) strategy for training in online environments. In this section, we present the data preparation methods for different downstream reinforcement learning tasks and introduce trajectory-aware relative policy optimization.

For grounding task, a subset of data from GUI-R1 (Luo et al., 2025), UI-Vision (Nayak et al., 2025) serves as the foundational dataset. To further enhance RL performance in challenging fine-grained grounding, a curated collection of high-difficulty fine-grained grounding samples is incorporated (i.e., where the target UI regions occupy less than  $0.1\%$  of the entire screenshot area). Subsequently, the all datasets are performed  $n_g$  sampling iterations (where  $n_g = 8$  in our implementation) utilizing the policy model GUI-Owl before RL, and sample instances that exhibit partial failure cases as training corpus for RL optimization.

To enhance the capabilities of low-level (i.e., step-level) actions, we introduce single-turn reinforcement learning. The training data is derived directly from individual steps within high-quality offline interaction trajectories.

While the preceding RL phases build foundational skills, applying them to complex, multi-step tasks in an online environment introduces significant challenges. Therefore, we also conduct online reinforcement learning for GUI-Owl on virtual environments. These tasks are selected from the training task pool and use either rule-based or critic-based rewards as feedback signals for determining task completion.

Trajectory-aware Relative Policy Optimization for Online Environment RL. Real-world user tasks are often characterized by long and variable-length action sequences. In such scenarios, rewards are typically sparse and only available as a delayed success signal upon task completion. To overcome these obstacles, we employ a trajectory-aware relative policy optimization strategy (TRPO) extended to GRPO (Guo et al., 2025). This approach circumvents the challenge of assigning per-step rewards, a task that is nearly impossible to perform accurately in complex GUI interactions. Instead, we evaluate the entire trajectory  $\tau$  after its completion. Our experiments are conducted on the OSWorld-Verified benchmark (Xie et al., 2024), where the outcome of each task is programmatically verifiable, allowing us to obtain a reliable, single, holistic reward scalar  $R(\tau)$ . Specifically, this reward is the sum of an accuracy reward (1 for a successful trajectory, 0 otherwise) and a format reward, which penalizes malformed actions with a value of -0.5.

This trajectory-level reward is then used to compute a normalized advantage estimate, which provides a stable learning signal across all steps of the trajectory. The advantage for a given trajectory  $\tau$  is calculated as:  $\hat{A}_{\tau} = \frac{R(\tau) - \bar{R}}{\sigma_R + \epsilon}$ , where  $\bar{R}$  and  $\sigma_{R}$  are the running mean and standard deviation of rewards observed across multiple trajectories. This normalized advantage  $\hat{A}_{\tau}$  is then uniformly distributed to every action taken within that trajectory, ensuring that all steps contributing to a successful outcome receive a consistent positive signal, thereby mitigating the credit assignment problem.

Given the inherent sparsity of successful trajectories in GUI tasks, we incorporate a replay buffer to stabilize training. This buffer stores historically successful trajectories, indexed by their task_id. During the sampling process, if a generated group of trajectories for a task results entirely in failures, one failing trajectory is randomly replaced with a successful one from the buffer corresponding to the same task. This injection of positive examples ensures the effectiveness of the training signal in every batch.

Our final policy optimization objective for a batch of  $G$  trajectories is defined by the following loss function:

$$
\mathcal {L} _ {\mathrm {T R P O}} = - \frac {1}{N} \sum_ {i = 1} ^ {G} \sum_ {s = 1} ^ {S _ {i}} \sum_ {t = 1} ^ {| \mathbf {o} _ {i, s} |} \left\{\min  \left[ r _ {t} (\theta) \hat {A} _ {\tau_ {i}}, \operatorname {c l i p} (r _ {t} (\theta), 1 - \epsilon , 1 + \epsilon) \hat {A} _ {\tau_ {i}} \right] \right\} \tag {3}
$$

where  $N$  is the total number of tokens in the batch,  $\hat{A}_{\tau_i}$  is the trajectory-level advantage for trajectory  $i$ , and  $r_t(\theta) = \frac{\pi_\theta(o_{s,t}|\dots)}{\pi_{\theta_{\mathrm{old}}}(o_{s,t}|\dots)}$  is the probability ratio of a token under the current and old policies. This clipped objective function stabilizes training while effectively leveraging the holistic trajectory-level reward signal for long-horizon GUI automation tasks.

In practice, the high resolution of GUI screenshots means that a complete interaction trajectory can quickly exceed the model's context length (e.g., 32k for Qwen2.5-VL). To manage this, we segment each full multi-turn trajectory into several single-step data instances for the policy update. The loss computed for each step-wise instance is then scaled by the total number of steps in its original, complete trajectory. This approach addresses the issue of unbalanced optimization for trajectories of different lengths.

# 4 MOBILE-AGENT-V3

The agent-based framework method modularizes complex GUI tasks into multiple relatively simple tasks, and can achieve higher performance with the cooperation of agents with different roles. (Zhang et al., 2025a; Li et al., 2024; Wang et al., 2024b; 2025b; Agashe et al., 2025; 2024; Zhang et al., 2025b; Li et al., 2025; Liu et al., 2024; Nong et al., 2024; Wu et al., 2024a;b; Sun et al., 2024; Zhang et al., 2024b; Zheng et al., 2024; Patel et al., 2024; Niu et al., 2024; Tan et al., 2024). As noted earlier, GUI-Owl possesses multi-agent collaboration capabilities. Building upon this foundation, we further propose Mobile-Agent-v3, a multi-agent framework endowed with capabilities for knowledge evolution, task planning, sub-task execution, and reflective reasoning. In this section, we discuss the architecture of Mobile-Agent-v3.

As presented in Figure 7, the Mobile-Agent-v3 framework coordinates four specialized agents to achieve robust, adaptive, and long-horizon GUI task automation:

- Manager Agent  $(\mathcal{M})$ : Serves as the strategic planner. At initialization, it decomposes a high-level instruction  $I$  into an ordered subgoal list  $SS_{0}$  using external knowledge  $K_{\mathrm{RAG}}$ . During execution, it updates the plan based on results and feedback, re-prioritizing, modifying, or inserting corrective subgoals.  
- Worker Agent  $(\mathcal{W})$ : Acts as the tactical executor. It selects and performs the most relevant actionable subgoal from  $SS_{t}$  given the current GUI state  $S_{t}$ , prior feedback  $F_{t - 1}$ , and accumulated notes  $\mathcal{N}_t$ , producing an action tuple  $A_{t}$  that records reasoning, action, and intent.  
- Reflector Agent  $(\mathcal{R})$ : Functions as the self-correction mechanism. It compares the intended outcome from the Worker with the actual state transition  $(S_{t} \rightarrow S_{t + 1})$ , classifying the result as SUCCESS or FAILURE and generating detailed causal feedback  $\phi_t$  for the Manager.  
- Notetaker Agent  $(\mathcal{C})$ : Maintains persistent contextual memory. Triggered only on SUCCESS, it extracts and stores critical screen elements (e.g., codes, credentials) as notes  $N_{t}$ . The cumulative memory  $\mathcal{N}_{t+1}$  supports both planning and execution in future steps.

The Manager decomposes and dynamically updates the plan, the Worker executes selected subgoals, the Reflector evaluates outcomes and provides diagnostic feedback, and the Notetaker preserves valuable transient information. This loop continues until all subgoals are completed or the instruction  $I$  is fulfilled. More details can be found in Section 7.

![](images/f50cbb0be393e7b3d64dd9fa72b6f8f07df0b58be1e1440f61c45b81d35e7160.jpg)  
Figure 7: Mobile-Agent-v3 architecture. The system consists of six modules: (1) a RAG module for retrieving external world knowledge, (2) a Manager Agent for subgoal planning and guidance, (3) a Worker Agent for GUI operation, (4) a Reflector Agent for evaluation and feedback (5) a Notetaker Agent for recording important note, and (6) A GUI device interface supporting phone and PC environments.

# 5 EXPERIMENTS

# 5.1 MODEL EVALUATION

In this section, we evaluate GUI-Owl on a wide range of benchmarks to thoroughly assess its performance as a fundamental agent in GUI-based scenarios. We train GUI-Owl-7B and GUI-Owl-32B, which are initialized from the Qwen2.5-VL models of the corresponding sizes. We conduct extensive experiments to evaluate GUI-Owl in four key dimensions: grounding capability, comprehensive GUI understanding, end-to-end agent capability, and multi-agent capability.

# 5.1.1 GROUNDING CAPABILITY

The grounding capability evaluates a model's ability to locate the corresponding UI element given a natural-language query. We use ScreenSpot v2, ScreenSpot Pro, OSWorld-G, and MMBench-GUI L2 as benchmarks. ScreenSpot v2 covers mobile, desktop, and web scenarios, while ScreenSpot-Pro primarily evaluates a model's localization ability at ultra-high resolutions. OSWorld-G contains finely annotated queries. MMBench-GUI L2 has the broadest coverage and more faithfully reflects a model's grounding performance in real-world settings. The performance comparisons are shown in Table 1, Table 2, Table 3 and Table 4.

GUI-Owl-7B achieves state-of-the-art performance among all 7B models. On screenshot-pro, which focuses on high-resolution images, we achieve a score of 54.9, significantly exceeding the performance of UI-TARS-72B and Qwen2.5-VL 72B. GUI-Owl-7B also achieves competitive performance on OSWorld-G compared to UI-TARS-72B. GUI-Owl-32B surpasses all models of the same size. MMBench-GUI-L2 evaluates a very broad and challenging set of queries, where our model scores 80.49, substantially outperforming all existing models. GUI-Owl-32B further achieves a performance level of 82.97 and demonstrates leading grounding capabilities across various domains.

# 5.1.2 COMPREHENSIVE GUI UNDERSTANDING

Comprehensive GUI Understanding examines whether a GUI model can accurately interpret interface states and produce appropriate responses. We adopt two benchmarks for this evaluation. MMBench-GUI-L1 assesses the model's UI understanding and single-step decision-making capability through a question-answering format. Android Control evaluates the model's ability to perform single-step decisions within pre-annotated trajectory contexts.

<table><tr><td rowspan="2">Agent Model</td><td colspan="2">Mobile</td><td colspan="2">Desktop</td><td colspan="2">Web</td><td rowspan="2">Overall</td></tr><tr><td>Text</td><td>Icon</td><td>Text</td><td>Icon</td><td>Text</td><td>Icon</td></tr><tr><td colspan="8">Proprietary Models</td></tr><tr><td>Operator (OpenAI, 2025a)</td><td>47.3</td><td>41.5</td><td>90.2</td><td>80.3</td><td>92.8</td><td>84.3</td><td>70.5</td></tr><tr><td>Claude 3.7 Sonnet (Anthropic, 2025a)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>87.6</td></tr><tr><td>UI-TARS-1.5 (Qin et al., 2025)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>94.2</td></tr><tr><td>Seed-1.5-VL (Team, 2025)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>95.2</td></tr><tr><td colspan="8">Open-Source Models</td></tr><tr><td>SeeClick (Cheng et al., 2024)</td><td>78.4</td><td>50.7</td><td>70.1</td><td>29.3</td><td>55.2</td><td>32.5</td><td>55.1</td></tr><tr><td>OmniParser-v2 (Yu et al., 2025)</td><td>95.5</td><td>74.6</td><td>92.3</td><td>60.9</td><td>88.0</td><td>59.6</td><td>80.7</td></tr><tr><td>Qwen2.5-VL-3B (Bai et al., 2025)</td><td>93.4</td><td>73.5</td><td>88.1</td><td>58.6</td><td>88.0</td><td>71.4</td><td>80.9</td></tr><tr><td>UI-TARS-2B (Qin et al., 2025)</td><td>95.2</td><td>79.1</td><td>90.7</td><td>68.6</td><td>87.2</td><td>78.3</td><td>84.7</td></tr><tr><td>OS-Atlas-Base-4B (Wu et al., 2024b)</td><td>95.2</td><td>75.8</td><td>90.7</td><td>63.6</td><td>90.6</td><td>77.3</td><td>85.1</td></tr><tr><td>OS-Atlas-Base-7B (Wu et al., 2024b)</td><td>96.2</td><td>83.4</td><td>89.7</td><td>69.3</td><td>94.0</td><td>79.8</td><td>87.1</td></tr><tr><td>JEDI-3B (Xie et al., 2025)</td><td>96.6</td><td>81.5</td><td>96.9</td><td>78.6</td><td>88.5</td><td>83.7</td><td>88.6</td></tr><tr><td>Qwen2.5-VL-7B (Bai et al., 2025)</td><td>97.6</td><td>87.2</td><td>90.2</td><td>74.2</td><td>93.2</td><td>81.3</td><td>88.8</td></tr><tr><td>UI-TARS-72B (Qin et al., 2025)</td><td>94.8</td><td>86.3</td><td>91.2</td><td>87.9</td><td>91.5</td><td>87.7</td><td>90.3</td></tr><tr><td>UI-TARS-7B (Qin et al., 2025)</td><td>96.9</td><td>89.1</td><td>95.4</td><td>85.0</td><td>93.6</td><td>85.2</td><td>91.6</td></tr><tr><td>JEDI-7B (Xie et al., 2025)</td><td>96.9</td><td>87.2</td><td>95.9</td><td>87.9</td><td>94.4</td><td>84.2</td><td>91.7</td></tr><tr><td>GUI-Owl-7B</td><td>99.0</td><td>92.4</td><td>96.9</td><td>85.0</td><td>93.6</td><td>85.2</td><td>92.8</td></tr><tr><td>GUI-Owl-32B</td><td>98.6</td><td>90.0</td><td>97.9</td><td>87.8</td><td>94.4</td><td>86.7</td><td>93.2</td></tr></table>

Table 1: Comparison with state-of-the-art methods on the ScreenSpot-V2 dataset. Underlined denotes the second-best open-source performance.  

<table><tr><td rowspan="2">Agent Model</td><td colspan="2">Development</td><td colspan="2">Creative</td><td colspan="2">CAD</td><td colspan="2">Scientific</td><td colspan="2">Office</td><td colspan="2">OS</td><td>Avg</td></tr><tr><td>Text</td><td>Icon</td><td>Text</td><td>Icon</td><td>Text</td><td>Icon</td><td>Text</td><td>Icon</td><td>Text</td><td>Icon</td><td>Text</td><td>Icon</td><td></td></tr><tr><td colspan="14">Proprietary Models</td></tr><tr><td>GPT-4o (Hurst et al., 2024)</td><td>1.3</td><td>0.0</td><td>1.0</td><td>0.0</td><td>2.0</td><td>0.0</td><td>2.1</td><td>0.0</td><td>1.1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.8</td></tr><tr><td>Claude 3.7 Sonnet (Anthropic, 2025a)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>27.7</td></tr><tr><td>Operator (OpenAI, 2025a)</td><td>50.0</td><td>19.3</td><td>51.5</td><td>23.1</td><td>16.8</td><td>14.1</td><td>58.3</td><td>24.5</td><td>60.5</td><td>28.3</td><td>34.6</td><td>30.3</td><td>36.6</td></tr><tr><td>Seed-1.5-VL (Team, 2025)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>60.9</td></tr><tr><td>UI-TARS-1.5 (Qin et al., 2025)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>61.6</td></tr><tr><td colspan="14">Open-Source Models</td></tr><tr><td>UI-TARS-2B (Qin et al., 2025)</td><td>47.4</td><td>4.1</td><td>42.9</td><td>6.3</td><td>17.8</td><td>4.7</td><td>56.9</td><td>17.3</td><td>50.3</td><td>17.0</td><td>21.5</td><td>5.6</td><td>27.7</td></tr><tr><td>Qwen2.5-VL-3B (Bai et al., 2025)</td><td>38.3</td><td>3.4</td><td>40.9</td><td>4.9</td><td>22.3</td><td>6.3</td><td>44.4</td><td>10.0</td><td>48.0</td><td>17.0</td><td>33.6</td><td>4.5</td><td>25.9</td></tr><tr><td>Qwen2.5-VL-7B (Bai et al., 2025)</td><td>51.9</td><td>4.8</td><td>36.9</td><td>8.4</td><td>17.8</td><td>1.6</td><td>48.6</td><td>8.2</td><td>53.7</td><td>18.9</td><td>34.6</td><td>7.9</td><td>27.6</td></tr><tr><td>UI-R1-E-3B (Lu et al., 2025)</td><td>46.1</td><td>6.9</td><td>41.9</td><td>4.2</td><td>37.1</td><td>12.5</td><td>56.9</td><td>21.8</td><td>65.0</td><td>26.4</td><td>32.7</td><td>10.1</td><td>33.5</td></tr><tr><td>UI-TARS-7B (Qin et al., 2025)</td><td>58.4</td><td>12.4</td><td>50.0</td><td>9.1</td><td>20.8</td><td>9.4</td><td>63.9</td><td>31.8</td><td>63.3</td><td>20.8</td><td>30.8</td><td>16.9</td><td>35.7</td></tr><tr><td>InfiGUI-R1-3B (Liu et al., 2025)</td><td>51.3</td><td>12.4</td><td>44.9</td><td>7.0</td><td>33.0</td><td>14.1</td><td>58.3</td><td>20.0</td><td>65.5</td><td>28.3</td><td>43.9</td><td>12.4</td><td>35.7</td></tr><tr><td>JEDI-3B (Xie et al., 2025)</td><td>61.0</td><td>13.8</td><td>53.5</td><td>8.4</td><td>27.4</td><td>9.4</td><td>54.2</td><td>18.2</td><td>64.4</td><td>32.1</td><td>38.3</td><td>9.0</td><td>36.1</td></tr><tr><td>GUI-G1-3B (Zhou et al., 2025)</td><td>50.7</td><td>10.3</td><td>36.6</td><td>11.9</td><td>39.6</td><td>9.4</td><td>61.8</td><td>30.0</td><td>67.2</td><td>32.1</td><td>23.5</td><td>10.6</td><td>37.1</td></tr><tr><td>UI-TARS-72B (Qin et al., 2025)</td><td>63.0</td><td>17.3</td><td>57.1</td><td>15.4</td><td>18.8</td><td>12.5</td><td>64.6</td><td>20.9</td><td>63.3</td><td>26.4</td><td>42.1</td><td>15.7</td><td>38.1</td></tr><tr><td>JEDI-7B (Xie et al., 2025)</td><td>42.9</td><td>11.0</td><td>50.0</td><td>11.9</td><td>38.0</td><td>14.1</td><td>72.9</td><td>25.5</td><td>75.1</td><td>47.2</td><td>33.6</td><td>16.9</td><td>39.5</td></tr><tr><td>Qwen2.5-VL-32B (Bai et al., 2025)</td><td>74.0</td><td>21.4</td><td>61.1</td><td>13.3</td><td>38.1</td><td>15.6</td><td>78.5</td><td>29.1</td><td>76.3</td><td>37.7</td><td>55.1</td><td>27.0</td><td>47.6</td></tr><tr><td>SE-GUI-7B (Yuan et al., 2025)</td><td>68.2</td><td>19.3</td><td>57.6</td><td>9.1</td><td>51.3</td><td>42.2</td><td>75.0</td><td>28.2</td><td>78.5</td><td>43.4</td><td>49.5</td><td>25.8</td><td>47.3</td></tr><tr><td>GUI-G2-7B (Tang et al., 2025)</td><td>68.8</td><td>17.2</td><td>57.1</td><td>15.4</td><td>55.8</td><td>12.5</td><td>77.1</td><td>24.5</td><td>74.0</td><td>32.7</td><td>57.9</td><td>21.3</td><td>47.5</td></tr><tr><td>Qwen2.5-VL-72B (Bai et al., 2025)</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>53.3</td></tr><tr><td>GUI-Owl-7B</td><td>76.6</td><td>31.0</td><td>59.6</td><td>27.3</td><td>64.5</td><td>21.9</td><td>79.1</td><td>37.3</td><td>77.4</td><td>39.6</td><td>59.8</td><td>33.7</td><td>54.9</td></tr><tr><td>GUI-Owl-32B</td><td>84.4</td><td>39.3</td><td>65.2</td><td>18.2</td><td>62.4</td><td>28.1</td><td>82.6</td><td>39.1</td><td>81.4</td><td>39.6</td><td>70.1</td><td>36.0</td><td>58.0</td></tr></table>

Table 2: Comparison with state-of-the-art methods on the ScreenSpot-Pro dataset. Underlined denotes the second-best open-source performance.

On the MMBench-GUI-L1 benchmark, GUI-Owl scores 84.5, 86.9, and 90.9 on the easy, medium, and hard levels, respectively, substantially outperforming all existing models. On Android Control, it achieves a score of 72.8, establishing the highest performance among all 7B models. We find that GUI-Owl-32B achieves a score of 76.6, surpassing the current state-of-the-art UI-TARS-72B. GUI-Owl-32B significantly outperforms GUI-Owl-7B across different difficulty levels and domains, reflecting its more comprehensive and sufficient reserve of GUI knowledge.

<table><tr><td>Agent Model</td><td>Text Matching</td><td>Element Recog.</td><td>Layout Underst.</td><td>Fine-grained Manip.</td><td>Overall</td></tr><tr><td colspan="6">Proprietary Models</td></tr><tr><td>Gemini-2.5-Pro (Deepmind, 2025b)</td><td>59.8</td><td>45.5</td><td>49.0</td><td>33.6</td><td>45.2</td></tr><tr><td>Operator (OpenAI, 2025a)</td><td>51.3</td><td>42.4</td><td>46.6</td><td>31.5</td><td>40.6</td></tr><tr><td>Seed1.5-VL (Team, 2025)</td><td>73.9</td><td>66.7</td><td>69.6</td><td>47.0</td><td>62.9</td></tr><tr><td colspan="6">Open-Source Models</td></tr><tr><td>OS-Atlas-7B (Wu et al., 2024b)</td><td>44.1</td><td>29.4</td><td>35.2</td><td>16.8</td><td>27.7</td></tr><tr><td>UGround-V1-7B (Gou et al., 2024)</td><td>51.3</td><td>40.3</td><td>43.5</td><td>24.8</td><td>36.4</td></tr><tr><td>Aguvis-7B (Xu et al., 2024)</td><td>55.9</td><td>41.2</td><td>43.9</td><td>28.2</td><td>38.7</td></tr><tr><td>UI-TARS-7B (Qin et al., 2025)</td><td>60.2</td><td>51.8</td><td>54.9</td><td>35.6</td><td>47.5</td></tr><tr><td>UI-TARS-72B (Qin et al., 2025)</td><td>69.4</td><td>60.6</td><td>62.9</td><td>45.6</td><td>57.1</td></tr><tr><td>Qwen2.5-VL-3B (Bai et al., 2025)</td><td>41.4</td><td>28.8</td><td>34.8</td><td>13.4</td><td>27.3</td></tr><tr><td>Qwen2.5-VL-7B (Bai et al., 2025)</td><td>45.6</td><td>32.7</td><td>41.9</td><td>18.1</td><td>31.4</td></tr><tr><td>Qwen2.5-VL-32B (Bai et al., 2025)</td><td>63.2</td><td>47.3</td><td>49.0</td><td>36.9</td><td>46.5</td></tr><tr><td>JEDI-3B (Xie et al., 2025)</td><td>67.4</td><td>53.0</td><td>53.8</td><td>44.3</td><td>50.9</td></tr><tr><td>JEDI-7B (Xie et al., 2025)</td><td>65.9</td><td>55.5</td><td>57.7</td><td>46.9</td><td>54.1</td></tr><tr><td>GUI-Owl-7B</td><td>64.8</td><td>63.6</td><td>61.3</td><td>41.0</td><td>55.9</td></tr><tr><td>GUI-Owl-32B</td><td>67.0</td><td>64.5</td><td>67.2</td><td>45.6</td><td>58.0</td></tr></table>

Table 3: Comparison with state-of-the-art methods on the OSWorld-G dataset. Underlined denotes the second-best open-source performance.  

<table><tr><td rowspan="2">Model</td><td colspan="2">Windows</td><td colspan="2">MacOS</td><td colspan="2">Linux</td><td colspan="2">iOS</td><td colspan="2">Android</td><td colspan="2">Web</td><td rowspan="2">Overall</td></tr><tr><td>Basic</td><td>Adv.</td><td>Basic</td><td>Adv.</td><td>Basic</td><td>Adv.</td><td>Basic</td><td>Adv.</td><td>Basic</td><td>Adv.</td><td>Basic</td><td>Adv.</td></tr><tr><td>GPT-4o (Hurst et al., 2024)</td><td>1.48</td><td>1.10</td><td>8.69</td><td>4.34</td><td>1.05</td><td>1.02</td><td>5.10</td><td>3.33</td><td>2.53</td><td>1.41</td><td>3.23</td><td>2.92</td><td>2.87</td></tr><tr><td>Claude-3.7 (Anthropic, 2025a)</td><td>1.48</td><td>0.74</td><td>12.46</td><td>7.51</td><td>1.05</td><td>0.00</td><td>13.69</td><td>10.61</td><td>1.40</td><td>1.40</td><td>3.23</td><td>2.27</td><td>4.66</td></tr><tr><td>Qwen-Max-VL (Bai et al., 2025)</td><td>43.91</td><td>36.76</td><td>58.84</td><td>56.07</td><td>53.93</td><td>30.10</td><td>77.39</td><td>59.09</td><td>79.49</td><td>70.14</td><td>74.84</td><td>58.77</td><td>58.03</td></tr><tr><td>Aguvis-7B-720P (Xu et al., 2024)</td><td>37.27</td><td>21.69</td><td>48.12</td><td>33.27</td><td>33.51</td><td>25.00</td><td>67.52</td><td>65.15</td><td>60.96</td><td>50.99</td><td>61.61</td><td>45.45</td><td>45.66</td></tr><tr><td>ShowUI-2B (Lin et al., 2025)</td><td>9.23</td><td>4.41</td><td>24.06</td><td>10.40</td><td>25.13</td><td>11.73</td><td>28.98</td><td>19.70</td><td>17.42</td><td>8.73</td><td>22.90</td><td>12.66</td><td>15.96</td></tr><tr><td>OS-Atlas-Base-7B (Wu et al., 2024b)</td><td>36.90</td><td>18.75</td><td>44.35</td><td>21.68</td><td>31.41</td><td>13.27</td><td>74.84</td><td>48.79</td><td>69.60</td><td>46.76</td><td>61.29</td><td>35.39</td><td>41.42</td></tr><tr><td>UGround-V1-7B (Gou et al., 2024)</td><td>66.79</td><td>38.97</td><td>71.30</td><td>48.55</td><td>56.54</td><td>31.12</td><td>92.68</td><td>70.91</td><td>93.54</td><td>70.99</td><td>88.71</td><td>64.61</td><td>65.68</td></tr><tr><td>InternVL3-72B (Zhu et al., 2025)</td><td>70.11</td><td>42.64</td><td>75.65</td><td>52.31</td><td>59.16</td><td>41.33</td><td>93.63</td><td>80.61</td><td>92.70</td><td>78.59</td><td>90.65</td><td>65.91</td><td>72.20</td></tr><tr><td>Qwen2.5-VL-72B (Bai et al., 2025)</td><td>55.72</td><td>33.82</td><td>49.86</td><td>30.06</td><td>40.31</td><td>20.92</td><td>56.05</td><td>28.18</td><td>55.62</td><td>25.35</td><td>68.39</td><td>45.78</td><td>41.83</td></tr><tr><td>Qwen2.5-VL-7B (Bai et al., 2025)</td><td>31.37</td><td>16.54</td><td>31.30</td><td>21.97</td><td>21.47</td><td>12.24</td><td>66.56</td><td>55.15</td><td>35.11</td><td>35.21</td><td>40.32</td><td>32.47</td><td>33.85</td></tr><tr><td>UI-TARS-1.5-7B (Qin et al., 2025)</td><td>68.27</td><td>38.97</td><td>68.99</td><td>44.51</td><td>64.40</td><td>37.76</td><td>88.54</td><td>69.39</td><td>90.45</td><td>69.29</td><td>80.97</td><td>56.49</td><td>64.32</td></tr><tr><td>UI-TARS-72B-DPO (Qin et al., 2025)</td><td>78.60</td><td>51.84</td><td>80.29</td><td>62.72</td><td>68.59</td><td>51.53</td><td>90.76</td><td>81.21</td><td>92.98</td><td>80.00</td><td>88.06</td><td>68.51</td><td>74.25</td></tr><tr><td>GUI-Owl-7B</td><td>86.35</td><td>61.76</td><td>81.74</td><td>64.45</td><td>74.35</td><td>61.73</td><td>94.90</td><td>83.03</td><td>95.78</td><td>83.66</td><td>93.22</td><td>72.72</td><td>80.49</td></tr><tr><td>GUI-Owl-32B</td><td>85.61</td><td>65.07</td><td>84.93</td><td>67.05</td><td>76.96</td><td>63.27</td><td>95.22</td><td>85.45</td><td>96.07</td><td>87.04</td><td>95.48</td><td>80.84</td><td>82.97</td></tr></table>

Table 4: Comparison with state-of-the-art methods on the MMBench-GUI-L2 dataset. Underlined denotes the second-best open-source performance.

# 5.1.3 END2END AND MULTI-AGENT CAPABILITY ON ONLINE ENVIRONMENT

While the aforementioned evaluations measure a model's performance in single-step decision-making, they suffer from two main limitations: (1) Errors in individual steps do not accumulate, making it impossible to assess the ability to accomplish complete tasks; (2) Although there may be multiple valid ways to complete a task, the ground-truth step sequences may reflect specific preferences, which can result in an unfair evaluation across models. To more comprehensively evaluate both the end-to-end agent capability and the multi-agent capability, we adopt realistic interactive environments — AndroidWorld and OSWorld-Verified.

GUI-Owl-7B outperformsUITARS 1.5 on AndroidWorld, and Mobile-Agent-v3 achieves an even greater lead, significantly surpassing all existing models. On OSWorld-Verified, GUI-Owl also outperforms the open-source OpenCUA-7B. We further adopt GUI-Owl-32B into Mobile-Agent-v3, it achieves 37.7 on OSWorld-Verified and 73.3 on AndroidWorld. This suggests that GUI-Owl is not only capable of independently solving tasks, but is also well-suited for integration into a multi-agent framework.

# 5.2 TRAJECTORY-LEVEL ONLINE REINFORCEMENT LEARNING

To validate the efficacy of our trajectory-level online reinforcement learning strategy, we conducted a series of experiments on the OSWorld-Verified (Xie et al., 2024) benchmark, with all tasks limited to a maximum of 15 steps. The results, illustrated in Figure 8, demonstrate the clear advantages of our proposed approach. Starting from an initial checkpoint with a 27.1 success rate, our method shows consistent, stable improvement throughout training, ultimately achieving a peak success rate of over 34.9. This steady learning curve underscores the effectiveness of our trajectory-aware relative policy optimization. By calculating a single, normalized advantage estimate  $\hat{A}_{\tau}$  for an entire trajectory, our method successfully mitigates the severe credit assignment problem inherent in long-horizon GUI tasks and provides a coherent learning signal.

<table><tr><td>Model</td><td>Windows</td><td>MacOS</td><td>Linux</td><td>iOS</td><td>Android</td><td>Web</td><td>Overall</td></tr><tr><td colspan="8">Easy Level</td></tr><tr><td>GPT-4o (Hurst et al., 2024)</td><td>62.47</td><td>67.89</td><td>62.38</td><td>58.52</td><td>56.41</td><td>58.51</td><td>60.16</td></tr><tr><td>Claude-3.5 (Anthropic, 2024)</td><td>41.34</td><td>50.04</td><td>41.61</td><td>42.03</td><td>38.96</td><td>41.79</td><td>41.54</td></tr><tr><td>Qwen2.5-VL-72B (Bai et al., 2025)</td><td>65.86</td><td>75.23</td><td>73.02</td><td>67.24</td><td>58.09</td><td>72.08</td><td>66.98</td></tr><tr><td>UI-TARS-72B-DPO (Qin et al., 2025)</td><td>41.59</td><td>28.52</td><td>35.16</td><td>31.08</td><td>52.25</td><td>35.33</td><td>40.18</td></tr><tr><td>InternVL3-72B (Zhu et al., 2025)</td><td>74.67</td><td>78.72</td><td>79.16</td><td>83.57</td><td>80.10</td><td>81.18</td><td>79.15</td></tr><tr><td>GUI-Owl-7B</td><td>82.96</td><td>84.52</td><td>85.57</td><td>82.61</td><td>83.28</td><td>88.13</td><td>84.50</td></tr><tr><td>GUI-Owl-32B</td><td>93.70</td><td>89.29</td><td>93.30</td><td>95.65</td><td>90.49</td><td>94.06</td><td>92.75</td></tr><tr><td colspan="8">Medium Level</td></tr><tr><td>GPT-4o (Hurst et al., 2024)</td><td>56.33</td><td>63.13</td><td>59.70</td><td>54.06</td><td>57.69</td><td>54.98</td><td>57.24</td></tr><tr><td>Claude-3.5 (Anthropic, 2025a)</td><td>39.28</td><td>47.63</td><td>45.97</td><td>44.57</td><td>42.03</td><td>34.33</td><td>41.26</td></tr><tr><td>Qwen2.5-VL-72B (Bai et al., 2025)</td><td>66.29</td><td>72.73</td><td>72.63</td><td>59.27</td><td>66.24</td><td>68.24</td><td>67.45</td></tr><tr><td>UI-TARS-72B-DPO (Qin et al., 2025)</td><td>38.83</td><td>41.60</td><td>37.14</td><td>41.72</td><td>54.74</td><td>31.55</td><td>41.77</td></tr><tr><td>InternVL3-72B (Zhu et al., 2025)</td><td>71.46</td><td>78.58</td><td>79.88</td><td>78.43</td><td>81.36</td><td>78.67</td><td>77.89</td></tr><tr><td>GUI-Owl-7B</td><td>88.89</td><td>88.10</td><td>91.24</td><td>84.35</td><td>85.25</td><td>83.56</td><td>86.86</td></tr><tr><td>GUI-Owl-32B</td><td>94.07</td><td>84.52</td><td>95.88</td><td>87.83</td><td>92.79</td><td>88.58</td><td>91.74</td></tr><tr><td colspan="8">Hard Level</td></tr><tr><td>GPT-4o (Hurst et al., 2024)</td><td>60.69</td><td>60.38</td><td>52.42</td><td>45.27</td><td>50.93</td><td>50.83</td><td>53.49</td></tr><tr><td>Claude-3.5 (Anthropic, 2025a)</td><td>37.40</td><td>42.70</td><td>34.07</td><td>40.86</td><td>36.96</td><td>38.11</td><td>37.55</td></tr><tr><td>Qwen2.5-VL-72B (Bai et al., 2025)</td><td>70.68</td><td>68.91</td><td>70.98</td><td>57.59</td><td>53.94</td><td>68.10</td><td>64.56</td></tr><tr><td>UI-TARS-72B-DPO (Qin et al., 2025)</td><td>31.48</td><td>35.87</td><td>24.19</td><td>36.33</td><td>58.13</td><td>19.94</td><td>35.78</td></tr><tr><td>InternVL3-72B (Zhu et al., 2025)</td><td>75.08</td><td>77.44</td><td>76.19</td><td>70.37</td><td>75.73</td><td>78.11</td><td>75.70</td></tr><tr><td>GUI-Owl-7B</td><td>87.78</td><td>96.43</td><td>94.33</td><td>87.83</td><td>88.85</td><td>94.06</td><td>90.90</td></tr><tr><td>GUI-Owl-32B</td><td>93.33</td><td>95.24</td><td>95.88</td><td>92.17</td><td>95.41</td><td>92.69</td><td>94.19</td></tr></table>

Table 5: Comparison with state-of-the-art methods on the MMBench-GUI-L1 dataset. Underlined denotes the second-best open-source performance.  

<table><tr><td>Model</td><td>Score</td></tr><tr><td>Claude-3.5 (Anthropic, 2024)</td><td>12.5</td></tr><tr><td>GPT-4o (Hurst et al., 2024)</td><td>20.8</td></tr><tr><td>Gemini 2.0 (Deepmind, 2025a)</td><td>28.5</td></tr><tr><td>Qwen2-VL-72B (Wang et al., 2024c)</td><td>59.1</td></tr><tr><td>Aguvis-72B (Xu et al., 2024)</td><td>66.4</td></tr><tr><td>Qwen2.5-VL-72B (Bai et al., 2025)</td><td>67.4</td></tr><tr><td>UI-TARS-7B (Qin et al., 2025)</td><td>72.5</td></tr><tr><td>UI-TARS-72B (Qin et al., 2025)</td><td>74.7</td></tr><tr><td>GUI-Owl-7B</td><td>72.8</td></tr><tr><td>GUI-Owl-32B</td><td>76.6</td></tr></table>

Table 6: Model performance on the Android Control benchmark. Extract match scores with high-level instruction are reported. Underlined denotes the second-best open-source performance.

<table><tr><td rowspan="2">Agent Model</td><td colspan="2">Online</td></tr><tr><td>OSWorld-Verified</td><td>AndroidWorld</td></tr><tr><td colspan="3">Proprietary Models</td></tr><tr><td>SeedVL-1.5 (Team, 2025)</td><td>34.1</td><td>62.1</td></tr><tr><td>Claude-4-sonnet (Anthropic, 2025b)</td><td>43.9</td><td>-</td></tr><tr><td>OpenAI CUA o3 (OpenAI, 2025b)</td><td>23.0</td><td>-</td></tr><tr><td>UI-TARS-1.5 (Qin et al., 2025)</td><td>-</td><td>64.2</td></tr><tr><td colspan="3">Open-Source Models</td></tr><tr><td>UI-TARS-72B-DPO (Qin et al., 2025)</td><td>24.0</td><td>46.6</td></tr><tr><td>OpenCUA-7B (Wang et al., 2025a)</td><td>28.2</td><td>-</td></tr><tr><td>OpenCUA-32B (Wang et al., 2025a)</td><td>34.8</td><td>-</td></tr><tr><td>UI-TARS1.5-7B (Qin et al., 2025)</td><td>27.4</td><td>-</td></tr><tr><td>GUI-Owl-7B</td><td>34.9*</td><td>66.4</td></tr><tr><td>Mobile-Agent-v3</td><td>37.7</td><td>73.3</td></tr></table>

*A variant of GUI-Owl specifically RL-tuned for a desktop environment (Section 5.2). The general version of GUI-Owl achieves a score of 29.4.

Table 7: Online evaluation results on OSWorld-Verified and AndroidWorld benchmarks. Underlined denotes the second-best open-source performance.

The most critical insight comes from the ablation study, Online Filtering (DAPO). This variant, which disables our successful-trajectory replay buffer and the mechanism for carrying over unused rollouts, confirms the value of our specific design choices. While this model still shows a positive learning trend, its performance is notably more volatile and ultimately inferior, peaking at around  $31.5\%$  before declining. This instability highlights the challenge of sparse positive feedback; without the replay buffer injecting successful examples, the agent struggles to learn from the vast space of failing trajectories. The final performance gap between our full model and this ablation underscores the importance of data efficiency. By retaining and reusing all generated rollouts, our full method maximizes the utility of costly interactions, providing a richer training signal that leads to more stable and superior final performance.

Our comparison with the Offline Filtering (GRPO) baseline further justifies our online data selection methodology. Offline filtering is a very common technique for preparing RL data by removing tasks that are statically identified as all-successful or all-failing across multiple inference runs. However, the results show this approach is not suitable for GUI automation tasks that require long-range, multi-step planning. After an initial small gain, its performance stagnates around a 29.1 success rate before degrading significantly. The failure arises because the

![](images/f61f44e82c0f687313347ebb46175085a7afd653f51d620e12e16f880f02a76a.jpg)  
Figure 8: Training dynamics of GUI-Owl-7B on OSWorld-Verified. We limit the maximum interaction steps to 15 by default. Offline Filtering removes tasks with all-success or all-failure outcomes before applying vanilla GRPO, serving as common preprocessing. Online Filtering moves all tasks to online training and applies DAPO for selective filtering. Experience Managing activates both the replay buffer and the use of leftover rollouts after batch filling, as described in Section 3.1.2.

final reward depends on a long sequence of actions, making outcomes highly sensitive to minor policy changes during training. Such sensitivity causes abrupt, non-linear shifts in success rates, in contrast to the smoother improvement observed in single-step reasoning tasks. Relying solely on offline filtering further aggravates this issue, leading to severe overfitting. As our results confirm, a more effective solution is a dynamic online filtering strategy, which continuously adapts the training distribution to the agent's evolving policy.

In summary, the results validate that while trajectory-level optimization provides a solid foundation, it is our novel experience management, which combines a success-replay mechanism with maximum data utilization, that is crucial for achieving stable and efficient performance. This methodology allows GUI-Owl-7B to achieve state-of-the-art results among open-source models of the same model size. Notably, under identical experimental settings, our model also surpasses the performance of powerful proprietary models like Claude-4-Sonnet. This demonstrates that our specialized online RL fine-tuning strategy can effectively elevate strong base models, enabling them to excel in complex, long-horizon interactive tasks and rival the capabilities of significantly larger systems.

# 5.2.1 SCALING OF INTERACTION STEPS AND HISTORICAL IMAGES

![](images/f5ddd3b384fc4ea9374973dedc95d991417f0affc28277fdaa28496e2d4f532d.jpg)  
Figure 9: Performance of GUI-Owl-7B on OSWorld-Verified with varying numbers of historical images and interaction-step budgets.

We further analyze, on OSWorld, how GUI-Owl's performance varies with the number of historical screenshots and the interaction-step budget. As shown in Figure 9, performance increases steadily as more historical images

are provided. This is because the model's understanding of UI changes relies on contrasts between consecutive frames, and additional images also help the model promptly reflect on and correct persistent erroneous behaviors. We also observe that increasing the interaction-step budget improves performance, indicating that our model has a significant advantage on long-horizon tasks.

# 5.3 EFFECT OF REASONING DATA SYNTHESIS

![](images/e7f51d8e7b8b4157b56b0d9564ca7b381db4ee30c87aa90736ec6e7853e961ab.jpg)  
Figure 10: Effect of reasoning data synthesis on Android World.

![](images/a6df1111b93367e0326afbcadf1ce0e9bce7e3450b0430bea69f3d6a009b8cb3.jpg)

Our offline reasoning data synthesis primarily comes from two methods: Offline Hint-Guided Rejection Sampling and Distillation from Multi-agent Framework. We also mix in general-purpose reasoning SFT data to maintain the model's generalization. Beyond the offline data, we use online iterative sampling, continually leveraging updated models to synthesize trajectories with reasoning. We analyze these components separately in Figure 10.

We begin validation from an early checkpoint and use performance on AndroidWorld to assess the impact of reasoning synthesis. First, we observe that as we incrementally add data from Offline Hint-Guided Rejection Sampling, distillation from a multi-agent framework, and general-purpose reasoning SFT data, the model's performance steadily improves. Moreover, adding general reasoning data yields a modest performance gain, indicating that maintaining general reasoning capability is also important for GUI interaction reasoning.

We also examine the gains from iterative training. Starting from the same checkpoint and iteratively training with newly updated trajectory data, we observe sustained performance improvements. It is because, as the model's reasoning ability improves, an increasing share of tasks in the training query set can be completed, thereby enriching the diversity of the training data and enabling the model to learn more robust reasoning capability.

# 5.4 EVALUATION ON AGENTIC FRAMEWORKS

To evaluate the adaptability of GUI-Owl in real-world scenarios, we benchmarked its performance as the core vision model within established agentic frameworks. We integrated various VLMs into two distinct setups: the Mobile-Agent-E (Wang et al., 2025b) framework on the dynamic AndroidWorld environment, and the AgentS2 (Agashe et al., 2025) framework on the OS World desktop environment. This tests the models' ability to generalize across both mobile and PC platforms. The models evaluated include UI-TARS-1.5, UI-TARS-72B, Qwen2.5-VL, Seed-1.5-VL, alongside our GUI-Owl-7B and GUI-Owl-32B.

The experimental results, presented in Table Section 5.4, show that GUI-Owl models achieve substantially higher success rates than all baselines on both mobile and desktop platforms. GUI-Owl-32B, in particular, sets the highest result with a score of 62.1 on AndroidWorld and 48.4 on OSWorld. We attribute this superior agentic adaptability primarily to GUI-Owl's enhanced instruction-following capability. Unlike baseline models that may struggle to interpret the specific directives from an agent's planner, GUI-Owl excels at grounding these commands to the correct visual elements on the screen. This leads to more precise action generation (e.g., clicks and text inputs) and critically reduces the accumulation of errors in multi-step tasks. By more reliably executing each step in a sequence, GUI-Owl ensures higher overall task success, making it a more robust and effective "brain" for GUI agents.

<table><tr><td rowspan="2">Model</td><td colspan="2">Success Rate (%)</td></tr><tr><td>Mobile-Agent-E (Wang et al., 2025b) on AndroidWorld</td><td>Agent-S2 (Agashe et al., 2025) on a subset of OSWorld-Verified</td></tr><tr><td colspan="3">Baseline Models</td></tr><tr><td>UI-TARS-1.5 (Qin et al., 2025)</td><td>14.1</td><td>14.7</td></tr><tr><td>UI-TARS-72B (Qin et al., 2025)</td><td>14.8</td><td>19.0</td></tr><tr><td>Qwen2.5-VL-72B (Bai et al., 2025)</td><td>52.6</td><td>38.6</td></tr><tr><td>Seed-1.5-VL (Team, 2025)</td><td>56.0</td><td>39.7</td></tr><tr><td colspan="3">Our Models</td></tr><tr><td>GUI-Owl-7B</td><td>59.5</td><td>40.8</td></tr><tr><td>GUI-Owl-32B</td><td>62.1</td><td>48.4</td></tr></table>

Table 8: Performance comparison on agentic frameworks. We report the Success Rate  $(\%)$  on both mobile (AndroidWorld) and desktop (OSWorld-Verified) environments. A representative subset of OSWorld-Verified are selected to capture core challenges while reducing computational costs. Underlined denotes the second-best performance.  

<table><tr><td>Action</td><td>Definition</td></tr><tr><td>key</td><td>Perform a key event on the mobile device usingadb&#x27;s keyevent syntax.</td></tr><tr><td>click</td><td>Click the point on the screen with specified (x, y) coordinates.</td></tr><tr><td>long_press</td><td>Press the point on the screen with specified (x, y) coordinates for a specified number of seconds.</td></tr><tr><td>swipe</td><td>Swipe from starting point with specified (x, y) coordinates to endpoint with specified (x2, y2) coordinates.</td></tr><tr><td>type</td><td>Input the specified text into the activated input box.</td></tr><tr><td>answer</td><td>Output the specified answer.</td></tr><tr><td>system_button</td><td>Press the specified system button: Back, Home, Menu, or Enter.</td></tr><tr><td>open</td><td>Open an application on the device specified by text.</td></tr><tr><td>wait</td><td>Wait for a specified number of seconds for changes to occur.</td></tr><tr><td>terminate</td><td>Terminate the current task and report its completion status: success or failure.</td></tr></table>

Table 9: Action Space of GUI-Owl on Mobile.

# 6 DETAILS OF SELF-EVOLVING TRAJECTORY DATA PRODUCTION

In this section, we present the details of our self-evolving trajectory data production pipeline.

# 6.1 OVERVIEW

GUI automation tasks operate in online interactive environments, which renders manual annotation of trajectory data exceedingly tedious and costly, posing significant challenges for GUI trajectory data collection. To address these challenges, we develop a self-evolving GUI trajectory data production pipeline. This approach leverages the capabilities of GUI-Owl itself, continuously generating new trajectories through rollout and assessing their correctness to obtain high-quality training data. Subsequently, these data are utilized to enhance the model's capabilities, creating a reinforcing cycle of improvement.

Our pipeline, illustrated in Figure 4, operates through the following stages: (1) The process initiates with the construction of online virtual environments encompassing mobile, PC, and web platforms, alongside the generation of diverse queries covering a wide range of potential GUI scenarios; (2) Given these queries, the GUI-Owl model predicts actions step-by-step, which are then executed within the online virtual environments, yielding rollout trajectories; (3) A Trajectory Correctness Judgement module, incorporating a multimodal critic framework, evaluates the correctness of all roll-out trajectories. Successful trajectories are collected to create a rich dataset of interaction sequences that capture temporal dependencies and diverse GUI states; (4) For challenging queries where the GUI-Owl model struggles to produce successful trajectories despite numerous attempts, we introduce a Query-specific Guidance Generation module. This module synthesizes step-level guidance based on ground-truth trajectories produced by human annotation or other models, facilitating GUI-Owl's handling of difficult tasks and enhancing the efficiency of the entire data generation pipeline; (5) Finally, all processed data is compiled for

<table><tr><td>Action</td><td>Definition</td></tr><tr><td>key</td><td>Performs key down presses on the arguments passed in order, then performs key releases in reverse order.</td></tr><tr><td>type</td><td>Input a string of text. Use the clear parameter to decide whether to overwrite the existing text, and use the enter parameter to decide whether the enter key should be pressed after typing the text.</td></tr><tr><td>mouse_move</td><td>Move the cursor to a specified (x, y) pixel coordinate on the screen.</td></tr><tr><td>click</td><td>Click the left mouse button at a specified (x, y) pixel coordinate on the screen.</td></tr><tr><td>drag</td><td>Click at a specified (x, y) pixel coordinate on the screen, and drag the cursor to another specified (x2, y2) pixel coordinate on the screen.</td></tr><tr><td>right.click</td><td>Click the right mouse button at a specified (x, y) pixel coordinate on the screen.</td></tr><tr><td>middle.click</td><td>Click the middle mouse button at a specified (x, y) pixel coordinate on the screen.</td></tr><tr><td>double.click</td><td>Double-click the left mouse button at a specified (x, y) pixel coordinate on the screen.</td></tr><tr><td>scroll</td><td>Performs a scroll of the mouse scroll wheel.</td></tr><tr><td>wait</td><td>Wait for a specified number of seconds for changes to occur.</td></tr><tr><td>terminate</td><td>Terminate the current task and report its completion status: success or failure.</td></tr></table>

Table 10: Action Space of GUI-Owl on Desktop.

reinforcement fine-tuning of GUI-Owl. The model undergoes continuous updates, creating a feedback loop where its ability to generate effective roll-out trajectories improves over time, progressively reducing reliance on manual data collection and achieving self-evolution.

This self-evolving data production pipeline effectively addresses the unique challenges of GUI automation tasks, enabling the creation of robust and versatile GUI intelligent agents capable of handling the complexities of modern graphical user interfaces while continuously improving the efficiency and quality of the data production process itself.

# 6.2 HIGH-QUALITY QUERY GENERATION

As highlighted in our overview, generating high-quality queries is a critical component of our self-evolving GUI trajectory data production pipeline. These queries need to cover a wide range of possible user intentions and tasks, reflecting the multifaceted nature of GUI interactions. In this section, we present our innovative approach to query generation for mobile and computer applications, which ensures diversity, realism, and accuracy in the produced queries.

Mobile. For mobile applications, we develop a screenshot-action framework that captures the essence of user interactions while maintaining controllability and extensibility. At the core of our query generation process is a human-annotated directed acyclic graph (DAG)  $\mathcal{G} = \langle P,A\rangle$  for each task. Here,  $P = \{p_{1},\dots ,p_{n}\}$  represents screenshots (e.g., home, ordering, payment), and  $A\subseteq P\times P$  defines valid transitions between them. Each screenshot  $p_i$  includes a description  $d_{i}$  of the screenshot's content and purpose and a set of available slot-value pairs that represent possible user choices or inputs on that screenshot. This structure allows us to model realistic navigation flows within apps and capture the multi-constraint nature of user queries.

Specifically, our query generation process involves the following steps: (1) Path Sampling: We sample a path  $P' = \{p_{\sigma_1}, \dots, p_{\sigma_k}\}$  on the DAG  $\mathcal{G}$ . This path represents a realistic sequence of screenshot transitions within the app. (2) Metadata Extraction: From the sampled path, we obtain screenshot descriptions  $D' = \{d_{\sigma_1}, \dots, d_{\sigma_k}\}$  and the corresponding slot-value pairs  $K', V'$ . (3) Instruction Synthesis: The extracted metadata is fed to a Large Language Model (LLM) to synthesize constrained instructions. This approach ensures that the generated queries are both realistic and aligned with the app's structure. (4) Refinement: To enhance naturalness, we refine the raw DAG paths using few-shot LLM prompting. This step transforms explicit navigation instructions into more natural user queries. For example, "Open the takeout app, click on the food entry" becomes "Order me takeout". (5) Interface Validation: To maintain accuracy, we employ web crawlers to collect real-time interface data from target applications. This ensures that aligned with current app functionality. In conclusion, in our screenshot-action framework, the use of manually defined slot-value pairs minimizes LLM hallucinations, while the DAG structure ensures realistic and controllable navigation flows.

Computer. To acquire operational trajectories for the training of intelligent agents, the initial and most crucial step is the batch acquisition of command data. Unlike mobile phones, the computer usage domain typically involves productivity applications, such as web browsers, document editors, file explorers, and email clients. When it comes to intelligent agents, the utilization of these software tools via keyboard and mouse manipulations presents two primary challenges.

Firstly, there is the fundamental issue of atomic operational skills. Humans, after learning, can proficiently use a mouse for clicking and scrolling and a keyboard for input and shortcut execution. However, intelligent agents driven by vision-language models often lack basic knowledge of atomic operations, such as scrolling through content on web pages or selecting editing targets via dragging in office software.

Secondly, software operational pathways must be navigated, such as accessing privacy settings in Chrome or adjusting page margins in Microsoft Word. Accomplishing these objectives necessitates a series of actions, including clicks, scrolls, and inputs, to reach the requisite configuration options.

Therefore, to bestow intelligent agents with computer usage capabilities, we have synthesized user instructions, targeting both atomic operational skills and software operational pathways, through a combination of manual annotation and automated generation facilitated by Large Language Models (LLMs).

1) Atomic Operations: For common atomic operations with the mouse and keyboard, we initially acquired operational objects within a PC environment via manual annotation. Examples include: a) Double-clicking: This involves creating software icons, folders, etc., to train the model in double-click operations. b) Input: This involves creating files in formats such as Word, Excel, and PowerPoint to train the model's capability to accurately input text at specified locations. c) Dragging: Similarly, files in Word, Excel, and PowerPoint formats are created to train the model to select specific text or move objects through dragging.

Once operational objects are obtained, we input screenshots of these objects and exemplar commands into the Vision-Language Model (VLM), leveraging its in-context learning capabilities to generate additional executable commands within the current page.

2) Software Operational Pathways: For common software operational pathways, we devised a set of automated deep-search chains. Utilizing an accessibility (a11y) tree, we acquire positional and functional information of actionable elements within software interfaces, and by integrating operational pathway memory and replay, we achieve a tree-structured search of actionable elements (e.g., multi-level menus) to garner corresponding operational pathways.

The endpoint settings of each operational pathway pertain to disparate objects. For example, some configurations alter global file attributes (such as image scaling), whereas others necessitate pre-selecting operational objects (such as altering the font size of a text segment).

Therefore, to derive legally executable commands based on operational paths, we employ an LLM to ascertain whether an operational pathway requires pre-selection of an operational object. For pathways necessitating selected objects, we input manually annotated file screenshots and operational pathways into the VLM, thereby generating commands executable within the current page.

# 6.3 TRAJECTORY CORRECTNESS JUDGMENT MODULE

The Trajectory Correctness Judgment Module plays a crucial role in our self-evolving GUI trajectory data production pipeline. Its primary purposes are twofold: to assess the correctness of roll-out trajectories generated by the GUI-Owl model, and to cleanse erroneous steps within otherwise correct trajectories, thereby enhancing the overall quality of our training data. This module is essential for maintaining high standards in our data collection process, ensuring that only accurate and complete trajectories are used for model training.

Our approach to trajectory correctness judgment is comprehensive, operating at both the step level and the trajectory level. This two-tiered system allows for a nuanced evaluation of each action within a trajectory, as well as an overall assessment of the entire interaction sequence.

Problem Definition of Trajectory Correctness Judgment. GUI automation tasks can be formalized as a Markov Decision Process:  $\mathcal{M} = (\mathcal{E},\mathcal{A},\mathcal{P})$ , where  $E$  represents the environment state (including user instructions, interaction history, and screenshots),  $A$  is the action space, and  $P$  is the transition probability.

The Trajectory Correctness Judgement Module consists of two interconnected components: (1) Step-Level Critic: it evaluates individual actions within a trajectory. It analyzes the pre-action state, the executed action, and the post-action state to determine the appropriateness of each step. (2) Trajectory-Level Critic: This component assesses the overall correctness of the entire trajectory. It utilizes the outputs from the Step-Level Critic along with the original user instruction to make a final judgment on the trajectory's success in accomplishing the user's goal.

The relationship between these two levels is hierarchical and complementary. The Step-Level Critic provides granular insights into each action, which are then synthesized by the Trajectory-Level Reflection to form a holistic evaluation of the entire interaction sequence.

Step-Level Critic. Achieving a reliable Step-Level Critic presents a sophisticated challenge that demands nuanced environmental perception and comprehensive understanding. The methodology necessitates a meticulous analysis of pre- and post-action screenshots, coupled with a detailed examination of the executed operation, to accurately assess its contribution towards fulfilling the user's designated objective.

Initially, we annotate the critical interaction regions on the pre-action screenshot, enabling the model to focus on pivotal areas of intervention, including precise operational details—such as clicking, long-pressing, or scrolling—to facilitate a comprehensive evaluation of the action's alignment with the user's goal.

Formally, Step-Level Critic can be conceptualized as a function  $\pi_{\text{critic}}^{\text{step}}(\epsilon, a, \epsilon')$ , where  $\epsilon$  represents the pre-action environmental state (encompassing user instructions, interaction history, and the initial screenshot),  $a$  denotes the executed operation, and  $\epsilon'$  encapsulates the post-action environmental state. The function generates three critical outputs:

- An analysis  $a \in \mathcal{A}$  that provides a detailed interpretation of the action's context and consequences  
- A summary  $s \in  S$  that concisely captures the key insights of the action (typically within 30 words)  
- An annotation  $l \in \{GOOD, NEUTRAL, HARMFUL\}$  that categorizes the action's effectiveness towards the user's objective

This detailed evaluation at the step level is crucial for identifying and potentially correcting erroneous actions within trajectories, thus improving the overall quality of our training data.

Trajectory-Level Critic. The Trajectory-Level Critic,  $\pi_{\text{critic}}^{\text{traj}}(I, T, \pi_{\text{critic}}^{\text{step}})$ , where  $I$  represents the user instruction and  $T$  represents the action trajectory, provides a comprehensive evaluation of the entire trajectory. It employs a two-channel approach: (1) Textual Reasoning Channel ( $\pi_{\text{text}}$ ): Utilizes large language models to assess trajectory correctness based on screenshot caption, textual summaries of each step. (2) Multi-Modal Reasoning Channel ( $\pi_{\text{multimodal}}$ ): Incorporates both visual screenshots and textual summaries for a more comprehensive evaluation. The textual channel provides concise semantic reasoning, while the multi-modal channel enriches the analysis with visual context, the combination of them helps to mitigate potential biases and limitations inherent in single-modal evaluation.

The final GUI trajectory correctness is determined by a consensus mechanism:

$$
\text {T r a j e c t o r y C o r r e c t n e s s} = \left\{ \begin{array}{l l} \text {C o r r e c t ,} & \text {i f} \pi_ {\text {t e x t}} (T, I) = \text {C o r r e c t} \wedge \pi_ {\text {m u l t i m o d a l}} (T, I) = \text {C o r r e c t} \\ \text {I n c o r r e c t ,} & \text {o t h e r w i s e} \end{array} \right. \tag {4}
$$

In conclusion, this multi-channel approach enhances robustness, processes complementary information, and ensures rigorous validation of trajectories.

# 6.4 QUERY-SPECIFIC GUIDANCE GENERATION

In our constructed query set, some queries pose significant challenges for the model, potentially requiring numerous rollouts to obtain a successful trajectory. Some other queries are even more insurmountable and necessitate manual annotation for reference operational trajectories. To acquire more diverse training data, we devise a Query-specific Guidance Generation module, which leverages existing successful trajectories to generate guidance that assists the model in producing more successful trajectories.

Initially, for the obtained reference trajectories, we employ a VLM to generate descriptions of the outcomes of each action. Specifically, the input consists of screenshots of the screen before and after the action execution, coupled with the model's or human's action decisions. The VLM is prompted to observe and describe the result of the current action execution, such as " clicked and activated the search box" or "entered the number 100". When actions involve coordinates, we annotate the interaction locations with circles on the pre-action screenshots to help the VLM focus on detailed screen changes.

Regarding the reference trajectories obtained from model rollouts, given the considerable difficulty of the queries, errors or ineffective operations are inevitable. Thus, the VLM also refers to the model's decision rationale, determining whether the outcomes of each step align with the model's expectations. Operations that do not meet expectations or fail to elicit effective responses are subsequently filtered out during the guidance synthesis process.

After acquiring descriptions for each step of the action execution results, we concatenate the descriptions for all steps within the trajectory. Utilizing a LLM, we summarize the essential steps required to complete the query, thereby yielding query-specific guidance.

# 6.5 EXAMPLES OF TRAINING DATA

We show the format of end-to-end training data on a desktop platform in Figure 11.

# 7 DETAILS OF MOBILE-AGENT-V3

# 7.1 CORE COMPONENTS AND FORMALISM

The operational dynamics of the Mobile-Agent-v3 framework are defined by a set of state variables and the specialized functions of its constituent agents. We formalize these components as follows.

# 7.1.1 STATE VARIABLES AND DEFINITIONS

Let the entire process be a sequence of operations indexed by timestep  $t \in \{0, 1, \dots, T\}$ .

- Device State  $(S_{t})$ : The state of the GUI device at timestep  $t$ , represented as a high-dimensional tensor  $S_{t} \in S \subseteq \mathbb{R}^{H \times W \times C}$ , where  $H, W, C$  are the height, width, and channel dimensions of the screen capture, respectively.  $S_{0}$  denotes the initial state.  
- Subsequent Subgoals  $(SS_{t})$ : An ordered list of pending subgoals formulated by the Manager Agent. It is defined as  $CS_{t} = (g_{1}, g_{2}, \ldots, g_{k})$ , where each  $g_{i}$  is a natural language string describing a discrete step towards the main goal.  
- Complteted Subgoals  $(CS_{t})$ : A set containing subgoals that have been successfully executed and verified. It is defined as  $SS_{t} = \{\bar{g}_{1},\bar{g}_{2},\dots ,\bar{g}_{m}\}$ . This prevents redundant operations and tracks progress.  
- Action  $(A_{t})$ : The operation executed by the Worker Agent at timestep  $t$ . An action is a structured tuple  $A_{t} = (\tau_{t}, \alpha_{t}, \sigma_{t}) \in \mathcal{A}$ , where:

-  $\tau_{t}$ : The thought process, a textual rationale for selecting the action.  
-  $\alpha_{t}$ : The concrete, low-level action command (e.g., click(x, y), type("text").  
-  $\sigma_{t}$ : A concise summary of the action's intended effect.

- Reflection Feedback  $(F_{t})$ : The output generated by the Reflector Agent after observing the consequences of action  $A_{t}$ . It is a tuple  $F_{t} = (j_{t},\phi_{t})\in \{\mathrm{SUCCESS, FALIURE}\} \times \Phi$ , where:

-  $j_{t}$ : A binary judgment on the outcome of  $A_{t}$ .  
-  $\phi_t$ : A detailed textual feedback, particularly a diagnostic analysis in case of "FAILURE".  $\Phi$  represents the space of all possible feedback texts.

- Notes  $(N_{t})$ : A collection of critical, potentially transient information captured by the Notetaker Agent. The cumulative knowledge base at step  $t$  is  $\mathcal{N}_t = \bigcup_{i=0}^{t-1} N_i$ .

# 7.2 AGENT ARCHITECTURE IN DETAIL

# 7.2.1 EXTERNAL KNOWLEDGE RETRIEVAL WITH RAG

To enable the agent to complete tasks requiring real-time information or domain-specific knowledge (e.g., checking today's weather, finding recent sports scores, or looking up app-specific tutorials), we incorporate a RAG module. This module is invoked at the beginning of a task to retrieve relevant information from external sources, such as the internet, and provide it as context to the agent system.

The process can be formalized as follows. Given an initial user instruction  $I$ , the RAG module first processes it into one or more search engine-friendly queries  $Q$ .

$$
Q = \operatorname {G e n e r a t e Q u e r i e s} (I)
$$

Subsequently, the system uses these queries  $Q$  to retrieve a set of relevant documents or text snippets  $D = \{d_{1}, d_{2}, \ldots, d_{n}\}$  from an external knowledge source (e.g., a web search engine).

$$
D = \operatorname {S e a r c h E n g i n e} (Q)
$$

Finally, the retrieved content is processed and summarized to form a concise, information-rich body of knowledge,  $K_{\mathrm{RAG}}$ .

$$
K _ {\mathrm {R A G}} = \operatorname {P r o c e s s} (D)
$$

This retrieved knowledge  $K_{\mathrm{RAG}}$  is passed to the Manager agent during its initialization phase (as shown in Algorithm Algorithm 1, lines 3-4). This allows the Manager to generate its initial plan  $(SS_0, CS_0)$  based on more comprehensive and accurate information, thereby significantly improving the quality of the plan and the likelihood of task success. For example, for an instruction like "Should I take an umbrella to the park today?",  $K_{\mathrm{RAG}}$  would contain the weather forecast, enabling the Manager to create a plan that includes steps like "open weather app" and "check for rain probability".

# 7.2.2 THE MANAGER AGENT: DYNAMIC TASK PLANNING AND COORDINATION

The Manager Agent serves as the strategic core of the framework. Its function  $\mathcal{M}$  is responsible for decomposing the high-level user instruction  $I$  into a coherent sequence of subgoals and dynamically adapting this plan throughout the execution process.

Initially, at  $t = 0$ , the Manager performs a decomposition:

$$
\left(S S _ {0}, C S _ {0}\right) = \mathcal {M} _ {\text {i n i t}} \left(I, S _ {0}, K _ {\mathrm {R A G}}\right) \tag {5}
$$

where  $K_{\mathrm{RAG}}$  is external knowledge retrieved by the RAG module to inform the decomposition of potentially domain-specific or complex instructions.  $CS_0$  is initialized as an empty set  $\emptyset$ .

In subsequent steps  $t > 0$ , the Manager updates the plan based on the latest execution results:

$$
\left(S S _ {t}, C S _ {t}\right) = \mathcal {M} _ {\text {u p d a t e}} \left(I, S _ {t - 1}, S S _ {t - 1}, C S _ {t - 1}, A _ {t - 1}, F _ {t - 1}, \mathcal {N} _ {t}\right) \tag {6}
$$

If the previous action was successful ( $j_{t-1} = \text{SUCCEED}$ ), the Manager identifies the completed subgoal in  $SS_{t-1}$ , moves it to  $CS_t$ , and re-prioritizes the remaining tasks in  $SS_t$ . If the action failed ( $j_{t-1} = \text{FAILURE}$ ), the Manager leverages the diagnostic feedback  $\phi_{t-1}$  to revise the plan. This may involve re-ordering subgoals, modifying an existing subgoal, inserting a new corrective subgoal, or even reverting to a previous strategy.

# 7.2.3 THE WORKER AGENT: GROUNDED ACTION EXECUTION

The Worker Agent is the tactical executor, translating the strategic plan from the Manager into concrete interactions with the GUI. Its function  $\mathcal{W}$  aims to execute the highest-priority, currently feasible subgoal from the guidance list  $CS_{t}$ .

$$
A _ {t} = \mathcal {W} \left(I, S _ {t}, S S _ {t}, F _ {t - 1}, \mathcal {N} _ {t}\right) \tag {7}
$$

Upon receiving the subgoal list  $SS_{t}$ , the Worker inspects a small subset from the top of the list (e.g., the top  $N$  subgoals). It analyzes the current screen  $S_{t}$  to determine which of these subgoals is most relevant and actionable. The decision-making process is informed by feedback from the previous step,  $F_{t-1}$ , to avoid repeating errors, and the accumulated notes,  $\mathcal{N}_{t}$ , to utilize previously stored information (e.g., using a password saved in notes). The output,  $A_{t} = (\tau_{t}, \alpha_{t}, \sigma_{t})$ , provides a transparent record of its reasoning, action, and intent, which is crucial for reflection.

# 7.2.4 THE REFLECTOR AGENT: SELF-CORRECTION THROUGH REFLECTION

The Reflector Agent is a critical component for ensuring robustness and learning from mistakes. It embodies the framework's capacity for self-assessment. Its function  $\mathcal{R}$  evaluates the efficacy of an action by comparing the state transition with the Worker's intent.

$$
F _ {t} = \mathcal {R} \left(I, S _ {t}, S _ {t + 1}, A _ {t}\right) \tag {8}
$$

The Reflector analyzes the pre-action state  $S_{t}$ , the post-action state  $S_{t + 1}$ , and the action tuple  $A_{t}$ . A judgment  $j_{t} = \mathrm{SUCCESS}$  is rendered if the state change  $S_{t}\rightarrow S_{t + 1}$  aligns with the progress articulated in the Worker's thought  $\tau_{t}$  and summary  $\sigma_t$ . Conversely,  $j_{t} = \mathrm{FAILURE}$  is returned if the GUI presents an error, remains unchanged unexpectedly, or transitions to an irrelevant state. In case of failure, the feedback  $\phi_t$  provides a causal analysis, such as action click(123, 456) on button "Submit" did not proceed to the next page; an error message "Invalid credentials" is now visible. This detailed feedback is vital for the Manager's replanning phase.

# 7.2.5 THE NOTETAKER AGENT: PERSISTENT CONTEXTUAL MEMORY

The Notetaker Agent addresses the challenge of state volatility in GUI interactions, where crucial information may appear on one screen and be required on a subsequent, different screen. The Notetaker's function  $\mathcal{C}$  is to identify and persist such information.

$$
N _ {t} = \mathcal {C} \left(S _ {t}\right) \tag {9}
$$

This agent is triggered only upon a successful action ( $j_{t} = \mathrm{SUCCESSION}$ ). It scans the state transition for pieces of information designated as vital for the ongoing task (e.g., reservation codes, order numbers, user-generated content, entered credentials). This information is structured into the note set  $N_{t}$ . The cumulative notes  $\mathcal{N}_{t+1} = \mathcal{N}_{t} \cup N_{t}$  are then made available to both the Manager and the Worker in future steps, creating a persistent memory that informs long-horizon planning and execution.

Algorithm 1 Mobile-Agent-v3 Execution Loop

1: Input: User instruction  $I$ , initial device state  $S_0$ , max timesteps  $T_{\mathrm{max}}$  
2: Initialize: Manager  $\mathcal{M}$ , Worker  $\mathcal{W}$ , Reflector  $\mathcal{R}$ , Notetaker  $\mathcal{C}$

$\triangleright$  Init Manager Phase: Initialize Plan

3: Retrieve external knowledge  $K_{\mathrm{RAG}} \gets \mathrm{RAG}(I)$  
4:  $(SS_0, CS_0) \gets \mathcal{M}_{\mathrm{init}}(I, S_0, K_{\mathrm{RAG}})$  
5:  $\mathcal{N}_0\gets \emptyset ,F_{-1}\gets \mathrm{null},t\gets 0$

6: while  $t < T_{\max}$  and  $SS_{t} \neq \emptyset$  do

$\triangleright$  Worker Phase: Execute Action

7:  $A_{t}\gets \mathcal{W}(I,S_{t},SS_{t},F_{t - 1},\mathcal{N}_{t})$  
8: if  $A_{t} =$  TERMINATE then  
9: Break  
10: end if  
11:  $S_{t + 1}\gets \mathrm{ExecuteOnDevice}(A_t)$

$\triangleright$  Reflector Phase: Evaluate Outcome

12:  $F_{t}\gets \mathcal{R}(I,S_{t},S_{t + 1},A_{t})$

Notetaker Phase: Persist Information

13: if  $F_{t}$ .status = SUCCESS then

14:  $N_{t}\gets \mathcal{C}(S_{t})$  
15:  $\mathcal{N}_{t + 1}\gets \mathcal{N}_t\cup \{N_t\}$  
16: else  
17:  $\mathcal{N}_{t + 1}\gets \mathcal{N}_t$  
18: end if

Manager Phase: Update Plan

19:  $(SS_{t + 1},CS_{t + 1})\gets \mathcal{M}_{\mathrm{update}}(I,S_t,SS_t,CS_t,A_t,F_t,\mathcal{N}_{t + 1})$  
20:  $t\gets t + 1$  
21: end while

22: if  $S S_{t} = \emptyset$  then

23: return Task Succeeded  
24: else  
25: return Task Failed (Timeout or Stalemate)  
26: end if

# 7.3 INTEGRATED WORKFLOW AND ALGORITHM

The Mobile-Agent-v3 framework operates as a cyclical, state-driven process. The workflow begins with a user instruction and terminates when the task is complete or deemed unachievable. The entire process is formalized in Algorithm 1.

The process is initialized with a high-level user instruction  $I$ . The Manager Agent, aided by the RAG module, creates an initial subgoal plan  $SS_{0}$ . The system then enters an iterative loop. In each iteration  $t$ , the Worker Agent selects and executes a subgoal, resulting in action  $A_{t}$ . The environment transitions to a new state  $S_{t+1}$ . The Reflector Agent evaluates this transition, producing feedback  $F_{t}$ . If the action was successful, the Notetaker Agent may record pertinent information as  $N_{t}$ . Finally, the Manager Agent updates the task plan to  $(SS_{t+1}, CS_{t+1})$  based on the feedback.

Termination occurs under two conditions:

1. Task Completion: The Manager determines the task is complete, resulting in an empty pending subgoal list  $(SS_{t} = \emptyset)$ .  
2. Execution Stalemate: The Worker Agent determines that no pending subgoals in  $SS_{t}$  can be executed on the current state  $S_{t}$ , even after several retries or plan revisions.

This structured, reflective, and adaptive loop enables the framework to navigate complex sequences of interactions, handle unexpected events, and robustly pursue the completion of the user's goal.

# 7.4 CASE STUDY

Figure 12 shows a complete Mobile-Agent-v3 operation flow, including the outputs of the manager, worker, and reflector. The manager's output shows that subgoals are constantly updated as the task progresses. The worker consistently outputs actions guided by the subgoals output by the manager. Notably, the red text in Figure 12 illustrates a successful reflection. After the worker's click operation in the previous step failed, the reflector successfully discovered the problem and provided feedback to the manager and worker in the next step. Finally, the worker repeated the click operation to correct the problem.

# 8 CONCLUSION

In this paper, we present GUI-Owl, a native end-to-end multimodal agent model that unifies perception, grounding, reasoning, planning, and action execution within a single scalable framework for GUI automation. Building upon Qwen2.5-VL and extensively post-trained on large-scale, diverse GUI interaction data, GUI-Owl achieves state-of-the-art performance across a broad range of challenging benchmarks, surpassing both open-source and proprietary systems, including GPT-4o and Claude 3.7. Through synthesized reasoning data and a scalable reinforcement learning framework, GUI-Owl is capable of versatile decision-making from autonomous single-agent execution to collaborative multi-agent role coordination within our Mobile-Agent-v3 framework.

# Example of end-to-end training data

```json
{
    "role": "system",
    "content": ["type": "text", "text": "You are a helpful assistant, \nYou may call one or more functions to assist with the user query. \nYou are provided with function signatures within <tools></tools> XML tags: n<tools>n{"type": "function", "function": {"name": "computer_use", "description": "Use a mouse and keyboard to interact with a computer, and take screenshots.\n* This is an interface to a desktop GUI. You do not have access to a terminal or applications menu. You must click on desktop icons to start applications.\n* Some applications may take time to start or process actions, so you may need to wait and take successive screenshots to see the results of your actions. E.g. if you click on Firefox and a window doesn't open, try wait and taking another screenshot.\n* The screen's resolution is 1932x1092.\n* Whenever you intend to move the cursor to click on an element like an icon, you should consult a screenshot to determine the coordinates of the element before moving the cursor.\n* If you tried clicking on a program or link but it failed to load, even after waiting, try adjusting your cursor position so that the tip of the cursor visually falls on the element that you want to click.\n* Make sure to click any buttons, links, icons, etc with the cursor tip in the center of the element. Don't click boxes on their edges unless asked.\n", "parameters": {"properties": {"action": {"description": "The action to perform. The available actions are:\n* 'key': Perform key down presses on the arguments passed in order, then performs key releases in reverse order."\n* 'type': Type a string of text on the keyboard.\n* 'mouse_move': Move the cursor to a specified (x, y) pixel coordinate on the screen.\n* 'leftclick': Click the left mouse button.\n* 'leftclickdrag': Click and drag the cursor to a specified (x, y) pixel coordinate on the screen.\n* 'rightclick': Click the right mouse button.\n* 'middleclick': Click the middle mouse button.\n* doubleclick': Double-click the left mouse button.\n* 'scroll': Performs a scroll of the mouse scroll wheel.\n* 'wait': Wait specified seconds for the change to happen.\n* 'terminate': Terminate the current task and report its completion status.\n", "enum": ["key", "type", "mouse_move", "leftclick", "leftclickdrag", "rightclick", "middleclick", "doubleclick", "scroll", "wait", "terminate"], "type": "string"], "keys": {"description": Required only by 'action=key', "type": "array"], "text": {"description": "Required only by 'action:type', "type": "string"], "coordinate": {"description": "(x, y): The x (pixels from the left edge) and y (pixels from the top edge) coordinates move to the mouse to. Required only by 'action=mouse_move' and 'action=leftclickdrag'.", "type": "array"},
"required": {"description": "The amount of scrolling to perform. Positive values scroll up, negative values scroll down. Required only by 'action=scroll', "type": "number"},
"required": {"description": "The seconds to wait. Required only by 'action=wait', "type": "number"},
"required": {"status": {"description": "The status of the task. Required only by 'action=terminate', "type": "string"], "enum": ["success", "failure"]
"required": ["action"], "type": "object"]} \n</tools>\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n</tool_call">
}
{
    "role": "user",
    "content": [
        {"type": "text", "text": "The user query: I want to share 'Oscar and the wolf - somebody wants' music to my friend karin.iversen@example.com via gmail\nBefore answering, explain your reasoning step-by-step in <thinking></thinking> tags, and insert them before the <tool_call></tool_call> XML tags.\nTask progress (You have done the following operation on the current device):",
    {
        "type": "image",
        "image": <image>
    }
},
{
    "role": "assistant",
    "content": ["type": "text", "text": "<thinking>\nThe global task objective is to share the music "\Oscar and the wolf - somebody wants" to the email karin.iversen@example.com via Gmail.
Currently, the screen shows a video upload interface, which is not relevant to the task at hand. To proceed towards the goal, I need to navigate back to a more relevant screen where I can find the music or access the sharing options.\n\nAction: Go back to the previous screen.\n</thinking>\n<tool_call>\n{{"name": "mobile_use", "arguments": {"action": }, "system_button", "button": Back}}\n</tool_call>\n}
}
```

Figure 11: Format of end-to-end training data.

Search for Alibaba's stock price in Microsoft Edge, then create a new spreadsheet in WPS. Enter the company name in the first column and the stock price in the second column.

# $\Rightarrow$  Mobile-Agent-v3

![](images/c9bac84356aee723609a9e71e37b03b1545f24a113f3f49cc8bef9474a5f9f29.jpg)

![](images/8f15bfdcb8a50c47944123cd5f797b647073357d71fb924a4610ac1fabcb8d84.jpg)

![](images/b8f65b4f6b1bbddb285d378c20407d55f427614a84a8335c1b0a8ace9a71a03b.jpg)  
Figure 12: A case of a complete Mobile-Agent-3 operation process on a desktop platform. The red text represents successful reflection content.

# REFERENCES

Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s: An open agentic framework that uses computers like a human. arXiv preprint arXiv:2410.08164, 2024. 10  
Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s2: A compositional generalist-specialist framework for computer use agents, 2025. URL https://arxiv.org/abs/2504.00906. 2, 10, 16, 17  
Anthropic. Claude-3-5-sonnet. Technical report, Anthropic, 2024. URL https://www.anthropic.com/news/claude-3-5-sonnet.14  
Anthropic. Claude 3.7 sonnet and claude code. Technical report, Anthropic, 2025a. URL https://www.anthropic.com/news/claudi-3-7-sonnet. System Card. 12, 13, 14  
Anthropic. Claude-4-sonnet. Technical report, Anthropic, 2025b. URL https://www.anthropic.com/news/claude-4.14  
Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 6, 12, 13, 14, 17  
Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Li YanTao, Jianbing Zhang, and Zhiyong Wu. SeeClick: Harnessing GUI grounding for advanced visual GUI agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 9313-9332, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.505.12  
Alibaba Cloud. Introducing alibaba cloud, 2018. 3  
Deepmind. Introducing gemini 2.0: our new ai model for the agentic era. Technical report, Deepmind, 2025a. URL https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#project-astra.14  
Deepmind. Gemini 2.5: Our most intelligent ai model. Technical report, Deepmind, 2025b. URL https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/.13  
Minghe Gao, Wendong Bu, Bingchen Miao, Yang Wu, Yunfei Li, Juncheng Li, Siliang Tang, Qi Wu, Yueting Zhuang, and Meng Wang. Generalist virtual agents: A survey on autonomous agents across digital platforms. ArXiv preprint, abs/2411.10943, 2024. URL https://arxiv.org/abs/2411.10943.2  
Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024. 13  
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 9  
Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, et al. Os agents: A survey on mllm-based agents for general computing devices use. 2024. 2  
Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 12, 13, 14  
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dólar, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. 6  
Ning Li, Xiangmou Qu, Jiamu Zhou, Jun Wang, Muning Wen, Kounianhua Du, Xingyu Lou, Qiuying Peng, and Weinan Zhang. Mobileuse: A gui agent with hierarchical reflection for autonomous mobile operation. arXiv preprint arXiv:2507.16853, 2025. 10  
Yanda Li, Chi Zhang, Wanqi Yang, Bin Fu, Pei Cheng, Xin Chen, Ling Chen, and Yunchao Wei. Appagent v2: Advanced agent for flexible mobile interactions. arXiv preprint arXiv:2408.11824, 2024. 10

Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zochen Bai, Stan Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 19498-19508, 2025. 13  
Xiao Liu, Bo Qin, Dongzhu Liang, Guang Dong, Hanyu Lai, Hanchen Zhang, Hanlin Zhao, Iat Long Long, Jiadai Sun, Jiaqi Wang, et al. Autoglm: Autonomous foundation agents for guis. arXiv preprint arXiv:2411.00820, 2024. 10  
Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners. arXiv preprint arXiv:2504.14239, 2025. 12  
Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025. 12  
Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1: A generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025. 6, 9  
Shravan Nayak, Xiangru Jian, Kevin Qinghong Lin, Juan A Rodriguez, Montek Kalsi, Rabiul Awal, Nicolas Chapados, M Tamer Ozsu, Aishwarya Agrawal, David Vazquez, et al. Ui-vision: A desktop-centric gui benchmark for visual perception and interaction. arXiv preprint arXiv:2503.15661, 2025. 6, 9  
Dang Nguyen, Jian Chen, Yu Wang, Gang Wu, Namyong Park, Zhengmian Hu, Hanjia Lyu, Junda Wu, Ryan Aponte, Yu Xia, et al. Gui agents: A survey. ArXiv preprint, abs/2412.13501, 2024. URL https://arxiv.org/abs/2412.13501.2  
Runliang Niu, Jindong Li, Shiqi Wang, Yali Fu, Xiyu Hu, Xueyuan Leng, He Kong, Yi Chang, and Qi Wang. Screenagent: A vision language model-driven computer control agent. arXiv preprint arXiv:2402.07945, 2024. 10  
Songqin Nong, Jiali Zhu, Rui Wu, Jiongchao Jin, Shuo Shan, Xiutian Huang, and Wenhao Xu. Mobileflow: A multimodal llm for mobile gui agent. arXiv preprint arXiv:2407.04346, 2024. 10  
OpenAI. Computer-using agent: Introducing a universal interface for ai to interact with the digital world. 2025a. URL https://openai.com/index/computer-using-agent.12, 13  
OpenAI. Openai o3 and o4-mini system card. Technical report, OpenAI, 2025b. URL https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf. System Card. 14  
Ajay Patel, Markus Hofmarcher, Claudiu Leoveanu-Condrei, Marius-Constantin Dinu, Chris Callison-Burch, and Sepp Hochreiter. Large language models can self-improve at web agent tasks. arXiv preprint arXiv:2405.20309, 2024. 10  
Shishir G. Patil, Huanzhi Mao, Charlie Cheng-Jie Ji, Fanjia Yan, Vishnu Suresh, Ion Stoica, and Joseph E. Gonzalez. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In *Forty-second International Conference on Machine Learning*, 2025. 5  
Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. 2, 4, 12, 13, 14, 17  
Linxin Song, Yutong Dai, Viraj Prabhu, Jieyu Zhang, Taiwei Shi, Li Li, Junnan Li, Silvio Savarese, Zeyuan Chen, Jieyu Zhao, et al. Coact-1: Computer-using agents with coding as actions. arXiv preprint arXiv:2508.03923, 2025.2  
Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, et al. Os-genesis: Automating gui agent trajectory construction via reverse task synthesis. arXiv preprint arXiv:2412.19723, 2024. 10  
Weihao Tan, Wentao Zhang, Xinrun Xu, Haochong Xia, Ziluo Ding, Boyu Li, Bohan Zhou, Junpeng Yue, Jiechuan Jiang, Yewen Li, et al. Cradle: Empowering foundation agents towards general computer control. arXiv preprint arXiv:2403.03186, 2024. 10  
Fei Tang, Zhangxuan Gu, Zhengxi Lu, Xuyang Liu, Shuheng Shen, Changhua Meng, Wen Wang, Wenqi Zhang, Yongliang Shen, Weiming Lu, et al. Gui-g²: Gaussian reward modeling for gui grounding. arXiv preprint arXiv:2507.15846, 2025. 12

ByteDance Seed Team. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. 12, 13, 14, 17  
Qwen Team. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024.6  
Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. arXiv preprint arXiv:2406.01014, 2024a. 2  
Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. Advances in Neural Information Processing Systems, 37:2686-2710, 2024b. 2, 10  
Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024c. 14  
Shuai Wang, Weiwen Liu, Jingxuan Chen, Weinan Gan, Xingshan Zeng, Shuai Yu, Xinlong Hao, Kun Shao, Yasheng Wang, and Ruiming Tang. Gui agents with foundation models: A comprehensive survey. ArXiv preprint, abs/2411.04890, 2024d. URL https://arxiv.org/abs/2411.04890.2  
Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Boyuan Zheng, Peihang Li, Fangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Jiarui Hu, Yuyan Wang, Jixuan Chen, Yuxiao Ye, Danyang Zhang, Dikang Du, Hao Hu, Huarong Chen, Zaida Zhou, Haotian Yao, Ziwei Chen, Qizheng Gu, Yipu Wang, Heng Wang, Diyi Yang, Victor Zhong, Flood Sung, Y. Charles, Zhilin Yang, and Tao Yu. Opencua: Open foundations for computer-use agents, 2025a. URL https://arxiv.org/abs/2508.09123. 2, 4, 14  
Zhenhailong Wang, Haiyang Xu, Junyang Wang, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, and Heng Ji. Mobile-agent-e: Self-evolving mobile assistant for complex tasks. arXiv preprint arXiv:2501.11733, 2025b. 10, 16, 17  
Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and Lingpeng Kong. Os-copilot: Towards generalist computer agents with self-improvement. arXiv preprint arXiv:2402.07456, 2024a.10  
Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: A foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024b. 10, 12, 13  
Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh J Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:52040-52094, 2024. 9, 13  
Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, Yiheng Xu, Junli Wang, Doyen Sahoo, Tao Yu, and Caiming Xiong. Scaling computer-use grounding via user interface decomposition and synthesis, 2025. URL https://arxiv.org/abs/2505.13227.2, 12, 13  
Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. 13, 14  
Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha, Zeyuan Chen, et al. Gta1: Gui test-time scaling agent. arXiv preprint arXiv:2507.05791, 2025. 2  
Wenwen Yu, Zhibo Yang, Jianqiang Wan, Sibo Song, Jun Tang, Wenqing Cheng, Yuliang Liu, and Xiang Bai. Omniparser v2: Structured-points-of-thought for unified visual text parsing and its generality to multimodal large language models. arXiv preprint arXiv:2502.16161, 2025. 6, 12  
Xinbin Yuan, Jian Zhang, Kaixin Li, Zhuoxuan Cai, Lujian Yao, Jie Chen, Enguang Wang, Qibin Hou, Jinwei Chen, Peng-Tao Jiang, et al. Enhancing visual grounding for gui agents via self-evolutionary reinforcement learning. arXiv preprint arXiv:2505.12370, 2025. 12  
Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liquin Li, Si Qin, Yu Kang, Minghua Ma, Qingwei Lin, Saravan Rajmohan, et al. Large language model-brained gui agents: A survey. ArXiv preprint, abs/2411.18279, 2024a. URL https://arxiv.org/abs/2411.18279. 2

Chaoyun Zhang, Liquin Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, et al. Ufo: A ui-focused agent for windows os interaction. arXiv preprint arXiv:2402.07939, 2024b. 10  
Chi Zhang, Zhao Yang, Jiaxuan Liu, Yanda Li, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pp. 1-20, 2025a. 10  
Zhong Zhang, Yaxi Lu, Yukun Fu, Yupeng Huo, Shenzhi Yang, Yesai Wu, Han Si, Xin Cong, Haotian Chen, Yankai Lin, et al. Agentcpm-gui: Building mobile-use agents with reinforcement fine-tuning. arXiv preprint arXiv:2506.01391, 2025b. 10  
Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is a generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. 10  
Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinqlin Jia, et al. Gui-g1: Understanding r1-zero-like training for visual grounding in gui agents. arXiv preprint arXiv:2505.15810, 2025. 12  
Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 13, 14